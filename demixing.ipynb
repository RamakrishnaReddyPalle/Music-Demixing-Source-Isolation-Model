{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# ***End to End Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conditioners**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Conditioning(nn.Module):\n",
    "    def __init__(\n",
    "        self, cond_embedding_dim: int, channels: int, channels_per_group: int = 16\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.cond_embedding_dim = cond_embedding_dim\n",
    "        self.channels_per_group = channels_per_group\n",
    "\n",
    "        self.gn = nn.GroupNorm(self.channels // self.channels_per_group, self.channels)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class PassThroughConditioning(Conditioning):\n",
    "    def __init__(\n",
    "        self, cond_embedding_dim: int, channels: int, channels_per_group: int = 16\n",
    "    ):\n",
    "        super().__init__(cond_embedding_dim, channels, channels_per_group)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        return self.gn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Film\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation as activation_\n",
    "\n",
    "\n",
    "class FiLM(Conditioning):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cond_embedding_dim: int,\n",
    "        channels: int,\n",
    "        additive: bool = True,\n",
    "        multiplicative: bool = False,\n",
    "        depth: int = 1,\n",
    "        activation: str = \"ELU\",\n",
    "        channels_per_group: int = 16,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            channels=channels,\n",
    "            channels_per_group=channels_per_group,\n",
    "            cond_embedding_dim=cond_embedding_dim,\n",
    "        )\n",
    "\n",
    "        self.additive = additive\n",
    "        self.multiplicative = multiplicative\n",
    "        self.depth = depth\n",
    "        self.activation = activation\n",
    "\n",
    "        Activation = activation_.__dict__[activation]\n",
    "\n",
    "        if self.multiplicative:\n",
    "\n",
    "            if depth == 1:\n",
    "                self.gamma = nn.Linear(self.cond_embedding_dim, self.channels)\n",
    "            else:\n",
    "                layers = [nn.Linear(self.cond_embedding_dim, self.channels)]\n",
    "                for _ in range(depth - 1):\n",
    "                    layers += [Activation(), nn.Linear(self.channels, self.channels)]\n",
    "                self.gamma = nn.Sequential(*layers)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        if self.additive:\n",
    "            if depth == 1:\n",
    "                self.beta = nn.Linear(self.cond_embedding_dim, self.channels)\n",
    "            else:\n",
    "                layers = [nn.Linear(self.cond_embedding_dim, self.channels)]\n",
    "                for _ in range(depth - 1):\n",
    "                    layers += [Activation(), nn.Linear(self.channels, self.channels)]\n",
    "                self.beta = nn.Sequential(*layers)\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def forward(self, x, w):\n",
    "\n",
    "        x = self.gn(x)\n",
    "\n",
    "        if self.multiplicative:\n",
    "            gamma = self.gamma(w)\n",
    "\n",
    "            if len(x.shape) == 4:\n",
    "                gamma = gamma[:, :, None, None]\n",
    "            elif len(x.shape) == 3:\n",
    "                gamma = gamma[:, :, None]\n",
    "            elif len(x.shape) == 2:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid shape for input tensor: {x.shape}\")\n",
    "\n",
    "            x = gamma * x\n",
    "\n",
    "        if self.additive:\n",
    "            beta = self.beta(w)\n",
    "            if len(x.shape) == 4:\n",
    "                beta = beta[:, :, None, None]\n",
    "            elif len(x.shape) == 3:\n",
    "                beta = beta[:, :, None]\n",
    "            elif len(x.shape) == 2:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid shape for input tensor: {x.shape}\")\n",
    "\n",
    "            x = x + beta\n",
    "\n",
    "        return x\n",
    "        \n",
    "class CosineSimiliarity(Conditioning):\n",
    "    def __init__(self, cond_embedding_dim: int, channels: int, channels_per_group: int = 16):\n",
    "        super().__init__(cond_embedding_dim, channels, channels_per_group)\n",
    "        \n",
    "        self.csim = nn.CosineSimilarity(dim=1)\n",
    "        self.proj = nn.Linear(self.cond_embedding_dim, self.channels * self.channels)\n",
    "        \n",
    "    def forward(self, x, w):\n",
    "        \n",
    "        \n",
    "        x = self.gn(x)\n",
    "\n",
    "        gamma = self.gamma(w)\n",
    "\n",
    "        if len(x.shape) == 4:\n",
    "            gamma = gamma[:, :, None, None]\n",
    "        elif len(x.shape) == 3:\n",
    "            gamma = gamma[:, :, None]\n",
    "        elif len(x.shape) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid shape for input tensor: {x.shape}\")\n",
    "        \n",
    "        c = self.csim(gamma, x)\n",
    "        \n",
    "        x = c[:, None, ...] * x\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class GeneralizedBilinear(nn.Bilinear):\n",
    "    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        super().__init__(in1_features, in2_features, out_features, bias, device, dtype)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        out = torch.einsum(\n",
    "            \"bc...,acd,bd->ba...\", x1, self.weight, x2\n",
    "        )\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            ndim = out.ndim\n",
    "            bias = torch.reshape(self.bias, (1, -1) + (1,) * (ndim - 2))\n",
    "            \n",
    "            out = out + bias\n",
    "            \n",
    "        return out\n",
    "           \n",
    "\n",
    "class BilinearFiLM(Conditioning):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cond_embedding_dim: int,\n",
    "        channels: int,\n",
    "        additive: bool = True,\n",
    "        multiplicative: bool = False,\n",
    "        depth: int = 2,\n",
    "        activation: str = \"ELU\",\n",
    "        channels_per_group: int = 16,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            channels=channels,\n",
    "            channels_per_group=channels_per_group,\n",
    "            cond_embedding_dim=cond_embedding_dim,\n",
    "        )\n",
    "\n",
    "        self.additive = additive\n",
    "        self.multiplicative = multiplicative\n",
    "        self.depth = depth\n",
    "        assert depth == 2, \"Only depth 2 is supported for BilinearFiLM\"\n",
    "        self.activation = activation\n",
    "\n",
    "        Activation = activation_.__dict__[activation]\n",
    "\n",
    "        if self.multiplicative:\n",
    "            self.gamma_proj = nn.Sequential(\n",
    "                nn.Linear(self.cond_embedding_dim, self.channels),\n",
    "                Activation(),\n",
    "            )\n",
    "            self.gamma_bilinear = GeneralizedBilinear(self.channels, self.channels, self.channels)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "        if self.additive:\n",
    "            self.beta_proj = nn.Sequential(\n",
    "                nn.Linear(self.cond_embedding_dim, self.channels),\n",
    "                Activation(),\n",
    "            )\n",
    "            self.beta_bilinear = GeneralizedBilinear(self.channels, self.channels, self.channels)\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def forward(self, x, w):\n",
    "\n",
    "        x = self.gn(x)\n",
    "\n",
    "        if self.multiplicative:\n",
    "            gamma = self.gamma_proj(w)\n",
    "            gamma = self.gamma_bilinear(x, gamma)\n",
    "            x = gamma * x\n",
    "\n",
    "        if self.additive:\n",
    "            beta = self.beta_proj(w)\n",
    "            beta = self.beta_bilinear(x, beta)\n",
    "            x = x + beta\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputType  OutputType LossOutputType MetricOutputType \n",
    "# ModelType  OptimizerType SchedulerType MetricType LossType \n",
    "# OptimizationBundle\n",
    "# LossHandler MetricHandler AugmentationHandle InferenceHandler \n",
    "\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchmetrics as tm\n",
    "\n",
    "\n",
    "class OperationMode:\n",
    "    TRAIN = \"train\"\n",
    "    VAL = \"val\"\n",
    "    TEST = \"test\"\n",
    "    PREDICT = \"predict\"\n",
    "\n",
    "\n",
    "RawInputType = Dict\n",
    "\n",
    "\n",
    "def nested_dict_to_nested_namespace(d: dict) -> SimpleNamespace:\n",
    "    d_ = d.copy()\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = nested_dict_to_nested_namespace(v)\n",
    "\n",
    "        d_[k] = v\n",
    "\n",
    "    return SimpleNamespace(**d_)\n",
    "\n",
    "\n",
    "RawInputType = TypedDict(\n",
    "    \"RawInputType\",\n",
    "    {\n",
    "        \"mixture\": torch.Tensor,\n",
    "        \"sources\": Dict[str, torch.Tensor],\n",
    "        \"estimates\": Optional[Dict[str, torch.Tensor]],\n",
    "        \"metadata\": Dict[str, Any],\n",
    "    },\n",
    "    total=False,\n",
    ")\n",
    "\n",
    "\n",
    "def input_dict(\n",
    "    mixture: torch.Tensor = None,\n",
    "    sources: Dict[str, torch.Tensor] = None,\n",
    "    query: torch.Tensor = None,\n",
    "    metadata: Dict[str, Any] = None,\n",
    "    modality: str = \"audio\",\n",
    ") -> RawInputType:\n",
    "\n",
    "    out = {\n",
    "        \"estimates\": {\n",
    "            k: {\n",
    "                modality: torch.empty(\n",
    "                    0,\n",
    "                )\n",
    "            }\n",
    "            for k, v in sources.items()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if mixture is not None:\n",
    "        out[\"mixture\"] = {modality: torch.from_numpy(mixture).to(torch.float32)}\n",
    "\n",
    "    if sources is not None:\n",
    "        out[\"sources\"] = {k: {modality: torch.from_numpy(v).to(torch.float32)} for k, v in sources.items()}\n",
    "\n",
    "    if query is not None:\n",
    "        out[\"query\"] = {modality: torch.from_numpy(query).to(torch.float32)}\n",
    "\n",
    "    if metadata is not None:\n",
    "        out[\"metadata\"] = metadata\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class SimpleishNamespace(SimpleNamespace):\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        kwargs_ = kwargs.copy()\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, dict):\n",
    "                v = SimpleishNamespace(**v)\n",
    "\n",
    "            kwargs_[k] = v\n",
    "\n",
    "        super().__init__(**kwargs_)\n",
    "\n",
    "    def copy(self) -> \"SimpleishNamespace\":\n",
    "        return SimpleishNamespace(**{k: v for k, v in self.__dict__.items()})\n",
    "\n",
    "    def add_subnamespace(self, name: str, **kwargs: Any) -> None:\n",
    "        if hasattr(self, name):\n",
    "            raise ValueError(f\"Namespace already has attribute {name}\")\n",
    "\n",
    "        setattr(self, name, SimpleishNamespace(**kwargs))\n",
    "\n",
    "    def keys(self):\n",
    "        return self.__dict__.keys()\n",
    "\n",
    "    def __getitem__(self, key: str) -> Any:\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key: str, value: Any) -> None:\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def items(self):\n",
    "        return self.__dict__.items()\n",
    "\n",
    "\n",
    "class BatchedInputOutput(SimpleishNamespace):\n",
    "    mixture: torch.Tensor\n",
    "    sources: Dict[str, torch.Tensor]\n",
    "    estimates: Optional[Dict[str, torch.Tensor]]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> \"BatchedInputOutput\":\n",
    "        return cls(**d)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return self.__dict__\n",
    "\n",
    "\n",
    "class TensorCollection(SimpleishNamespace):\n",
    "    def __init__(self, **kwargs: torch.Tensor) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def apply(self, func: Any, *args: Any, **kwargs: Any) -> \"TensorCollection\":\n",
    "        return TensorCollection(\n",
    "            **{k: func(v, *args, **kwargs) for k, v in self.__dict__.items()}\n",
    "        )\n",
    "\n",
    "    def as_stacked_tensor(self, dim: int = 0) -> torch.Tensor:\n",
    "        return torch.stack(list(self.__dict__.values()), dim=dim)\n",
    "\n",
    "    def as_concatenated_tensor(self, dim: int = 0) -> torch.Tensor:\n",
    "        return torch.cat(list(self.__dict__.values()), dim=dim)\n",
    "\n",
    "    def __getitem__(self, key: str) -> torch.Tensor:\n",
    "        return self.__dict__[key]\n",
    "\n",
    "\n",
    "InputType = BatchedInputOutput\n",
    "OutputType = BatchedInputOutput\n",
    "LossOutputType = Any\n",
    "MetricOutputType = Any\n",
    "\n",
    "ModelType = nn.Module\n",
    "OptimizerType = optim.Optimizer\n",
    "SchedulerType = optim.lr_scheduler._LRScheduler\n",
    "MetricType = tm.Metric\n",
    "LossType = nn.Module\n",
    "\n",
    "OptimizationBundle = Any\n",
    "\n",
    "LossHandler = Any\n",
    "MetricHandler = Any\n",
    "AugmentationHandler = Any\n",
    "InferenceHandler = Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **End to end base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseEndToEndModule()\n",
      "BaseEndToEndModule\n",
      "__main__\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#from audiocraft.models import encodec\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from ...types import (\n",
    "#     BatchedInputOutput,\n",
    "#     InputType,\n",
    "#     OperationMode,\n",
    "#     OutputType,\n",
    "#     SimpleishNamespace,\n",
    "#     TensorCollection\n",
    "# )\n",
    "\n",
    "import torchaudio as ta\n",
    "\n",
    "\n",
    "class BaseEndToEndModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = BaseEndToEndModule()\n",
    "    print(model)\n",
    "    print(model.__class__.__name__)\n",
    "    print(model.__module__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Querier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PaSST\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "from hear21passt.base import get_basic_model\n",
    "from torch import nn\n",
    "\n",
    "class Passt(nn.Module):\n",
    "\n",
    "    PASST_EMB_DIM: int = 768\n",
    "    PASST_FS: int = 32000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_fs: int=44100,\n",
    "        passt_fs: int=PASST_FS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.passt = get_basic_model(mode=\"embed_only\", arch=\"openmic\").eval()\n",
    "        self.resample = ta.transforms.Resample(\n",
    "            orig_freq=original_fs, new_freq=passt_fs\n",
    "        ).eval()\n",
    "\n",
    "        for p in self.passt.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the PasstWrapper model.\n",
    "\n",
    "        Args:\n",
    "            qspec (torch.Tensor): Query spectrogram.\n",
    "            qaudio (torch.Tensor): Query audio.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedding output.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.mean(x, dim=1)\n",
    "            x = self.resample(x)\n",
    "\n",
    "            specs = self.passt.mel(x)[..., :998]\n",
    "            specs = specs[:, None, ...]\n",
    "            _, z = self.passt.net(specs)\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class PasstWrapper(nn.Module):\n",
    "\n",
    "    PASST_EMB_DIM: int = 768\n",
    "    PASST_FS: int = 32000\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cond_emb_dim: int = 384,\n",
    "        original_cond_emb_dim=PASST_EMB_DIM,\n",
    "        original_fs: int=44100,\n",
    "        passt_fs: int=PASST_FS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cond_emb_dim = cond_emb_dim\n",
    "\n",
    "        self.passt = get_basic_model(mode=\"embed_only\", arch=\"openmic\").eval()\n",
    "        self.proj = nn.Linear(original_cond_emb_dim, cond_emb_dim) if cond_emb_dim is not None else nn.Identity()\n",
    "        self.resample = ta.transforms.Resample(\n",
    "            orig_freq=original_fs, new_freq=passt_fs\n",
    "        ).eval()\n",
    "\n",
    "        for p in self.passt.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, qspec, qaudio):\n",
    "        \"\"\"\n",
    "        Forward pass of the PasstWrapper model.\n",
    "\n",
    "        Args:\n",
    "            qspec (torch.Tensor): Query spectrogram.\n",
    "            qaudio (torch.Tensor): Query audio.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedding output.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.mean(qaudio, dim=1)\n",
    "            x = self.resample(x)\n",
    "\n",
    "            specs = self.passt.mel(x)[..., :998]\n",
    "            specs = specs[:, None, ...]\n",
    "            _, z = self.passt.net(specs)\n",
    "\n",
    "        z = self.proj(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bandit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "from abc import abstractmethod\n",
    "from typing import Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from librosa import hz_to_midi, midi_to_hz\n",
    "from torch import Tensor\n",
    "from torchaudio import functional as taF\n",
    "# from spafe.fbanks import bark_fbanks\n",
    "# from spafe.utils.converters import erb2hz, hz2bark, hz2erb\n",
    "from torchaudio.functional.functional import _create_triangular_filterbank\n",
    "\n",
    "\n",
    "def band_widths_from_specs(band_specs):\n",
    "    return [e - i for i, e in band_specs]\n",
    "\n",
    "\n",
    "def check_nonzero_bandwidth(band_specs):\n",
    "    # pprint(band_specs)\n",
    "    for fstart, fend in band_specs:\n",
    "        if fend - fstart <= 0:\n",
    "            raise ValueError(\"Bands cannot be zero-width\")\n",
    "\n",
    "\n",
    "def check_no_overlap(band_specs):\n",
    "    fend_prev = -1\n",
    "    for fstart_curr, fend_curr in band_specs:\n",
    "        if fstart_curr <= fend_prev:\n",
    "            raise ValueError(\"Bands cannot overlap\")\n",
    "\n",
    "\n",
    "def check_no_gap(band_specs):\n",
    "    fstart, _ = band_specs[0]\n",
    "    assert fstart == 0\n",
    "\n",
    "    fend_prev = -1\n",
    "    for fstart_curr, fend_curr in band_specs:\n",
    "        if fstart_curr - fend_prev > 1:\n",
    "            raise ValueError(\"Bands cannot leave gap\")\n",
    "        fend_prev = fend_curr\n",
    "\n",
    "\n",
    "class BandsplitSpecification:\n",
    "    def __init__(self, nfft: int, fs: int) -> None:\n",
    "        self.fs = fs\n",
    "        self.nfft = nfft\n",
    "        self.nyquist = fs / 2\n",
    "        self.max_index = nfft // 2 + 1\n",
    "\n",
    "        self.split500 = self.hertz_to_index(500)\n",
    "        self.split1k = self.hertz_to_index(1000)\n",
    "        self.split2k = self.hertz_to_index(2000)\n",
    "        self.split4k = self.hertz_to_index(4000)\n",
    "        self.split8k = self.hertz_to_index(8000)\n",
    "        self.split16k = self.hertz_to_index(16000)\n",
    "        self.split20k = self.hertz_to_index(20000)\n",
    "\n",
    "        self.above20k = [(self.split20k, self.max_index)]\n",
    "        self.above16k = [(self.split16k, self.split20k)] + self.above20k\n",
    "\n",
    "    def index_to_hertz(self, index: int):\n",
    "        return index * self.fs / self.nfft\n",
    "\n",
    "    def hertz_to_index(self, hz: float, round: bool = True):\n",
    "        index = hz * self.nfft / self.fs\n",
    "\n",
    "        if round:\n",
    "            index = int(np.round(index))\n",
    "\n",
    "        return index\n",
    "\n",
    "    def get_band_specs_with_bandwidth(\n",
    "            self,\n",
    "            start_index,\n",
    "            end_index,\n",
    "            bandwidth_hz\n",
    "            ):\n",
    "        band_specs = []\n",
    "        lower = start_index\n",
    "\n",
    "        while lower < end_index:\n",
    "            upper = int(np.floor(lower + self.hertz_to_index(bandwidth_hz)))\n",
    "            upper = min(upper, end_index)\n",
    "\n",
    "            band_specs.append((lower, upper))\n",
    "            lower = upper\n",
    "\n",
    "        return band_specs\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_band_specs(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class VocalBandsplitSpecification(BandsplitSpecification):\n",
    "    def __init__(self, nfft: int, fs: int, version: str = \"7\") -> None:\n",
    "        super().__init__(nfft=nfft, fs=fs)\n",
    "\n",
    "        self.version = version\n",
    "\n",
    "    def get_band_specs(self):\n",
    "        return getattr(self, f\"version{self.version}\")()\n",
    "\n",
    "    @property\n",
    "    def version1(self):\n",
    "        return self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.max_index, bandwidth_hz=1000\n",
    "        )\n",
    "\n",
    "    def version2(self):\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split16k, bandwidth_hz=1000\n",
    "        )\n",
    "        below20k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split16k,\n",
    "                end_index=self.split20k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "\n",
    "        return below16k + below20k + self.above20k\n",
    "\n",
    "    def version3(self):\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split8k, bandwidth_hz=1000\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "\n",
    "        return below8k + below16k + self.above16k\n",
    "\n",
    "    def version4(self):\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split1k, bandwidth_hz=100\n",
    "        )\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split8k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "\n",
    "        return below1k + below8k + below16k + self.above16k\n",
    "\n",
    "    def version5(self):\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split1k, bandwidth_hz=100\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        below20k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split16k,\n",
    "                end_index=self.split20k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "        return below1k + below16k + below20k + self.above20k\n",
    "\n",
    "    def version6(self):\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split1k, bandwidth_hz=100\n",
    "        )\n",
    "        below4k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split4k,\n",
    "                bandwidth_hz=500\n",
    "        )\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split4k,\n",
    "                end_index=self.split8k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "        return below1k + below4k + below8k + below16k + self.above16k\n",
    "\n",
    "    def version7(self):\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split1k, bandwidth_hz=100\n",
    "        )\n",
    "        below4k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split4k,\n",
    "                bandwidth_hz=250\n",
    "        )\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split4k,\n",
    "                end_index=self.split8k,\n",
    "                bandwidth_hz=500\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        below20k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split16k,\n",
    "                end_index=self.split20k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "        return below1k + below4k + below8k + below16k + below20k + self.above20k\n",
    "\n",
    "\n",
    "class OtherBandsplitSpecification(VocalBandsplitSpecification):\n",
    "    def __init__(self, nfft: int, fs: int) -> None:\n",
    "        super().__init__(nfft=nfft, fs=fs, version=\"7\")\n",
    "\n",
    "\n",
    "class BassBandsplitSpecification(BandsplitSpecification):\n",
    "    def __init__(self, nfft: int, fs: int, version: str = \"7\") -> None:\n",
    "        super().__init__(nfft=nfft, fs=fs)\n",
    "\n",
    "    def get_band_specs(self):\n",
    "        below500 = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split500, bandwidth_hz=50\n",
    "        )\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split500,\n",
    "                end_index=self.split1k,\n",
    "                bandwidth_hz=100\n",
    "        )\n",
    "        below4k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split4k,\n",
    "                bandwidth_hz=500\n",
    "        )\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split4k,\n",
    "                end_index=self.split8k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=2000\n",
    "        )\n",
    "        above16k = [(self.split16k, self.max_index)]\n",
    "\n",
    "        return below500 + below1k + below4k + below8k + below16k + above16k\n",
    "\n",
    "\n",
    "class DrumBandsplitSpecification(BandsplitSpecification):\n",
    "    def __init__(self, nfft: int, fs: int) -> None:\n",
    "        super().__init__(nfft=nfft, fs=fs)\n",
    "\n",
    "    def get_band_specs(self):\n",
    "        below1k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=0, end_index=self.split1k, bandwidth_hz=50\n",
    "        )\n",
    "        below2k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split1k,\n",
    "                end_index=self.split2k,\n",
    "                bandwidth_hz=100\n",
    "        )\n",
    "        below4k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split2k,\n",
    "                end_index=self.split4k,\n",
    "                bandwidth_hz=250\n",
    "        )\n",
    "        below8k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split4k,\n",
    "                end_index=self.split8k,\n",
    "                bandwidth_hz=500\n",
    "        )\n",
    "        below16k = self.get_band_specs_with_bandwidth(\n",
    "                start_index=self.split8k,\n",
    "                end_index=self.split16k,\n",
    "                bandwidth_hz=1000\n",
    "        )\n",
    "        above16k = [(self.split16k, self.max_index)]\n",
    "\n",
    "        return below1k + below2k + below4k + below8k + below16k + above16k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerceptualBandsplitSpecification(BandsplitSpecification):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nfft: int,\n",
    "            fs: int,\n",
    "            fbank_fn: Callable[[int, int, float, float, int], torch.Tensor],\n",
    "            n_bands: int,\n",
    "            f_min: float = 0.0,\n",
    "            f_max: float = None\n",
    "    ) -> None:\n",
    "        super().__init__(nfft=nfft, fs=fs)\n",
    "        self.n_bands = n_bands\n",
    "        if f_max is None:\n",
    "            f_max = fs / 2\n",
    "\n",
    "        self.filterbank = fbank_fn(\n",
    "                n_bands, fs, f_min, f_max, self.max_index\n",
    "        )\n",
    "\n",
    "        weight_per_bin = torch.sum(\n",
    "            self.filterbank,\n",
    "            dim=0,\n",
    "            keepdim=True\n",
    "            )  # (1, n_freqs)\n",
    "        normalized_mel_fb = self.filterbank / weight_per_bin  # (n_mels, n_freqs)\n",
    "\n",
    "        freq_weights = []\n",
    "        band_specs = []\n",
    "        for i in range(self.n_bands):\n",
    "            active_bins = torch.nonzero(self.filterbank[i, :]).squeeze().tolist()\n",
    "            if isinstance(active_bins, int):\n",
    "                active_bins = (active_bins, active_bins)\n",
    "            if len(active_bins) == 0:\n",
    "                continue\n",
    "            start_index = active_bins[0]\n",
    "            end_index = active_bins[-1] + 1\n",
    "            band_specs.append((start_index, end_index))\n",
    "            freq_weights.append(normalized_mel_fb[i, start_index:end_index])\n",
    "\n",
    "        self.freq_weights = freq_weights\n",
    "        self.band_specs = band_specs\n",
    "\n",
    "    def get_band_specs(self):\n",
    "        return self.band_specs\n",
    "\n",
    "    def get_freq_weights(self):\n",
    "        return self.freq_weights\n",
    "\n",
    "    def save_to_file(self, dir_path: str) -> None:\n",
    "\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        with open(os.path.join(dir_path, \"mel_bandsplit_spec.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                    {\n",
    "                            \"band_specs\": self.band_specs,\n",
    "                            \"freq_weights\": self.freq_weights,\n",
    "                            \"filterbank\": self.filterbank,\n",
    "                    },\n",
    "                    f,\n",
    "            )\n",
    "\n",
    "def mel_filterbank(n_bands, fs, f_min, f_max, n_freqs):\n",
    "    fb = taF.melscale_fbanks(\n",
    "                n_mels=n_bands,\n",
    "                sample_rate=fs,\n",
    "                f_min=f_min,\n",
    "                f_max=f_max,\n",
    "                n_freqs=n_freqs,\n",
    "        ).T\n",
    "\n",
    "    fb[0, 0] = 1.0\n",
    "\n",
    "    return fb\n",
    "\n",
    "\n",
    "class MelBandsplitSpecification(PerceptualBandsplitSpecification):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nfft: int,\n",
    "            fs: int,\n",
    "            n_bands: int,\n",
    "            f_min: float = 0.0,\n",
    "            f_max: float = None\n",
    "    ) -> None:\n",
    "        super().__init__(fbank_fn=mel_filterbank, nfft=nfft, fs=fs, n_bands=n_bands, f_min=f_min, f_max=f_max)\n",
    "\n",
    "def musical_filterbank(n_bands, fs, f_min, f_max, n_freqs,\n",
    "                       scale=\"constant\"):\n",
    "\n",
    "    nfft = 2 * (n_freqs - 1)\n",
    "    df = fs / nfft\n",
    "    # init freqs\n",
    "    f_max = f_max or fs / 2\n",
    "    f_min = f_min or 0\n",
    "    f_min = fs / nfft\n",
    "\n",
    "    n_octaves = np.log2(f_max / f_min)\n",
    "    n_octaves_per_band = n_octaves / n_bands\n",
    "    bandwidth_mult = np.power(2.0, n_octaves_per_band)\n",
    "\n",
    "    low_midi = max(0, hz_to_midi(f_min))\n",
    "    high_midi = hz_to_midi(f_max)\n",
    "    midi_points = np.linspace(low_midi, high_midi, n_bands)\n",
    "    hz_pts = midi_to_hz(midi_points)\n",
    "\n",
    "    low_pts = hz_pts / bandwidth_mult\n",
    "    high_pts = hz_pts * bandwidth_mult\n",
    "\n",
    "    low_bins = np.floor(low_pts / df).astype(int)\n",
    "    high_bins = np.ceil(high_pts / df).astype(int)\n",
    "\n",
    "    fb = np.zeros((n_bands, n_freqs))\n",
    "\n",
    "    for i in range(n_bands):\n",
    "        fb[i, low_bins[i]:high_bins[i]+1] = 1.0\n",
    "\n",
    "    fb[0, :low_bins[0]] = 1.0\n",
    "    fb[-1, high_bins[-1]+1:] = 1.0\n",
    "\n",
    "    return torch.as_tensor(fb)\n",
    "\n",
    "class MusicalBandsplitSpecification(PerceptualBandsplitSpecification):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nfft: int,\n",
    "            fs: int,\n",
    "            n_bands: int,\n",
    "            f_min: float = 0.0,\n",
    "            f_max: float = None\n",
    "    ) -> None:\n",
    "        super().__init__(fbank_fn=musical_filterbank, nfft=nfft, fs=fs, n_bands=n_bands, f_min=f_min, f_max=f_max)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    band_defs = []\n",
    "\n",
    "    for bands in [VocalBandsplitSpecification]:  \n",
    "        band_name = bands.__name__.replace(\"BandsplitSpecification\", \"\")\n",
    "\n",
    "        mbs = bands(nfft=2048, fs=44100).get_band_specs()\n",
    "\n",
    "        for i, (f_min, f_max) in enumerate(mbs):\n",
    "            band_defs.append({\n",
    "                \"band\": band_name,\n",
    "                \"band_index\": i,\n",
    "                \"f_min\": f_min,\n",
    "                \"f_max\": f_max\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(band_defs)\n",
    "    df.to_csv(\"vox7bands.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Commented this example input as memory is not sufficient on my disc'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Frequency Model\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import rnn\n",
    "\n",
    "import torch.backends.cuda\n",
    "\n",
    "\n",
    "class TimeFrequencyModellingModule(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "class ResidualRNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            rnn_dim: int,\n",
    "            bidirectional: bool = True,\n",
    "            rnn_type: str = \"LSTM\",\n",
    "            use_batch_trick: bool = True,\n",
    "            use_layer_norm: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        if use_layer_norm:\n",
    "            self.norm = nn.LayerNorm(emb_dim)\n",
    "        else:\n",
    "            self.norm = nn.GroupNorm(num_groups=emb_dim, num_channels=emb_dim)\n",
    "\n",
    "        self.rnn = rnn.__dict__[rnn_type](\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=rnn_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=rnn_dim * (2 if bidirectional else 1),\n",
    "            out_features=emb_dim\n",
    "        )\n",
    "\n",
    "        self.use_batch_trick = use_batch_trick\n",
    "        if not self.use_batch_trick:\n",
    "            warnings.warn(\"NOT USING BATCH TRICK IS EXTREMELY SLOW!!\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z = (batch, n_uncrossed, n_across, emb_dim)\n",
    "\n",
    "        z0 = torch.clone(z)\n",
    "\n",
    "        if self.use_layer_norm:\n",
    "            z = self.norm(z)  # (batch, n_uncrossed, n_across, emb_dim)\n",
    "        else:\n",
    "            z = torch.permute(\n",
    "                z, (0, 3, 1, 2)\n",
    "            )  # (batch, emb_dim, n_uncrossed, n_across)\n",
    "\n",
    "            z = self.norm(z)  # (batch, emb_dim, n_uncrossed, n_across)\n",
    "\n",
    "            z = torch.permute(\n",
    "                z, (0, 2, 3, 1)\n",
    "            )  # (batch, n_uncrossed, n_across, emb_dim)\n",
    "\n",
    "        batch, n_uncrossed, n_across, emb_dim = z.shape\n",
    "\n",
    "        if self.use_batch_trick:\n",
    "            z = torch.reshape(z, (batch * n_uncrossed, n_across, emb_dim))\n",
    "            z = self.rnn(z.contiguous())[0]  # (batch * n_uncrossed, n_across, dir_rnn_dim)\n",
    "\n",
    "            z = torch.reshape(z, (batch, n_uncrossed, n_across, -1))\n",
    "            # (batch, n_uncrossed, n_across, dir_rnn_dim)\n",
    "        else:\n",
    "            # Note: this is EXTREMELY SLOW\n",
    "            zlist = []\n",
    "            for i in range(n_uncrossed):\n",
    "                zi = self.rnn(z[:, i, :, :])[0]  # (batch, n_across, emb_dim)\n",
    "                zlist.append(zi)\n",
    "\n",
    "            z = torch.stack(\n",
    "                zlist,\n",
    "                dim=1\n",
    "            )  # (batch, n_uncrossed, n_across, dir_rnn_dim)\n",
    "\n",
    "        z = self.fc(z)  # (batch, n_uncrossed, n_across, emb_dim)\n",
    "\n",
    "        z = z + z0\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class SeqBandModellingModule(TimeFrequencyModellingModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_modules: int = 12,\n",
    "            emb_dim: int = 128,\n",
    "            rnn_dim: int = 256,\n",
    "            bidirectional: bool = True,\n",
    "            rnn_type: str = \"LSTM\",\n",
    "            parallel_mode=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.seqband = nn.ModuleList([])\n",
    "\n",
    "        if parallel_mode:\n",
    "            for _ in range(n_modules):\n",
    "                self.seqband.append(\n",
    "                    nn.ModuleList(\n",
    "                        [ResidualRNN(\n",
    "                            emb_dim=emb_dim,\n",
    "                            rnn_dim=rnn_dim,\n",
    "                            bidirectional=bidirectional,\n",
    "                            rnn_type=rnn_type,\n",
    "                        ),\n",
    "                            ResidualRNN(\n",
    "                                emb_dim=emb_dim,\n",
    "                                rnn_dim=rnn_dim,\n",
    "                                bidirectional=bidirectional,\n",
    "                                rnn_type=rnn_type,\n",
    "                            )]\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "\n",
    "            for _ in range(2 * n_modules):\n",
    "                self.seqband.append(\n",
    "                    ResidualRNN(\n",
    "                        emb_dim=emb_dim,\n",
    "                        rnn_dim=rnn_dim,\n",
    "                        bidirectional=bidirectional,\n",
    "                        rnn_type=rnn_type,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.parallel_mode = parallel_mode\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z = (batch, n_bands, n_time, emb_dim)\n",
    "\n",
    "        if self.parallel_mode:\n",
    "            for sbm_pair in self.seqband:\n",
    "                # z: (batch, n_bands, n_time, emb_dim)\n",
    "                sbm_t, sbm_f = sbm_pair[0], sbm_pair[1]\n",
    "                zt = sbm_t(z)  # (batch, n_bands, n_time, emb_dim)\n",
    "                zf = sbm_f(z.transpose(1, 2))  # (batch, n_time, n_bands, emb_dim)\n",
    "                z = zt + zf.transpose(1, 2)\n",
    "        else:\n",
    "            for sbm in self.seqband:\n",
    "                z = sbm(z)\n",
    "                z = z.transpose(1, 2)\n",
    "\n",
    "                # (batch, n_bands, n_time, emb_dim)\n",
    "                #   --> (batch, n_time, n_bands, emb_dim)\n",
    "                # OR\n",
    "                # (batch, n_time, n_bands, emb_dim)\n",
    "                #   --> (batch, n_bands, n_time, emb_dim)\n",
    "\n",
    "        q = z\n",
    "        return q  # (batch, n_bands, n_time, emb_dim)\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example of moving model and data to the correct device\n",
    "model = SeqBandModellingModule().to(device)\n",
    "\n",
    "# # Example data tensor\n",
    "# input_data = torch.randn(32, 64, 128, 128).to(device)  # Adjust dimensions as needed\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(input_data)\n",
    "\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "'''Commented this example input as memory is not sufficient on my disc'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask estimation\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "\n",
    "# from core.models.e2e.bandit.utils import (\n",
    "#     band_widths_from_specs,\n",
    "#     check_no_gap,\n",
    "#     check_no_overlap,\n",
    "#     check_nonzero_bandwidth,\n",
    "# )\n",
    "\n",
    "\n",
    "class BaseNormMLP(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            mlp_dim: int,\n",
    "            bandwidth: int,\n",
    "            in_channel: Optional[int],\n",
    "            hidden_activation: str = \"Tanh\",\n",
    "            hidden_activation_kwargs=None,\n",
    "            complex_mask: bool = True, ):\n",
    "\n",
    "        super().__init__()\n",
    "        if hidden_activation_kwargs is None:\n",
    "            hidden_activation_kwargs = {}\n",
    "        self.hidden_activation_kwargs = hidden_activation_kwargs\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.hidden = torch.jit.script(nn.Sequential(\n",
    "                nn.Linear(in_features=emb_dim, out_features=mlp_dim),\n",
    "                activation.__dict__[hidden_activation](\n",
    "                        **self.hidden_activation_kwargs\n",
    "                ),\n",
    "        ))\n",
    "\n",
    "        self.bandwidth = bandwidth\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "        self.complex_mask = complex_mask\n",
    "        self.reim = 2 if complex_mask else 1\n",
    "        self.glu_mult = 2\n",
    "\n",
    "\n",
    "class NormMLP(BaseNormMLP):\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            mlp_dim: int,\n",
    "            bandwidth: int,\n",
    "            in_channel: Optional[int],\n",
    "            hidden_activation: str = \"Tanh\",\n",
    "            hidden_activation_kwargs=None,\n",
    "            complex_mask: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "                emb_dim=emb_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                bandwidth=bandwidth,\n",
    "                in_channel=in_channel,\n",
    "                hidden_activation=hidden_activation,\n",
    "                hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "                complex_mask=complex_mask,\n",
    "        )\n",
    "\n",
    "        self.output = torch.jit.script(\n",
    "                nn.Sequential(\n",
    "                        nn.Linear(\n",
    "                                in_features=mlp_dim,\n",
    "                                out_features=bandwidth * in_channel * self.reim * 2,\n",
    "                        ),\n",
    "                        nn.GLU(dim=-1),\n",
    "                )\n",
    "        )\n",
    "\n",
    "    def reshape_output(self, mb):\n",
    "        # print(mb.shape)\n",
    "        batch, n_time, _ = mb.shape\n",
    "        if self.complex_mask:\n",
    "            mb = mb.reshape(\n",
    "                    batch,\n",
    "                    n_time,\n",
    "                    self.in_channel,\n",
    "                    self.bandwidth,\n",
    "                    self.reim\n",
    "            ).contiguous()\n",
    "            # print(mb.shape)\n",
    "            mb = torch.view_as_complex(\n",
    "                    mb\n",
    "            )  # (batch, n_time, in_channel, bandwidth)\n",
    "        else:\n",
    "            mb = mb.reshape(batch, n_time, self.in_channel, self.bandwidth)\n",
    "\n",
    "        mb = torch.permute(\n",
    "                mb,\n",
    "                (0, 2, 3, 1)\n",
    "        )  # (batch, in_channel, bandwidth, n_time)\n",
    "\n",
    "        return mb\n",
    "\n",
    "    def forward(self, qb):\n",
    "        # qb = (batch, n_time, emb_dim)\n",
    "\n",
    "        # if torch.any(torch.isnan(qb)):\n",
    "        #     raise ValueError(\"qb0\")\n",
    "\n",
    "\n",
    "        qb = self.norm(qb)  # (batch, n_time, emb_dim)\n",
    "\n",
    "        # if torch.any(torch.isnan(qb)):\n",
    "        #     raise ValueError(\"qb1\")\n",
    "\n",
    "        qb = self.hidden(qb)  # (batch, n_time, mlp_dim)\n",
    "        # if torch.any(torch.isnan(qb)):\n",
    "        #     raise ValueError(\"qb2\")\n",
    "        mb = self.output(qb)  # (batch, n_time, bandwidth * in_channel * reim)\n",
    "        # if torch.any(torch.isnan(qb)):\n",
    "        #     raise ValueError(\"mb\")\n",
    "        mb = self.reshape_output(mb)  # (batch, in_channel, bandwidth, n_time)\n",
    "\n",
    "        return mb\n",
    "\n",
    "\n",
    "# class MultAddNormMLP(NormMLP):\n",
    "#     def __init__(self, emb_dim: int, mlp_dim: int, bandwidth: int, in_channel: int | None, hidden_activation: str = \"Tanh\", hidden_activation_kwargs=None, complex_mask: bool = True) -> None:\n",
    "#         super().__init__(emb_dim, mlp_dim, bandwidth, in_channel, hidden_activation, hidden_activation_kwargs, complex_mask)\n",
    "\n",
    "#         self.output2 = torch.jit.script(\n",
    "#                 nn.Sequential(\n",
    "#                         nn.Linear(\n",
    "#                                 in_features=mlp_dim,\n",
    "#                                 out_features=bandwidth * in_channel * self.reim * 2,\n",
    "#                         ),\n",
    "#                         nn.GLU(dim=-1),\n",
    "#                 )\n",
    "#         )\n",
    "\n",
    "#     def forward(self, qb):\n",
    "\n",
    "#         qb = self.norm(qb)  # (batch, n_time, emb_dim)\n",
    "#         qb = self.hidden(qb)  # (batch, n_time, mlp_dim)\n",
    "#         mmb = self.output(qb)  # (batch, n_time, bandwidth * in_channel * reim)\n",
    "#         mmb = self.reshape_output(mmb)  # (batch, in_channel, bandwidth, n_time)\n",
    "#         amb = self.output2(qb)  # (batch, n_time, bandwidth * in_channel * reim)\n",
    "#         amb = self.reshape_output(amb)  # (batch, in_channel, bandwidth, n_time)\n",
    "\n",
    "#         return mmb, amb\n",
    "\n",
    "\n",
    "class MaskEstimationModuleSuperBase(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MaskEstimationModuleBase(MaskEstimationModuleSuperBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            band_specs: List[Tuple[float, float]],\n",
    "            emb_dim: int,\n",
    "            mlp_dim: int,\n",
    "            in_channel: Optional[int],\n",
    "            hidden_activation: str = \"Tanh\",\n",
    "            hidden_activation_kwargs: Dict = None,\n",
    "            complex_mask: bool = True,\n",
    "            norm_mlp_cls: Type[nn.Module] = NormMLP,\n",
    "            norm_mlp_kwargs: Dict = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.band_widths = band_widths_from_specs(band_specs)\n",
    "        self.n_bands = len(band_specs)\n",
    "\n",
    "        if hidden_activation_kwargs is None:\n",
    "            hidden_activation_kwargs = {}\n",
    "\n",
    "        if norm_mlp_kwargs is None:\n",
    "            norm_mlp_kwargs = {}\n",
    "\n",
    "        self.norm_mlp = nn.ModuleList(\n",
    "                [\n",
    "                        (\n",
    "                                norm_mlp_cls(\n",
    "                                        bandwidth=self.band_widths[b],\n",
    "                                        emb_dim=emb_dim,\n",
    "                                        mlp_dim=mlp_dim,\n",
    "                                        in_channel=in_channel,\n",
    "                                        hidden_activation=hidden_activation,\n",
    "                                        hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "                                        complex_mask=complex_mask,\n",
    "                                        **norm_mlp_kwargs,\n",
    "                                )\n",
    "                        )\n",
    "                        for b in range(self.n_bands)\n",
    "                ]\n",
    "        )\n",
    "\n",
    "    def compute_masks(self, q):\n",
    "        batch, n_bands, n_time, emb_dim = q.shape\n",
    "\n",
    "        masks = []\n",
    "\n",
    "        for b, nmlp in enumerate(self.norm_mlp):\n",
    "            # print(f\"maskestim/{b:02d}\")\n",
    "            qb = q[:, b, :, :]\n",
    "            mb = nmlp(qb)\n",
    "            masks.append(mb)\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "\n",
    "class OverlappingMaskEstimationModule(MaskEstimationModuleBase):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel: int,\n",
    "            band_specs: List[Tuple[float, float]],\n",
    "            freq_weights: List[torch.Tensor],\n",
    "            n_freq: int,\n",
    "            emb_dim: int,\n",
    "            mlp_dim: int,\n",
    "            cond_dim: int = 0,\n",
    "            hidden_activation: str = \"Tanh\",\n",
    "            hidden_activation_kwargs: Dict = None,\n",
    "            complex_mask: bool = True,\n",
    "            norm_mlp_cls: Type[nn.Module] = NormMLP,\n",
    "            norm_mlp_kwargs: Dict = None,\n",
    "            use_freq_weights: bool = True,\n",
    "    ) -> None:\n",
    "        check_nonzero_bandwidth(band_specs)\n",
    "        check_no_gap(band_specs)\n",
    "\n",
    "        # if cond_dim > 0:\n",
    "        #     raise NotImplementedError\n",
    "\n",
    "        super().__init__(\n",
    "                band_specs=band_specs,\n",
    "                emb_dim=emb_dim + cond_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                in_channel=in_channel,\n",
    "                hidden_activation=hidden_activation,\n",
    "                hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "                complex_mask=complex_mask,\n",
    "                norm_mlp_cls=norm_mlp_cls,\n",
    "                norm_mlp_kwargs=norm_mlp_kwargs,\n",
    "        )\n",
    "\n",
    "        self.n_freq = n_freq\n",
    "        self.band_specs = band_specs\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "        if freq_weights is not None:\n",
    "            for i, fw in enumerate(freq_weights):\n",
    "                self.register_buffer(f\"freq_weights/{i}\", fw)\n",
    "\n",
    "                self.use_freq_weights = use_freq_weights\n",
    "        else:\n",
    "            self.use_freq_weights = False\n",
    "\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "    def forward(self, q, cond=None):\n",
    "        # q = (batch, n_bands, n_time, emb_dim)\n",
    "\n",
    "        batch, n_bands, n_time, emb_dim = q.shape\n",
    "\n",
    "        if cond is not None:\n",
    "            print(cond)\n",
    "            if cond.ndim == 2:\n",
    "                cond = cond[:, None, None, :].expand(-1, n_bands, n_time, -1)\n",
    "            elif cond.ndim == 3:\n",
    "                assert cond.shape[1] == n_time\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid cond shape: {cond.shape}\")\n",
    "\n",
    "            q = torch.cat([q, cond], dim=-1)\n",
    "        elif self.cond_dim > 0:\n",
    "            cond = torch.ones(\n",
    "                    (batch, n_bands, n_time, self.cond_dim),\n",
    "                    device=q.device,\n",
    "                    dtype=q.dtype,\n",
    "            )\n",
    "            q = torch.cat([q, cond], dim=-1)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        mask_list = self.compute_masks(\n",
    "                q\n",
    "        )  # [n_bands  * (batch, in_channel, bandwidth, n_time)]\n",
    "\n",
    "        masks = torch.zeros(\n",
    "                (batch, self.in_channel, self.n_freq, n_time),\n",
    "                device=q.device,\n",
    "                dtype=mask_list[0].dtype,\n",
    "        )\n",
    "\n",
    "        for im, mask in enumerate(mask_list):\n",
    "            fstart, fend = self.band_specs[im]\n",
    "            if self.use_freq_weights:\n",
    "                fw = self.get_buffer(f\"freq_weights/{im}\")[:, None]\n",
    "                mask = mask * fw\n",
    "            masks[:, :, fstart:fend, :] += mask\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "class MaskEstimationModule(OverlappingMaskEstimationModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            band_specs: List[Tuple[float, float]],\n",
    "            emb_dim: int,\n",
    "            mlp_dim: int,\n",
    "            in_channel: Optional[int],\n",
    "            hidden_activation: str = \"Tanh\",\n",
    "            hidden_activation_kwargs: Dict = None,\n",
    "            complex_mask: bool = True,\n",
    "            **kwargs,\n",
    "    ) -> None:\n",
    "        check_nonzero_bandwidth(band_specs)\n",
    "        check_no_gap(band_specs)\n",
    "        check_no_overlap(band_specs)\n",
    "        super().__init__(\n",
    "                in_channel=in_channel,\n",
    "                band_specs=band_specs,\n",
    "                freq_weights=None,\n",
    "                n_freq=None,\n",
    "                emb_dim=emb_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                hidden_activation=hidden_activation,\n",
    "                hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "                complex_mask=complex_mask,\n",
    "        )\n",
    "\n",
    "    def forward(self, q, cond=None):\n",
    "        # q = (batch, n_bands, n_time, emb_dim)\n",
    "\n",
    "        masks = self.compute_masks(\n",
    "                q\n",
    "        )  # [n_bands  * (batch, in_channel, bandwidth, n_time)]\n",
    "\n",
    "        # TODO: currently this requires band specs to have no gap and no overlap\n",
    "        masks = torch.concat(\n",
    "                masks,\n",
    "                dim=2\n",
    "        )  # (batch, in_channel, n_freq, n_time)\n",
    "\n",
    "        return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band split\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NormFC(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            bandwidth: int,\n",
    "            in_channel: int,\n",
    "            normalize_channel_independently: bool = False,\n",
    "            treat_channel_as_feature: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.treat_channel_as_feature = treat_channel_as_feature\n",
    "\n",
    "        if normalize_channel_independently:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        reim = 2\n",
    "\n",
    "        self.norm = nn.LayerNorm(in_channel * bandwidth * reim)\n",
    "\n",
    "        fc_in = bandwidth * reim\n",
    "\n",
    "        if treat_channel_as_feature:\n",
    "            fc_in *= in_channel\n",
    "        else:\n",
    "            assert emb_dim % in_channel == 0\n",
    "            emb_dim = emb_dim // in_channel\n",
    "\n",
    "        self.fc = nn.Linear(fc_in, emb_dim)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # xb = (batch, n_time, in_chan, reim * band_width)\n",
    "\n",
    "        batch, n_time, in_chan, ribw = xb.shape\n",
    "        xb = self.norm(xb.reshape(batch, n_time, in_chan * ribw))\n",
    "        # (batch, n_time, in_chan * reim * band_width)\n",
    "\n",
    "        if not self.treat_channel_as_feature:\n",
    "            xb = xb.reshape(batch, n_time, in_chan, ribw)\n",
    "            # (batch, n_time, in_chan, reim * band_width)\n",
    "\n",
    "        zb = self.fc(xb)\n",
    "        # (batch, n_time, emb_dim)\n",
    "        # OR\n",
    "        # (batch, n_time, in_chan, emb_dim_per_chan)\n",
    "\n",
    "        if not self.treat_channel_as_feature:\n",
    "            batch, n_time, in_chan, emb_dim_per_chan = zb.shape\n",
    "            # (batch, n_time, in_chan, emb_dim_per_chan)\n",
    "            zb = zb.reshape((batch, n_time, in_chan * emb_dim_per_chan))\n",
    "\n",
    "        return zb  # (batch, n_time, emb_dim)\n",
    "\n",
    "\n",
    "class BandSplitModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            band_specs: List[Tuple[float, float]],\n",
    "            emb_dim: int,\n",
    "            in_channel: int,\n",
    "            require_no_overlap: bool = False,\n",
    "            require_no_gap: bool = True,\n",
    "            normalize_channel_independently: bool = False,\n",
    "            treat_channel_as_feature: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        check_nonzero_bandwidth(band_specs)\n",
    "\n",
    "        if require_no_gap:\n",
    "            check_no_gap(band_specs)\n",
    "\n",
    "        if require_no_overlap:\n",
    "            check_no_overlap(band_specs)\n",
    "\n",
    "        self.band_specs = band_specs\n",
    "        # list of [fstart, fend) in index.\n",
    "        # Note that fend is exclusive.\n",
    "        self.band_widths = band_widths_from_specs(band_specs)\n",
    "        self.n_bands = len(band_specs)\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.norm_fc_modules = nn.ModuleList(\n",
    "                [  # type: ignore\n",
    "                        (\n",
    "                                NormFC(\n",
    "                                        emb_dim=emb_dim,\n",
    "                                        bandwidth=bw,\n",
    "                                        in_channel=in_channel,\n",
    "                                        normalize_channel_independently=normalize_channel_independently,\n",
    "                                        treat_channel_as_feature=treat_channel_as_feature,\n",
    "                                )\n",
    "                        )\n",
    "                        for bw in self.band_widths\n",
    "                ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x = complex spectrogram (batch, in_chan, n_freq, n_time)\n",
    "\n",
    "        batch, in_chan, _, n_time = x.shape\n",
    "\n",
    "        z = torch.zeros(\n",
    "            size=(batch, self.n_bands, n_time, self.emb_dim),\n",
    "            device=x.device\n",
    "        )\n",
    "\n",
    "        xr = torch.view_as_real(x)  # batch, in_chan, n_freq, n_time, 2\n",
    "        xr = torch.permute(\n",
    "            xr,\n",
    "            (0, 3, 1, 4, 2)\n",
    "            )  # batch, n_time, in_chan, 2, n_freq\n",
    "        batch, n_time, in_chan, reim, band_width = xr.shape\n",
    "        for i, nfm in enumerate(self.norm_fc_modules):\n",
    "            # print(f\"bandsplit/band{i:02d}\")\n",
    "            fstart, fend = self.band_specs[i]\n",
    "            xb = xr[..., fstart:fend]\n",
    "            # (batch, n_time, in_chan, reim, band_width)\n",
    "            xb = torch.reshape(xb, (batch, n_time, in_chan, -1))\n",
    "            # (batch, n_time, in_chan, reim * band_width)\n",
    "            # z.append(nfm(xb))  # (batch, n_time, emb_dim)\n",
    "            z[:, i, :, :] = nfm(xb.contiguous())\n",
    "\n",
    "        # z = torch.stack(z, dim=1)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandit\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from torch import Tensor, nn\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "\n",
    "\n",
    "\n",
    "class BaseBandit(BaseEndToEndModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        band_type: str = \"musical\",\n",
    "        n_bands: int = 64,\n",
    "        require_no_overlap: bool = False,\n",
    "        require_no_gap: bool = True,\n",
    "        normalize_channel_independently: bool = False,\n",
    "        treat_channel_as_feature: bool = True,\n",
    "        n_sqm_modules: int = 12,\n",
    "        emb_dim: int = 128,\n",
    "        rnn_dim: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        n_fft: int = 2048,\n",
    "        win_length: Optional[int] = 2048,\n",
    "        hop_length: int = 512,\n",
    "        window_fn: str = \"hann_window\",\n",
    "        wkwargs: Optional[Dict] = None,\n",
    "        power: Optional[int] = None,\n",
    "        center: bool = True,\n",
    "        normalized: bool = True,\n",
    "        pad_mode: str = \"constant\",\n",
    "        onesided: bool = True,\n",
    "        fs: int = 44100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.instantitate_spectral(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            power=power,\n",
    "            normalized=normalized,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            onesided=onesided,\n",
    "        )\n",
    "\n",
    "        self.instantiate_bandsplit(\n",
    "            in_channel=in_channel,\n",
    "            band_type=band_type,\n",
    "            n_bands=n_bands,\n",
    "            require_no_overlap=require_no_overlap,\n",
    "            require_no_gap=require_no_gap,\n",
    "            normalize_channel_independently=normalize_channel_independently,\n",
    "            treat_channel_as_feature=treat_channel_as_feature,\n",
    "            emb_dim=emb_dim,\n",
    "            n_fft=n_fft,\n",
    "            fs=fs,\n",
    "        )\n",
    "\n",
    "        self.instantiate_tf_modelling(\n",
    "            n_sqm_modules=n_sqm_modules,\n",
    "            emb_dim=emb_dim,\n",
    "            rnn_dim=rnn_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type,\n",
    "        )\n",
    "\n",
    "    def instantitate_spectral(\n",
    "        self,\n",
    "        n_fft: int = 2048,\n",
    "        win_length: Optional[int] = 2048,\n",
    "        hop_length: int = 512,\n",
    "        window_fn: str = \"hann_window\",\n",
    "        wkwargs: Optional[Dict] = None,\n",
    "        power: Optional[int] = None,\n",
    "        normalized: bool = True,\n",
    "        center: bool = True,\n",
    "        pad_mode: str = \"constant\",\n",
    "        onesided: bool = True,\n",
    "    ):\n",
    "\n",
    "        assert power is None\n",
    "\n",
    "        window_fn = torch.__dict__[window_fn]\n",
    "\n",
    "        self.stft = ta.transforms.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            pad_mode=pad_mode,\n",
    "            pad=0,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            power=power,\n",
    "            normalized=normalized,\n",
    "            center=center,\n",
    "            onesided=onesided,\n",
    "        )\n",
    "\n",
    "        self.istft = ta.transforms.InverseSpectrogram(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            pad_mode=pad_mode,\n",
    "            pad=0,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            normalized=normalized,\n",
    "            center=center,\n",
    "            onesided=onesided,\n",
    "        )\n",
    "\n",
    "    def instantiate_bandsplit(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        band_type: str = \"musical\",\n",
    "        n_bands: int = 64,\n",
    "        require_no_overlap: bool = False,\n",
    "        require_no_gap: bool = True,\n",
    "        normalize_channel_independently: bool = False,\n",
    "        treat_channel_as_feature: bool = True,\n",
    "        emb_dim: int = 128,\n",
    "        n_fft: int = 2048,\n",
    "        fs: int = 44100,\n",
    "    ):\n",
    "\n",
    "        assert band_type == \"musical\"\n",
    "\n",
    "        self.band_specs = MusicalBandsplitSpecification(\n",
    "            nfft=n_fft, fs=fs, n_bands=n_bands\n",
    "        )\n",
    "\n",
    "        self.band_split = BandSplitModule(\n",
    "            in_channel=in_channel,\n",
    "            band_specs=self.band_specs.get_band_specs(),\n",
    "            require_no_overlap=require_no_overlap,\n",
    "            require_no_gap=require_no_gap,\n",
    "            normalize_channel_independently=normalize_channel_independently,\n",
    "            treat_channel_as_feature=treat_channel_as_feature,\n",
    "            emb_dim=emb_dim,\n",
    "        )\n",
    "\n",
    "    def instantiate_tf_modelling(\n",
    "        self,\n",
    "        n_sqm_modules: int = 12,\n",
    "        emb_dim: int = 128,\n",
    "        rnn_dim: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "    ):\n",
    "        self.tf_model = SeqBandModellingModule(\n",
    "            n_modules=n_sqm_modules,\n",
    "            emb_dim=emb_dim,\n",
    "            rnn_dim=rnn_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type,\n",
    "        )\n",
    "\n",
    "    def mask(self, x, m):\n",
    "        return x * m\n",
    "\n",
    "    def forward(self, batch: InputType, mode: OperationMode = OperationMode.TRAIN):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.stft(batch.mixture.audio)\n",
    "            batch.mixture.spectrogram = x\n",
    "\n",
    "            if \"sources\" in batch.keys():\n",
    "                for stem in batch.sources.keys():\n",
    "                    s = batch.sources[stem].audio\n",
    "                    s = self.stft(s)\n",
    "                    batch.sources[stem].spectrogram = s\n",
    "\n",
    "        batch = self.separate(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def encode(self, batch):\n",
    "        x = batch.mixture.spectrogram\n",
    "        length = batch.mixture.audio.shape[-1]\n",
    "\n",
    "        z = self.band_split(x)  # (batch, emb_dim, n_band, n_time)\n",
    "        q = self.tf_model(z)  # (batch, emb_dim, n_band, n_time)\n",
    "\n",
    "        return x, q, length\n",
    "\n",
    "    def separate(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Bandit(BaseBandit):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        stems: List[str],\n",
    "        band_type: str = \"musical\",\n",
    "        n_bands: int = 64,\n",
    "        require_no_overlap: bool = False,\n",
    "        require_no_gap: bool = True,\n",
    "        normalize_channel_independently: bool = False,\n",
    "        treat_channel_as_feature: bool = True,\n",
    "        n_sqm_modules: int = 12,\n",
    "        emb_dim: int = 128,\n",
    "        rnn_dim: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        mlp_dim: int = 512,\n",
    "        hidden_activation: str = \"Tanh\",\n",
    "        hidden_activation_kwargs: Dict | None = None,\n",
    "        complex_mask: bool = True,\n",
    "        use_freq_weights: bool = True,\n",
    "        n_fft: int = 2048,\n",
    "        win_length: int | None = 2048,\n",
    "        hop_length: int = 512,\n",
    "        window_fn: str = \"hann_window\",\n",
    "        wkwargs: Dict | None = None,\n",
    "        power: int | None = None,\n",
    "        center: bool = True,\n",
    "        normalized: bool = True,\n",
    "        pad_mode: str = \"constant\",\n",
    "        onesided: bool = True,\n",
    "        fs: int = 44100,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channel=in_channel,\n",
    "            band_type=band_type,\n",
    "            n_bands=n_bands,\n",
    "            require_no_overlap=require_no_overlap,\n",
    "            require_no_gap=require_no_gap,\n",
    "            normalize_channel_independently=normalize_channel_independently,\n",
    "            treat_channel_as_feature=treat_channel_as_feature,\n",
    "            n_sqm_modules=n_sqm_modules,\n",
    "            emb_dim=emb_dim,\n",
    "            rnn_dim=rnn_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            power=power,\n",
    "            center=center,\n",
    "            normalized=normalized,\n",
    "            pad_mode=pad_mode,\n",
    "            onesided=onesided,\n",
    "            fs=fs,\n",
    "        )\n",
    "\n",
    "        self.instantiate_mask_estim(\n",
    "            in_channel=in_channel,\n",
    "            stems=stems,\n",
    "            emb_dim=emb_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            hidden_activation=hidden_activation,\n",
    "            hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "            complex_mask=complex_mask,\n",
    "            n_freq=n_fft // 2 + 1,\n",
    "            use_freq_weights=use_freq_weights,\n",
    "        )\n",
    "\n",
    "    def instantiate_mask_estim(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        stems: List[str],\n",
    "        emb_dim: int,\n",
    "        mlp_dim: int,\n",
    "        hidden_activation: str,\n",
    "        hidden_activation_kwargs: Optional[Dict] = None,\n",
    "        complex_mask: bool = True,\n",
    "        n_freq: Optional[int] = None,\n",
    "        use_freq_weights: bool = True,\n",
    "    ):\n",
    "        if hidden_activation_kwargs is None:\n",
    "            hidden_activation_kwargs = {}\n",
    "\n",
    "        assert n_freq is not None\n",
    "\n",
    "        self.mask_estim = nn.ModuleDict(\n",
    "            {\n",
    "                stem: OverlappingMaskEstimationModule(\n",
    "                    band_specs=self.band_specs.get_band_specs(),\n",
    "                    freq_weights=self.band_specs.get_freq_weights(),\n",
    "                    n_freq=n_freq,\n",
    "                    emb_dim=emb_dim,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    in_channel=in_channel,\n",
    "                    hidden_activation=hidden_activation,\n",
    "                    hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "                    complex_mask=complex_mask,\n",
    "                    use_freq_weights=use_freq_weights,\n",
    "                )\n",
    "                for stem in stems\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def separate(self, batch):\n",
    "\n",
    "        x, q, length = self.encode(batch)\n",
    "\n",
    "        for stem, mem in self.mask_estim.items():\n",
    "            m = mem(q)\n",
    "            s = self.mask(x, m)\n",
    "            s = torch.reshape(s, x.shape)\n",
    "            batch.estimates[stem] = SimpleishNamespace(\n",
    "                audio=self.istft(s, length), spectrogram=s\n",
    "            )\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class BaseConditionedBandit(BaseBandit):\n",
    "    query_encoder: nn.Module\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        band_type: str = \"musical\",\n",
    "        n_bands: int = 64,\n",
    "        require_no_overlap: bool = False,\n",
    "        require_no_gap: bool = True,\n",
    "        normalize_channel_independently: bool = False,\n",
    "        treat_channel_as_feature: bool = True,\n",
    "        n_sqm_modules: int = 12,\n",
    "        emb_dim: int = 128,\n",
    "        rnn_dim: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        mlp_dim: int = 512,\n",
    "        hidden_activation: str = \"Tanh\",\n",
    "        hidden_activation_kwargs: Dict | None = None,\n",
    "        complex_mask: bool = True,\n",
    "        use_freq_weights: bool = True,\n",
    "        n_fft: int = 2048,\n",
    "        win_length: int | None = 2048,\n",
    "        hop_length: int = 512,\n",
    "        window_fn: str = \"hann_window\",\n",
    "        wkwargs: Dict | None = None,\n",
    "        power: int | None = None,\n",
    "        center: bool = True,\n",
    "        normalized: bool = True,\n",
    "        pad_mode: str = \"constant\",\n",
    "        onesided: bool = True,\n",
    "        fs: int = 44100,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channel=in_channel,\n",
    "            band_type=band_type,\n",
    "            n_bands=n_bands,\n",
    "            require_no_overlap=require_no_overlap,\n",
    "            require_no_gap=require_no_gap,\n",
    "            normalize_channel_independently=normalize_channel_independently,\n",
    "            treat_channel_as_feature=treat_channel_as_feature,\n",
    "            n_sqm_modules=n_sqm_modules,\n",
    "            emb_dim=emb_dim,\n",
    "            rnn_dim=rnn_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            power=power,\n",
    "            center=center,\n",
    "            normalized=normalized,\n",
    "            pad_mode=pad_mode,\n",
    "            onesided=onesided,\n",
    "            fs=fs,\n",
    "        )\n",
    "\n",
    "        self.instantiate_mask_estim(\n",
    "            in_channel=in_channel,\n",
    "            emb_dim=emb_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            hidden_activation=hidden_activation,\n",
    "            hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "            complex_mask=complex_mask,\n",
    "            n_freq=n_fft // 2 + 1,\n",
    "            use_freq_weights=use_freq_weights,\n",
    "        )\n",
    "\n",
    "    def instantiate_mask_estim(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        emb_dim: int,\n",
    "        mlp_dim: int,\n",
    "        hidden_activation: str,\n",
    "        hidden_activation_kwargs: Optional[Dict] = None,\n",
    "        complex_mask: bool = True,\n",
    "        n_freq: Optional[int] = None,\n",
    "        use_freq_weights: bool = True,\n",
    "    ):\n",
    "        if hidden_activation_kwargs is None:\n",
    "            hidden_activation_kwargs = {}\n",
    "\n",
    "        assert n_freq is not None\n",
    "\n",
    "        self.mask_estim = OverlappingMaskEstimationModule(\n",
    "            band_specs=self.band_specs.get_band_specs(),\n",
    "            freq_weights=self.band_specs.get_freq_weights(),\n",
    "            n_freq=n_freq,\n",
    "            emb_dim=emb_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            in_channel=in_channel,\n",
    "            hidden_activation=hidden_activation,\n",
    "            hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "            complex_mask=complex_mask,\n",
    "            use_freq_weights=use_freq_weights,\n",
    "        )\n",
    "\n",
    "    def separate(self, batch):\n",
    "\n",
    "        x, q, length = self.encode(batch)\n",
    "\n",
    "        q = self.adapt_query(q, batch)\n",
    "\n",
    "        m = self.mask_estim(q)\n",
    "        s = self.mask(x, m)\n",
    "        s = torch.reshape(s, x.shape)\n",
    "        batch.estimates[\"target\"] = SimpleishNamespace(\n",
    "            audio=self.istft(s, length), spectrogram=s\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def adapt_query(self, q, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class PasstFiLMConditionedBandit(BaseConditionedBandit):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel: int,\n",
    "        band_type: str = \"musical\",\n",
    "        n_bands: int = 64,\n",
    "        additive_film: bool = True,\n",
    "        multiplicative_film: bool = True,\n",
    "        film_depth: int = 2,\n",
    "        require_no_overlap: bool = False,\n",
    "        require_no_gap: bool = True,\n",
    "        normalize_channel_independently: bool = False,\n",
    "        treat_channel_as_feature: bool = True,\n",
    "        n_sqm_modules: int = 12,\n",
    "        emb_dim: int = 128,\n",
    "        rnn_dim: int = 256,\n",
    "        bidirectional: bool = True,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        mlp_dim: int = 512,\n",
    "        hidden_activation: str = \"Tanh\",\n",
    "        hidden_activation_kwargs: Dict | None = None,\n",
    "        complex_mask: bool = True,\n",
    "        use_freq_weights: bool = True,\n",
    "        n_fft: int = 2048,\n",
    "        win_length: int | None = 2048,\n",
    "        hop_length: int = 512,\n",
    "        window_fn: str = \"hann_window\",\n",
    "        wkwargs: Dict | None = None,\n",
    "        power: int | None = None,\n",
    "        center: bool = True,\n",
    "        normalized: bool = True,\n",
    "        pad_mode: str = \"constant\",\n",
    "        onesided: bool = True,\n",
    "        fs: int = 44100,\n",
    "        pretrain_encoder = None,\n",
    "        freeze_encoder = False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channel=in_channel,\n",
    "            band_type=band_type,\n",
    "            n_bands=n_bands,\n",
    "            require_no_overlap=require_no_overlap,\n",
    "            require_no_gap=require_no_gap,\n",
    "            normalize_channel_independently=normalize_channel_independently,\n",
    "            treat_channel_as_feature=treat_channel_as_feature,\n",
    "            n_sqm_modules=n_sqm_modules,\n",
    "            emb_dim=emb_dim,\n",
    "            rnn_dim=rnn_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            rnn_type=rnn_type,\n",
    "            mlp_dim=mlp_dim,\n",
    "            hidden_activation=hidden_activation,\n",
    "            hidden_activation_kwargs=hidden_activation_kwargs,\n",
    "            complex_mask=complex_mask,\n",
    "            use_freq_weights=use_freq_weights,\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_length,\n",
    "            hop_length=hop_length,\n",
    "            window_fn=window_fn,\n",
    "            wkwargs=wkwargs,\n",
    "            power=power,\n",
    "            center=center,\n",
    "            normalized=normalized,\n",
    "            pad_mode=pad_mode,\n",
    "            onesided=onesided,\n",
    "            fs=fs,\n",
    "        )\n",
    "\n",
    "        self.query_encoder = Passt(\n",
    "            original_fs=fs,\n",
    "            passt_fs=32000,\n",
    "        )\n",
    "        \n",
    "        self.film = FiLM(\n",
    "            self.query_encoder.PASST_EMB_DIM,\n",
    "            emb_dim,\n",
    "            additive=additive_film,\n",
    "            multiplicative=multiplicative_film,\n",
    "            depth=film_depth,\n",
    "        )\n",
    "        \n",
    "        if pretrain_encoder is not None:\n",
    "            self.load_pretrained_encoder(pretrain_encoder)\n",
    "            \n",
    "            for p in self.band_split.parameters():\n",
    "                p.requires_grad = not freeze_encoder\n",
    "                \n",
    "            for p in self.tf_model.parameters():\n",
    "                p.requires_grad = not freeze_encoder\n",
    "            \n",
    "        \n",
    "        \n",
    "    def load_pretrained_encoder(self, path):\n",
    "        \n",
    "        state_dict = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        \n",
    "        state_dict_ = {k.replace(\"model.\", \"\") if k.startswith(\"model.\") else k: v for k, v in state_dict.items()}\n",
    "        \n",
    "        state_dict = {}\n",
    "\n",
    "        for k, v in state_dict_.items():\n",
    "            if \"mask_estim\" in k:\n",
    "                continue\n",
    "            \n",
    "            if \"tf_seqband\" in k:\n",
    "                k = k.replace(\"tf_seqband\", \"tf_model.seqband\")\n",
    "            \n",
    "            state_dict[k] = v\n",
    "            \n",
    "        \n",
    "        res = self.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        for k in res.unexpected_keys:\n",
    "            if \"mask_estim\" in k:\n",
    "                continue\n",
    "            print(f\"Unexpected key: {k}\")\n",
    "        \n",
    "        for k in res.missing_keys:\n",
    "            print(f\"Missing key: {k}\")\n",
    "            for kw in [\"band_split\", \"tf_model\"]:\n",
    "                if kw in k:\n",
    "                    raise ValueError(f\"Missing key: {k}\")\n",
    "                \n",
    "            for kw in [\"mask_estim\", \"query_encoder\"]:\n",
    "                if kw in k:\n",
    "                    continue\n",
    "            \n",
    "\n",
    "\n",
    "    def adapt_query(self, q, batch):\n",
    "        \n",
    "        w = self.query_encoder(batch.query.audio)\n",
    "        q = torch.permute(q, (0, 3, 1, 2)) # (batch, n_band, n_time, emb_dim) -> (batch, emb_dim, n_band, n_time)\n",
    "        q = self.film(q, w)\n",
    "        q = torch.permute(q, (0, 2, 3, 1)) # -> (batch, n_band, n_time, emb_dim)\n",
    "        \n",
    "        return q\n",
    "\n",
    "\n",
    "    def optimized_forward(self, batch: InputType, mode: OperationMode = OperationMode.TRAIN):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.stft(batch.mixture.audio)\n",
    "            batch.mixture.spectrogram = x\n",
    "\n",
    "            if \"sources\" in batch.keys():\n",
    "                for stem in batch.sources.keys():\n",
    "                    s = batch.sources[stem].audio\n",
    "                    s = self.stft(s)\n",
    "                    batch.sources[stem].spectrogram = s\n",
    "\n",
    "        batch = self.optimized_separate(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def optimized_separate(self, batch):\n",
    "\n",
    "        x, q, length = self.encode(batch)\n",
    "\n",
    "        for stem, query in batch.query.items():\n",
    "\n",
    "            batch_ = SimpleishNamespace(**batch.__dict__)\n",
    "            batch_.query = query\n",
    "\n",
    "            q = self.adapt_query(q, batch_)\n",
    "\n",
    "            m = self.mask_estim(q)\n",
    "            s = self.mask(x, m)\n",
    "            s = torch.reshape(s, x.shape)\n",
    "            batch.estimates[stem] = SimpleishNamespace(\n",
    "                audio=self.istft(s, length), spectrogram=s\n",
    "            )\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ebase.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Probably\n",
    "import math\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "from itertools import chain, combinations\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, Iterator, Mapping, Optional, Tuple, Type, TypedDict\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "import torchmetrics as tm\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from core.types import BatchedInputOutput, OperationMode, RawInputType, SimpleishNamespace\n",
    "# from core.types import (\n",
    "#     InputType,\n",
    "#     OutputType,\n",
    "#     LossOutputType,\n",
    "#     MetricOutputType,\n",
    "#     ModelType,\n",
    "#     OptimizerType,\n",
    "#     SchedulerType,\n",
    "#     MetricType,\n",
    "#     LossType,\n",
    "#     OptimizationBundle,\n",
    "#     LossHandler,\n",
    "#     MetricHandler,\n",
    "#     AugmentationHandler,\n",
    "#     InferenceHandler,\n",
    "# )\n",
    "\n",
    "\n",
    "class EndToEndLightningSystem(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: ModelType,\n",
    "        loss_handler: LossHandler,\n",
    "        metrics: MetricHandler,\n",
    "        augmentation_handler: AugmentationHandler,\n",
    "        inference_handler: InferenceHandler,\n",
    "        optimization_bundle: OptimizationBundle,\n",
    "        fast_run: bool = False,\n",
    "        commitment_weight: float = 1.0,\n",
    "        batch_size: Optional[int] = None,\n",
    "        effective_batch_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.loss = loss_handler\n",
    "\n",
    "        self.metrics = metrics\n",
    "        self.optimization = optimization_bundle\n",
    "        self.augmentation = augmentation_handler\n",
    "        self.inference = inference_handler\n",
    "\n",
    "        self.fast_run = fast_run\n",
    "\n",
    "        self.model.fast_run = fast_run\n",
    "\n",
    "        self.commitment_weight = commitment_weight\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.effective_batch_size = effective_batch_size if effective_batch_size is not None else batch_size\n",
    "        self.accum_ratio = self.effective_batch_size // self.batch_size if self.effective_batch_size is not None else 1\n",
    "\n",
    "        self.output_dir = None\n",
    "        self.split_size = None\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = self.optimization.optimizer.cls(\n",
    "            self.model.parameters(),\n",
    "            **self.optimization.optimizer.kwargs\n",
    "        )\n",
    "\n",
    "        ret = {\n",
    "            \"optimizer\": optimizer,\n",
    "        }\n",
    "\n",
    "        if self.optimization.scheduler is not None:\n",
    "            scheduler = self.optimization.scheduler.cls(\n",
    "                optimizer,\n",
    "                **self.optimization.scheduler.kwargs\n",
    "            )\n",
    "            ret[\"lr_scheduler\"] = scheduler\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        batch: BatchedInputOutput,\n",
    "        mode=OperationMode.TRAIN\n",
    "    ) -> LossOutputType:\n",
    "        loss_dict = self.loss(batch)\n",
    "        return loss_dict\n",
    "\n",
    "    # TODO: move to a metric handler\n",
    "    def update_metrics(\n",
    "        self,\n",
    "        batch: BatchedInputOutput,\n",
    "        mode: OperationMode = OperationMode.TRAIN,\n",
    "    ) -> None:\n",
    "        metrics: MetricType = self.metrics.get_mode(mode)\n",
    "\n",
    "        for stem, metric in metrics.items():\n",
    "            if stem not in batch.estimates.keys():\n",
    "                continue\n",
    "            metric.update(batch)\n",
    "\n",
    "    # TODO: move to a metric handler\n",
    "    def compute_metrics(self, mode: OperationMode) -> MetricOutputType:\n",
    "        metrics: MetricType = self.metrics.get_mode(mode)\n",
    "\n",
    "        metric_dict = {}\n",
    "\n",
    "        for stem, metric in metrics.items():\n",
    "            md = metric.compute()\n",
    "            metric_dict.update({f\"{stem}/{k}\": v for k, v in md.items()})\n",
    "\n",
    "        self.log_dict(metric_dict, prog_bar=True, logger=False)\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    # TODO: move to a metric handler\n",
    "    def reset_metrics(self, mode: OperationMode) -> None:\n",
    "        metrics: MetricType = self.metrics.get_mode(mode)\n",
    "\n",
    "        for _, metric in metrics.items():\n",
    "            metric.reset()\n",
    "\n",
    "    def forward(self, batch: RawInputType) -> Tuple[InputType, OutputType]:\n",
    "        batch = self.model(batch)\n",
    "        return batch\n",
    "\n",
    "    def common_step(\n",
    "        self, batch: RawInputType, mode: OperationMode, batch_idx: int = -1\n",
    "    ) -> Tuple[OutputType, LossOutputType]:\n",
    "        batch = BatchedInputOutput.from_dict(batch)\n",
    "        batch = self.forward(batch)\n",
    "\n",
    "        loss_dict = self.compute_loss(batch, mode=mode)\n",
    "\n",
    "        if not self.fast_run:\n",
    "            with torch.no_grad():\n",
    "                self.update_metrics(batch, mode=mode)\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def training_step(self, batch: RawInputType, batch_idx: int) -> LossOutputType:\n",
    "        # augmented_batch = self.augmentation(batch, mode=OperationMode.TRAIN)\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        loss_dict = self.common_step(batch, mode=OperationMode.TRAIN, batch_idx=batch_idx)\n",
    "\n",
    "        self.log_dict_with_prefix(loss_dict, prefix=OperationMode.TRAIN, prog_bar=True)\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self, outputs: OutputType, batch: RawInputType, batch_idx: int\n",
    "    ) -> None:\n",
    "\n",
    "        if self.fast_run:\n",
    "            return\n",
    "\n",
    "        if (batch_idx + 1) % self.accum_ratio == 0:\n",
    "            metric_dict = self.compute_metrics(mode=OperationMode.TRAIN)\n",
    "            self.log_dict_with_prefix(metric_dict, prefix=OperationMode.TRAIN)\n",
    "            self.reset_metrics(mode=OperationMode.TRAIN)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def validation_step(\n",
    "        self, batch: RawInputType, batch_idx: int, dataloader_idx: int = 0\n",
    "    ) -> Dict[str, Any]:\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            loss_dict = self.common_step(batch, mode=OperationMode.VAL)\n",
    "\n",
    "        self.log_dict_with_prefix(loss_dict, prefix=OperationMode.VAL)\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def on_validation_epoch_start(self) -> None:\n",
    "        self.reset_metrics(mode=OperationMode.VAL)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        if self.fast_run:\n",
    "            return\n",
    "\n",
    "        metric_dict = self.compute_metrics(mode=OperationMode.VAL)\n",
    "        self.log_dict_with_prefix(\n",
    "            metric_dict, OperationMode.VAL, prog_bar=True, add_dataloader_idx=False\n",
    "        )\n",
    "        self.reset_metrics(mode=OperationMode.VAL)\n",
    "\n",
    "    \n",
    "    def save_to_audio(self, batch: BatchedInputOutput, batch_idx: int) -> None:\n",
    "        \n",
    "        batch_size = batch[\"mixture\"][\"audio\"].shape[0]\n",
    "        \n",
    "        assert batch_size == 1, \"Batch size must be 1 for inference\"\n",
    "        \n",
    "        metadata = batch.metadata\n",
    "        \n",
    "        song_id = metadata[\"mix\"][0]\n",
    "        stem = metadata[\"stem\"][0]\n",
    "        \n",
    "        log_dir = os.path.join(self.logger.log_dir, \"audio\")\n",
    "        \n",
    "        os.makedirs(os.path.join(log_dir, song_id), exist_ok=True)\n",
    "        \n",
    "        audio = batch.estimates[stem][\"audio\"]\n",
    "        \n",
    "        audio = audio.squeeze(0).cpu().numpy()\n",
    "        \n",
    "        audio_path = os.path.join(log_dir, song_id, f\"{stem}.wav\")\n",
    "        \n",
    "        ta.save(audio_path, torch.tensor(audio), self.inference.fs)\n",
    "\n",
    "    def save_vdbo_to_audio(self, batch: BatchedInputOutput, batch_idx: int) -> None:\n",
    "        \n",
    "        batch_size = batch[\"mixture\"][\"audio\"].shape[0]\n",
    "        \n",
    "        assert batch_size == 1, \"Batch size must be 1 for inference\"\n",
    "        \n",
    "        metadata = batch.metadata\n",
    "        \n",
    "        song_id = metadata[\"song_id\"][0]\n",
    "        \n",
    "        log_dir = os.path.join(self.logger.log_dir, \"audio\")\n",
    "        \n",
    "        os.makedirs(os.path.join(log_dir, song_id), exist_ok=True)\n",
    "        \n",
    "        for stem, audio in batch.estimates.items():\n",
    "            audio = audio[\"audio\"]\n",
    "            audio = audio.squeeze(0).cpu().numpy()\n",
    "            \n",
    "            audio_path = os.path.join(log_dir, song_id, f\"{stem}.wav\")\n",
    "            \n",
    "            ta.save(audio_path, torch.tensor(audio), self.inference.fs)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def chunked_inference(\n",
    "        self, batch: RawInputType, batch_idx: int = -1, dataloader_idx: int = 0\n",
    "    ) -> BatchedInputOutput:\n",
    "        batch = BatchedInputOutput.from_dict(batch)\n",
    "        \n",
    "        audio = batch[\"mixture\"][\"audio\"]\n",
    "        \n",
    "        b, c, n_samples = audio.shape\n",
    "        \n",
    "        assert b == 1\n",
    "\n",
    "        fs = self.inference.fs\n",
    "\n",
    "        chunk_size = int(self.inference.chunk_size_seconds * fs)\n",
    "        hop_size = int(self.inference.hop_size_seconds * fs)\n",
    "        \n",
    "        batch_size = self.inference.batch_size\n",
    "        \n",
    "        overlap = chunk_size - hop_size\n",
    "        \n",
    "        scaler = chunk_size / (2 * hop_size)\n",
    "\n",
    "        n_chunks = int(math.ceil(\n",
    "            (n_samples + 4 * overlap - chunk_size) / hop_size\n",
    "        )) + 1\n",
    "        \n",
    "        pad = (n_chunks - 1) * hop_size + chunk_size - n_samples\n",
    "\n",
    "        # print(audio.shape)\n",
    "        audio = F.pad(\n",
    "            audio,\n",
    "            pad=(2 * overlap, 2 * overlap + pad),\n",
    "            mode=\"reflect\"\n",
    "        )\n",
    "        padded_length = audio.shape[-1]\n",
    "        audio = audio.reshape(c, 1, -1, 1)\n",
    "        \n",
    "        chunked_audio = F.unfold(\n",
    "            audio,\n",
    "            kernel_size=(chunk_size, 1), \n",
    "            stride=(hop_size, 1)\n",
    "        ) # (c, chunk_size, n_chunk)\n",
    "\n",
    "        # print(chunked_audio.shape)\n",
    "\n",
    "        chunked_audio = chunked_audio.permute(2, 0, 1).reshape(-1, c, chunk_size)\n",
    "        \n",
    "        n_chunks = chunked_audio.shape[0]\n",
    "        \n",
    "        n_batch = math.ceil(n_chunks / batch_size)\n",
    "\n",
    "        outputs = []\n",
    "        \n",
    "        for i in tqdm(range(n_batch)):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, n_chunks)\n",
    "            \n",
    "            chunked_batch = SimpleishNamespace(\n",
    "                mixture={\n",
    "                    \"audio\": chunked_audio[start:end]\n",
    "                },\n",
    "                query=batch[\"query\"],\n",
    "                estimates=batch[\"estimates\"]\n",
    "            )\n",
    "            \n",
    "            output = self.forward(chunked_batch)\n",
    "            outputs.append(output.estimates[\"target\"][\"audio\"])\n",
    "\n",
    "        output = torch.cat(outputs, dim=0) # (n_chunks, c, chunk_size)\n",
    "        window = torch.hann_window(chunk_size, device=self.device).reshape(1, 1, chunk_size)\n",
    "        output = output * window / scaler\n",
    "\n",
    "        output = torch.permute(output, (1, 2, 0))\n",
    "\n",
    "        output = F.fold(\n",
    "            output,\n",
    "            output_size=(padded_length, 1),\n",
    "            kernel_size=(chunk_size, 1),\n",
    "            stride=(hop_size, 1)\n",
    "        ) # (c, 1, t, 1)\n",
    "\n",
    "        output = output[None, :, 0, 2*overlap: n_samples + 2*overlap, 0]\n",
    "\n",
    "        stem = batch.metadata[\"stem\"][0]\n",
    "\n",
    "        batch[\"estimates\"][stem] = {\n",
    "            \"audio\": output\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def chunked_vdbo_inference(\n",
    "        self, batch: RawInputType, batch_idx: int = -1, dataloader_idx: int = 0\n",
    "    ) -> BatchedInputOutput:\n",
    "        batch = BatchedInputOutput.from_dict(batch)\n",
    "        \n",
    "        audio = batch[\"mixture\"][\"audio\"]\n",
    "        \n",
    "        b, c, n_samples = audio.shape\n",
    "        \n",
    "        assert b == 1\n",
    "\n",
    "        fs = self.inference.fs\n",
    "\n",
    "        chunk_size = int(self.inference.chunk_size_seconds * fs)\n",
    "        hop_size = int(self.inference.hop_size_seconds * fs)\n",
    "        \n",
    "        batch_size = self.inference.batch_size\n",
    "        \n",
    "        overlap = chunk_size - hop_size\n",
    "        \n",
    "        scaler = chunk_size / (2 * hop_size)\n",
    "\n",
    "        n_chunks = int(math.ceil(\n",
    "            (n_samples + 4 * overlap - chunk_size) / hop_size\n",
    "        )) + 1\n",
    "        \n",
    "        pad = (n_chunks - 1) * hop_size + chunk_size - n_samples\n",
    "\n",
    "        # print(audio.shape)\n",
    "        audio = F.pad(\n",
    "            audio,\n",
    "            pad=(2 * overlap, 2 * overlap + pad),\n",
    "            mode=\"reflect\"\n",
    "        )\n",
    "        padded_length = audio.shape[-1]\n",
    "        audio = audio.reshape(c, 1, -1, 1)\n",
    "        \n",
    "        chunked_audio = F.unfold(\n",
    "            audio,\n",
    "            kernel_size=(chunk_size, 1), \n",
    "            stride=(hop_size, 1)\n",
    "        ) # (c, chunk_size, n_chunk)\n",
    "\n",
    "        # print(chunked_audio.shape)\n",
    "\n",
    "        chunked_audio = chunked_audio.permute(2, 0, 1).reshape(-1, c, chunk_size)\n",
    "        \n",
    "        n_chunks = chunked_audio.shape[0]\n",
    "        \n",
    "        n_batch = math.ceil(n_chunks / batch_size)\n",
    "\n",
    "        outputs = defaultdict(list)\n",
    "        \n",
    "        for i in tqdm(range(n_batch)):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, n_chunks)\n",
    "            \n",
    "            chunked_batch = SimpleishNamespace(\n",
    "                mixture={\n",
    "                    \"audio\": chunked_audio[start:end]\n",
    "                },\n",
    "                estimates=batch[\"estimates\"]\n",
    "            )\n",
    "            \n",
    "            output = self.forward(chunked_batch)\n",
    "            \n",
    "            for stem, estimate in output.estimates.items():\n",
    "                outputs[stem].append(estimate[\"audio\"])\n",
    "\n",
    "        for stem, outputs_ in outputs.items():                \n",
    "\n",
    "            output = torch.cat(outputs_, dim=0) # (n_chunks, c, chunk_size)\n",
    "            window = torch.hann_window(chunk_size, device=self.device).reshape(1, 1, chunk_size)\n",
    "            output = output * window / scaler\n",
    "\n",
    "            output = torch.permute(output, (1, 2, 0))\n",
    "\n",
    "            output = F.fold(\n",
    "                output,\n",
    "                output_size=(padded_length, 1),\n",
    "                kernel_size=(chunk_size, 1),\n",
    "                stride=(hop_size, 1)\n",
    "            ) # (c, 1, t, 1)\n",
    "\n",
    "            output = output[None, :, 0, 2*overlap: n_samples + 2*overlap, 0]\n",
    "\n",
    "            batch[\"estimates\"][stem] = {\n",
    "                \"audio\": output\n",
    "            }\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def on_test_epoch_start(self) -> None:\n",
    "        self.reset_metrics(mode=OperationMode.TEST)\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: RawInputType, batch_idx: int, dataloader_idx: int = 0\n",
    "    ) -> Any:\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        if \"query\" in batch.keys():\n",
    "            batch = self.chunked_inference(batch, batch_idx, dataloader_idx)\n",
    "        else:\n",
    "            batch = self.chunked_vdbo_inference(batch, batch_idx, dataloader_idx)\n",
    "        \n",
    "        self.reset_metrics(mode=OperationMode.TEST)\n",
    "        self.update_metrics(batch, mode=OperationMode.TEST)\n",
    "        metrics = self.compute_metrics(mode=OperationMode.TEST)\n",
    "        # metrics[\"song_id\"] = batch.metadata[\"mix\"][0]\n",
    "        self.log_dict_with_prefix(metrics, OperationMode.TEST, \n",
    "                                  on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.reset_metrics(mode=OperationMode.TEST)\n",
    "\n",
    "        # pprint(metrics)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        self.reset_metrics(mode=OperationMode.TEST)\n",
    "\n",
    "    def set_output_path(self, output_dir: str) -> None:\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch: RawInputType, batch_idx: int, dataloader_idx: int = 0\n",
    "    ) -> Any:\n",
    "\n",
    "        self.model.eval()\n",
    "        \n",
    "        if \"query\" in batch.keys():    \n",
    "            batch = self.chunked_inference(batch, batch_idx, dataloader_idx)\n",
    "            \n",
    "            self.save_to_audio(batch, batch_idx)\n",
    "        else:\n",
    "            batch = self.chunked_vdbo_inference(batch, batch_idx, dataloader_idx)\n",
    "            self.save_vdbo_to_audio(batch, batch_idx)\n",
    "\n",
    "    def load_state_dict(\n",
    "        self, state_dict: Mapping[str, Any], strict: bool = False\n",
    "    ) -> Any:\n",
    "        return super().load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def log_dict_with_prefix(\n",
    "        self,\n",
    "        dict_: Dict[str, torch.Tensor],\n",
    "        prefix: str,\n",
    "        batch_size: Optional[int] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "\n",
    "        \n",
    "        self.log_dict(\n",
    "            {f\"{prefix}/{k}\": v for k, v in dict_.items()},\n",
    "            batch_size=batch_size,\n",
    "            logger=True,\n",
    "            sync_dist=True,\n",
    "            **kwargs,\n",
    "            # on_step=True,\n",
    "            # on_epoch=False,\n",
    "        )\n",
    "\n",
    "        self.logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# ***Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base Dataset handlers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Val, Test, Predict Data loaders\n",
    "import inspect\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List, Mapping, Optional, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from torch.utils import data\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "\n",
    "def from_datasets(\n",
    "    train_dataset: Optional[Union[Dataset, Sequence[Dataset], Mapping[str, Dataset]]] = None,\n",
    "    val_dataset: Optional[Union[Dataset, Sequence[Dataset]]] = None,\n",
    "    test_dataset: Optional[Union[Dataset, Sequence[Dataset]]] = None,\n",
    "    predict_dataset: Optional[Union[Dataset, Sequence[Dataset]]] = None,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 0,\n",
    "    **datamodule_kwargs: Any,\n",
    ") -> \"LightningDataModule\":\n",
    "\n",
    "    def dataloader(ds: Dataset, shuffle: bool = False) -> DataLoader:\n",
    "        shuffle &= not isinstance(ds, IterableDataset)\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=4,\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def train_dataloader() -> TRAIN_DATALOADERS:\n",
    "        assert train_dataset\n",
    "\n",
    "        if isinstance(train_dataset, Mapping):\n",
    "            return {key: dataloader(ds, shuffle=True) for key, ds in train_dataset.items()}\n",
    "        if isinstance(train_dataset, Sequence):\n",
    "            return [dataloader(ds, shuffle=True) for ds in train_dataset]\n",
    "        return dataloader(train_dataset, shuffle=True)\n",
    "\n",
    "    def val_dataloader() -> EVAL_DATALOADERS:\n",
    "        assert val_dataset\n",
    "\n",
    "        if isinstance(val_dataset, Sequence):\n",
    "            return [dataloader(ds) for ds in val_dataset]\n",
    "        return dataloader(val_dataset)\n",
    "\n",
    "    def test_dataloader() -> EVAL_DATALOADERS:\n",
    "        assert test_dataset\n",
    "\n",
    "        if isinstance(test_dataset, Sequence):\n",
    "            return [dataloader(ds) for ds in test_dataset]\n",
    "        return dataloader(test_dataset)\n",
    "\n",
    "    def predict_dataloader() -> EVAL_DATALOADERS:\n",
    "        assert predict_dataset\n",
    "\n",
    "        if isinstance(predict_dataset, Sequence):\n",
    "            return [dataloader(ds) for ds in predict_dataset]\n",
    "        return dataloader(predict_dataset)\n",
    "\n",
    "    candidate_kwargs = {\"batch_size\": batch_size, \"num_workers\": num_workers}\n",
    "    accepted_params = inspect.signature(LightningDataModule.__init__).parameters\n",
    "    accepts_kwargs = any(param.kind == param.VAR_KEYWORD for param in accepted_params.values())\n",
    "    if accepts_kwargs:\n",
    "        special_kwargs = candidate_kwargs\n",
    "    else:\n",
    "        accepted_param_names = set(accepted_params)\n",
    "        accepted_param_names.discard(\"self\")\n",
    "        special_kwargs = {k: v for k, v in candidate_kwargs.items() if k in accepted_param_names}\n",
    "\n",
    "    datamodule = LightningDataModule(**datamodule_kwargs, **special_kwargs)\n",
    "    if train_dataset is not None:\n",
    "        datamodule.train_dataloader = train_dataloader  # type: ignore[method-assign]\n",
    "    if val_dataset is not None:\n",
    "        datamodule.val_dataloader = val_dataloader  # type: ignore[method-assign]\n",
    "    if test_dataset is not None:\n",
    "        datamodule.test_dataloader = test_dataloader  # type: ignore[method-assign]\n",
    "    if predict_dataset is not None:\n",
    "        datamodule.predict_dataloader = predict_dataloader  # type: ignore[method-assign]\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "\n",
    "class BaseSourceSeparationDataset(data.Dataset, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        stems: List[str],\n",
    "        files: List[str],\n",
    "        data_path: str,\n",
    "        fs: int,\n",
    "        npy_memmap: bool,\n",
    "        recompute_mixture: bool,\n",
    "    ):\n",
    "        if \"mixture\" not in stems:\n",
    "            stems = [\"mixture\"] + stems\n",
    "\n",
    "        self.split = split\n",
    "        self.stems = stems\n",
    "        self.stems_no_mixture = [s for s in stems if s != \"mixture\"]\n",
    "        self.files = files\n",
    "        self.data_path = data_path\n",
    "        self.fs = fs\n",
    "        self.npy_memmap = npy_memmap\n",
    "        self.recompute_mixture = recompute_mixture\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_stem(self, *, stem: str, identifier: Dict[str, Any]) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_audio(self, stems, identifier: Dict[str, Any]):\n",
    "        audio = {}\n",
    "        for stem in stems:\n",
    "            audio[stem] = self.get_stem(stem=stem, identifier=identifier)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_audio(self, identifier: Dict[str, Any]):\n",
    "        if self.recompute_mixture:\n",
    "            audio = self._get_audio(self.stems_no_mixture, identifier=identifier)\n",
    "            audio[\"mixture\"] = self.compute_mixture(audio)\n",
    "            return audio\n",
    "        else:\n",
    "            return self._get_audio(self.stems, identifier=identifier)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_identifier(self, index: int) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "    def compute_mixture(self, audio) -> torch.Tensor:\n",
    "        return sum(audio[stem] for stem in audio if stem != \"mixture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***MoisesDB***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Taxonomy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxonomy\n",
    "taxonomy = {\n",
    "    \"vocals\": [\n",
    "        \"lead male singer\",\n",
    "        \"lead female singer\",\n",
    "        \"human choir\",\n",
    "        \"background vocals\",\n",
    "        \"other (vocoder, beatboxing etc)\",\n",
    "    ],\n",
    "    \"bass\": [\n",
    "        \"bass guitar\",\n",
    "        \"bass synthesizer (moog etc)\",\n",
    "        \"contrabass/double bass (bass of instrings)\",\n",
    "        \"tuba (bass of brass)\",\n",
    "        \"bassoon (bass of woodwind)\",\n",
    "    ],\n",
    "    \"drums\": [\n",
    "        \"snare drum\",\n",
    "        \"toms\",\n",
    "        \"kick drum\",\n",
    "        \"cymbals\",\n",
    "        \"overheads\",\n",
    "        \"full acoustic drumkit\",\n",
    "        \"drum machine\",\n",
    "    ],\n",
    "    \"other\": [\n",
    "        \"fx/processed sound, scratches, gun shots, explosions etc\",\n",
    "        \"click track\",\n",
    "    ],\n",
    "    \"guitar\": [\n",
    "        \"clean electric guitar\",\n",
    "        \"distorted electric guitar\",\n",
    "        \"lap steel guitar or slide guitar\",\n",
    "        \"acoustic guitar\",\n",
    "    ],\n",
    "    \"other plucked\": [\"banjo, mandolin, ukulele, harp etc\"],\n",
    "    \"percussion\": [\n",
    "        \"a-tonal percussion (claps, shakers, congas, cowbell etc)\",\n",
    "        \"pitched percussion (mallets, glockenspiel, ...)\",\n",
    "    ],\n",
    "    \"piano\": [\n",
    "        \"grand piano\",\n",
    "        \"electric piano (rhodes, wurlitzer, piano sound alike)\",\n",
    "    ],\n",
    "    \"other keys\": [\n",
    "        \"organ, electric organ\",\n",
    "        \"synth pad\",\n",
    "        \"synth lead\",\n",
    "        \"other sounds (hapischord, melotron etc)\",\n",
    "    ],\n",
    "    \"bowed strings\": [\n",
    "        \"violin (solo)\",\n",
    "        \"viola (solo)\",\n",
    "        \"cello (solo)\",\n",
    "        \"violin section\",\n",
    "        \"viola section\",\n",
    "        \"cello section\",\n",
    "        \"string section\",\n",
    "        \"other strings\",\n",
    "    ],\n",
    "    \"wind\": [\n",
    "        \"brass (trumpet, trombone, french horn, brass etc)\",\n",
    "        \"flutes (piccolo, bamboo flute, panpipes, flutes etc)\",\n",
    "        \"reeds (saxophone, clarinets, oboe, english horn, bagpipe)\",\n",
    "        \"other wind\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def clean_track_inst(inst):\n",
    "\n",
    "    if \"fx\" in inst:\n",
    "        inst = \"fx\"\n",
    "\n",
    "    if \"contrabass_double_bass\" in inst:\n",
    "        inst = \"double_bass\"\n",
    "\n",
    "    if \"banjo\" in inst:\n",
    "        return \"other_plucked\"\n",
    "\n",
    "    if \"(\" in inst:\n",
    "        inst = inst.split(\"(\")[0]\n",
    "\n",
    "    for s in [\",\", \"-\"]:\n",
    "        if s in inst:\n",
    "            inst = inst.replace(s, \"\")\n",
    "\n",
    "    for s in [\"/\"]:\n",
    "        if s in inst:\n",
    "            inst = inst.replace(s, \"_\")\n",
    "\n",
    "    if inst[-1] == \"_\":\n",
    "        inst = inst[:-1]\n",
    "\n",
    "    return inst\n",
    "\n",
    "\n",
    "taxonomy = {k: [clean_track_inst(i.replace(\" \", \"_\")) for i in v] for k, v in taxonomy.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main Data Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1363: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1363: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12252\\503925005.py:1363: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  config = \"config\\data\\setup-c\\moisesdb-everything-query-d-aug.yml\"\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12252\\503925005.py:1363: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  config = \"config\\data\\setup-c\\moisesdb-everything-query-d-aug.yml\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning\n",
      "Loaded config\n"
     ]
    },
    {
     "ename": "InterpolationResolutionError",
     "evalue": "KeyError raised while resolving interpolation: \"Environment variable 'DATA_ROOT' not found\"\n    full_key: data_root\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterpolationResolutionError\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1370\u001b[0m\n\u001b[0;32m   1365\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(config)\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1369\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MoisesDBRandomChunkRandomQueryDataset(\n\u001b[1;32m-> 1370\u001b[0m     data_root\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_root\u001b[49m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain_kwargs\n\u001b[0;32m   1371\u001b[0m )\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset)):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\dictconfig.py:359\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_and_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[1;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    225\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcause\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[1;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[0;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[0;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[1;32m--> 899\u001b[0m \u001b[43m_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[1;34m(ex, cause)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_MARKER_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\dictconfig.py:451\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[1;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Node)\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_with_default\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_value\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\basecontainer.py:98\u001b[0m, in \u001b[0;36mBaseContainer._resolve_with_default\u001b[1;34m(self, key, value, default_value)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default_value\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $FULL_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m resolved_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_resolve_interpolation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_value(resolved_node)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:719\u001b[0m, in \u001b[0;36mContainer._maybe_resolve_interpolation\u001b[1;34m(self, parent, key, value, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m    718\u001b[0m parse_tree \u001b[38;5;241m=\u001b[39m parse(_get_value(value))\n\u001b[1;32m--> 719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_interpolation_from_parse_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:584\u001b[0m, in \u001b[0;36mContainer._resolve_interpolation_from_parse_tree\u001b[1;34m(self, parent, value, key, parse_tree, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03mResolve an interpolation.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m    `throw_on_resolution_failure` is `False` and an error occurs during resolution.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_parse_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InterpolationResolutionError:\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_resolution_failure:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:769\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree\u001b[1;34m(self, parse_tree, node, memo, key)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;66;03m# Other kinds of exceptions are wrapped in an `InterpolationResolutionError`.\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InterpolationResolutionError(\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(exc)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m raised while resolving interpolation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m     )\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:764\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree\u001b[1;34m(self, parse_tree, node, memo, key)\u001b[0m\n\u001b[0;32m    758\u001b[0m visitor \u001b[38;5;241m=\u001b[39m GrammarVisitor(\n\u001b[0;32m    759\u001b[0m     node_interpolation_callback\u001b[38;5;241m=\u001b[39mnode_interpolation_callback,\n\u001b[0;32m    760\u001b[0m     resolver_interpolation_callback\u001b[38;5;241m=\u001b[39mresolver_interpolation_callback,\n\u001b[0;32m    761\u001b[0m     memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    762\u001b[0m )\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InterpolationResolutionError:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\antlr4\\tree\\Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[1;34m(self, tree)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar\\gen\\OmegaConfGrammarParser.py:206\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.ConfigValueContext.accept\u001b[1;34m(self, visitor)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitConfigValue\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitConfigValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m visitor\u001b[38;5;241m.\u001b[39mvisitChildren(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar_visitor.py:101\u001b[0m, in \u001b[0;36mGrammarVisitor.visitConfigValue\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisitConfigValue\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx: OmegaConfGrammarParser\u001b[38;5;241m.\u001b[39mConfigValueContext) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# text EOF\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mgetChildCount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetChild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\antlr4\\tree\\Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[1;34m(self, tree)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar\\gen\\OmegaConfGrammarParser.py:342\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.TextContext.accept\u001b[1;34m(self, visitor)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitText\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[1;32m--> 342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m visitor\u001b[38;5;241m.\u001b[39mvisitChildren(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar_visitor.py:301\u001b[0m, in \u001b[0;36mGrammarVisitor.visitText\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisitInterpolation(c)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Otherwise, concatenate string representations together.\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unescape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetChildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar_visitor.py:389\u001b[0m, in \u001b[0;36mGrammarVisitor._unescape\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, OmegaConfGrammarParser\u001b[38;5;241m.\u001b[39mInterpolationContext)\n\u001b[1;32m--> 389\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitInterpolation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    390\u001b[0m     chrs\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chrs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar_visitor.py:125\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolation\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisitInterpolation\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m, ctx: OmegaConfGrammarParser\u001b[38;5;241m.\u001b[39mInterpolationContext\n\u001b[0;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mgetChildCount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# interpolationNode | interpolationResolver\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetChild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\antlr4\\tree\\Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[1;34m(self, tree)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar\\gen\\OmegaConfGrammarParser.py:1041\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.InterpolationResolverContext.accept\u001b[1;34m(self, visitor)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor:ParseTreeVisitor):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitInterpolationResolver\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[1;32m-> 1041\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitInterpolationResolver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m visitor\u001b[38;5;241m.\u001b[39mvisitChildren(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\grammar_visitor.py:179\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolationResolver\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m    176\u001b[0m         args\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[0;32m    177\u001b[0m         args_str\u001b[38;5;241m.\u001b[39mappend(txt)\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver_interpolation_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:750\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree.<locals>.resolver_interpolation_callback\u001b[1;34m(name, args, args_str)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolver_interpolation_callback\u001b[39m(\n\u001b[0;32m    748\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], args_str: Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m    749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_custom_resolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\base.py:694\u001b[0m, in \u001b[0;36mContainer._evaluate_custom_resolver\u001b[1;34m(self, key, node, inter_type, inter_args, inter_args_str)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    693\u001b[0m     root_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_root()\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedInterpolationType(\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported interpolation type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minter_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\omegaconf.py:445\u001b[0m, in \u001b[0;36mOmegaConf.register_new_resolver.<locals>.resolver_wrapper\u001b[1;34m(config, parent, node, args, args_str)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pass_root:\n\u001b[0;32m    443\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_root_\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 445\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mresolver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[0;32m    448\u001b[0m     cache[args_str] \u001b[38;5;241m=\u001b[39m ret\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\demix\\Lib\\site-packages\\omegaconf\\resolvers\\oc\\__init__.py:38\u001b[0m, in \u001b[0;36menv\u001b[1;34m(key, default)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(default) \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mInterpolationResolutionError\u001b[0m: KeyError raised while resolving interpolation: \"Environment variable 'DATA_ROOT' not found\"\n    full_key: data_root\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "# Dataset.py\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from abc import ABC\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_audiomentations.utils.object_dict import ObjectDict\n",
    "import torchaudio as ta\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from core.data.base import BaseSourceSeparationDataset\n",
    "# from core.types import input_dict\n",
    "\n",
    "# from . import clean_track_inst\n",
    "\n",
    "from torch import Tensor, nn\n",
    "\n",
    "DBFS_HOP_SIZE = int(0.125 * 44100)\n",
    "DBFS_CHUNK_SIZE = int(1 * 44100)\n",
    "\n",
    "INST_BY_OCCURRENCE = [\n",
    "    \"bass_guitar\",\n",
    "    \"kick_drum\",\n",
    "    \"snare_drum\",\n",
    "    \"lead_male_singer\",\n",
    "    \"distorted_electric_guitar\",\n",
    "    \"clean_electric_guitar\",\n",
    "    \"toms\",\n",
    "    \"acoustic_guitar\",\n",
    "    \"background_vocals\",\n",
    "    \"hi_hat\",\n",
    "    \"overheads\",\n",
    "    \"atonal_percussion\",\n",
    "    \"grand_piano\",\n",
    "    \"cymbals\",\n",
    "    \"lead_female_singer\",\n",
    "    \"synth_lead\",\n",
    "    \"bass_synthesizer\",\n",
    "    \"synth_pad\",\n",
    "    \"organ_electric_organ\",\n",
    "    \"fx\",\n",
    "    \"drum_machine\",\n",
    "    \"string_section\",\n",
    "    \"electric_piano\",\n",
    "    \"full_acoustic_drumkit\",\n",
    "    \"other_sounds\",\n",
    "    \"pitched_percussion\",\n",
    "    \"brass\",\n",
    "    \"reeds\",\n",
    "    \"contrabass_double_bass\",\n",
    "    \"other_plucked\",\n",
    "    \"other_strings\",\n",
    "    \"other_wind\",\n",
    "    \"cello\",\n",
    "    \"other\",\n",
    "    \"flutes\",\n",
    "    \"viola_section\",\n",
    "    \"viola\",\n",
    "    \"cello_section\",\n",
    "]\n",
    "\n",
    "FINE_LEVEL_INSTRUMENTS = {\n",
    "    \"lead_male_singer\",\n",
    "    \"lead_female_singer\",\n",
    "    \"human_choir\",\n",
    "    \"background_vocals\",\n",
    "    \"other_vocals\",\n",
    "    \"bass_guitar\",\n",
    "    \"bass_synthesizer\",\n",
    "    \"contrabass_double_bass\",\n",
    "    \"tuba\",\n",
    "    \"bassoon\",\n",
    "    \"snare_drum\",\n",
    "    \"toms\",\n",
    "    \"kick_drum\",\n",
    "    \"cymbals\",\n",
    "    \"overheads\",\n",
    "    \"full_acoustic_drumkit\",\n",
    "    \"drum_machine\",\n",
    "    \"hihat\",\n",
    "    \"fx\",\n",
    "    \"click_track\",\n",
    "    \"clean_electric_guitar\",\n",
    "    \"distorted_electric_guitar\",\n",
    "    \"lap_steel_guitar_or_slide_guitar\",\n",
    "    \"acoustic_guitar\",\n",
    "    \"other_plucked\",\n",
    "    \"atonal_percussion\",\n",
    "    \"pitched_percussion\",\n",
    "    \"grand_piano\",\n",
    "    \"electric_piano\",\n",
    "    \"organ_electric_organ\",\n",
    "    \"synth_pad\",\n",
    "    \"synth_lead\",\n",
    "    \"other_sounds\",\n",
    "    \"violin\",\n",
    "    \"viola\",\n",
    "    \"cello\",\n",
    "    \"violin_section\",\n",
    "    \"viola_section\",\n",
    "    \"cello_section\",\n",
    "    \"string_section\",\n",
    "    \"other_strings\",\n",
    "    \"brass\",\n",
    "    \"flutes\",\n",
    "    \"reeds\",\n",
    "    \"other_wind\",\n",
    "}\n",
    "\n",
    "COARSE_LEVEL_INSTRUMENTS = {\n",
    "    \"vocals\",\n",
    "    \"bass\",\n",
    "    \"drums\",\n",
    "    \"guitar\",\n",
    "    \"other_plucked\",\n",
    "    \"percussion\",\n",
    "    \"piano\",\n",
    "    \"other_keys\",\n",
    "    \"bowed_strings\",\n",
    "    \"wind\",\n",
    "    \"other\",\n",
    "}\n",
    "\n",
    "COARSE_TO_FINE = {\n",
    "    \"vocals\": [\n",
    "        \"lead_male_singer\",\n",
    "        \"lead_female_singer\",\n",
    "        \"human_choir\",\n",
    "        \"background_vocals\",\n",
    "        \"other_vocals\",\n",
    "    ],\n",
    "    \"bass\": [\n",
    "        \"bass_guitar\",\n",
    "        \"bass_synthesizer\",\n",
    "        \"contrabass_double_bass\",\n",
    "        \"tuba\",\n",
    "        \"bassoon\",\n",
    "    ],\n",
    "    \"drums\": [\n",
    "        \"snare_drum\",\n",
    "        \"toms\",\n",
    "        \"kick_drum\",\n",
    "        \"cymbals\",\n",
    "        \"overheads\",\n",
    "        \"full_acoustic_drumkit\",\n",
    "        \"drum_machine\",\n",
    "        \"hihat\",\n",
    "    ],\n",
    "    \"other\": [\"fx\", \"click_track\"],\n",
    "    \"guitar\": [\n",
    "        \"clean_electric_guitar\",\n",
    "        \"distorted_electric_guitar\",\n",
    "        \"lap_steel_guitar_or_slide_guitar\",\n",
    "        \"acoustic_guitar\",\n",
    "    ],\n",
    "    \"other_plucked\": [\"other_plucked\"],\n",
    "    \"percussion\": [\"atonal_percussion\", \"pitched_percussion\"],\n",
    "    \"piano\": [\"grand_piano\", \"electric_piano\"],\n",
    "    \"other_keys\": [\"organ_electric_organ\", \"synth_pad\", \"synth_lead\", \"other_sounds\"],\n",
    "    \"bowed_strings\": [\n",
    "        \"violin\",\n",
    "        \"viola\",\n",
    "        \"cello\",\n",
    "        \"violin_section\",\n",
    "        \"viola_section\",\n",
    "        \"cello_section\",\n",
    "        \"string_section\",\n",
    "        \"other_strings\",\n",
    "    ],\n",
    "    \"wind\": [\"brass\", \"flutes\", \"reeds\", \"other_wind\"],\n",
    "}\n",
    "\n",
    "COARSE_TO_FINE = {k: set(v) for k, v in COARSE_TO_FINE.items()}\n",
    "FINE_TO_COARSE = {k: kk for kk, v in COARSE_TO_FINE.items() for k in v}\n",
    "\n",
    "ALL_LEVEL_INSTRUMENTS = COARSE_LEVEL_INSTRUMENTS.union(FINE_LEVEL_INSTRUMENTS)\n",
    "\n",
    "\n",
    "class MoisesDBBaseDataset(BaseSourceSeparationDataset, ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        data_path: str = \"/home/kwatchar3/Documents/data/moisesdb\",\n",
    "        fs: int = 44100,\n",
    "        return_stems: Union[bool, List[str]] = False,\n",
    "        npy_memmap=True,\n",
    "        recompute_mixture=False,\n",
    "        train_folds=None,\n",
    "        val_folds=None,\n",
    "        test_folds=None,\n",
    "        query_file=\"query\",\n",
    "    ) -> None:\n",
    "        if test_folds is None:\n",
    "            test_folds = [5]\n",
    "\n",
    "        if val_folds is None:\n",
    "            val_folds = [4]\n",
    "\n",
    "        if train_folds is None:\n",
    "            train_folds = [1, 2, 3]\n",
    "\n",
    "        split_path = os.path.join(data_path, \"splits.csv\")\n",
    "        splits = pd.read_csv(split_path)\n",
    "\n",
    "        metadata_path = os.path.join(data_path, \"stems.csv\")\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "        if split == \"train\":\n",
    "            folds = train_folds\n",
    "        elif split == \"val\":\n",
    "            folds = val_folds\n",
    "        elif split == \"test\":\n",
    "            folds = test_folds\n",
    "        else:\n",
    "            raise NameError\n",
    "\n",
    "        files = splits[splits[\"split\"].isin(folds)][\"song_id\"].tolist()\n",
    "        metadata = metadata[metadata[\"song_id\"].isin(files)]\n",
    "\n",
    "        super().__init__(\n",
    "            split=split,\n",
    "            stems=[\"mixture\"],\n",
    "            files=files,\n",
    "            data_path=data_path,\n",
    "            fs=fs,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=recompute_mixture,\n",
    "        )\n",
    "\n",
    "        self.folds = folds\n",
    "\n",
    "        self.metadata = metadata.rename(\n",
    "            columns={k: k.replace(\" \", \"_\") for k in metadata.columns}\n",
    "        )\n",
    "\n",
    "        self.song_to_stem = (\n",
    "            metadata.set_index(\"song_id\")\n",
    "            .apply(lambda row: row[row == 1].index.tolist(), axis=1)\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.stem_to_song = (\n",
    "            metadata.set_index(\"song_id\")\n",
    "            .transpose()\n",
    "            .apply(lambda row: row[row == 1].index.tolist(), axis=1)\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        self.true_length = len(self.files)\n",
    "        self.n_channels = 2\n",
    "\n",
    "        self.audio_path = os.path.join(data_path, \"npy2\")\n",
    "\n",
    "        self.return_stems = return_stems\n",
    "\n",
    "        self.query_file = query_file\n",
    "\n",
    "    def get_full_stem(self, *, stem: str, identifier) -> torch.Tensor:\n",
    "        song_id = identifier[\"song_id\"]\n",
    "        path = os.path.join(self.data_path, \"npy2\", song_id)\n",
    "        # noinspection PyUnresolvedReferences\n",
    "\n",
    "        assert self.npy_memmap\n",
    "\n",
    "        if os.path.exists(os.path.join(path, f\"{stem}.npy\")):\n",
    "            audio = np.load(os.path.join(path, f\"{stem}.npy\"), mmap_mode=\"r\")\n",
    "        else:\n",
    "            audio = None\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_query_stem(self, *, stem: str, identifier) -> torch.Tensor:\n",
    "        song_id = identifier[\"song_id\"]\n",
    "        path = os.path.join(self.data_path, \"npyq\", song_id)\n",
    "        # noinspection PyUnresolvedReferences\n",
    "\n",
    "        if self.npy_memmap:\n",
    "            # print(self.npy_memmap)\n",
    "            audio = np.load(\n",
    "                os.path.join(path, f\"{stem}.{self.query_file}.npy\"), mmap_mode=\"r\"\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_stem(self, *, stem: str, identifier) -> torch.Tensor:\n",
    "        audio = self.get_full_stem(stem=stem, identifier=identifier)\n",
    "        return audio\n",
    "\n",
    "    def get_identifier(self, index):\n",
    "        return dict(song_id=self.files[index % self.true_length])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        identifier = self.get_identifier(index)\n",
    "        audio = self.get_audio(identifier)\n",
    "\n",
    "        mixture = audio[\"mixture\"].copy()\n",
    "\n",
    "        if isinstance(self.return_stems, list):\n",
    "            sources = {\n",
    "                stem: audio.get(stem, np.zeros_like(mixture))\n",
    "                for stem in self.return_stems\n",
    "            }\n",
    "        elif isinstance(self.return_stems, bool):\n",
    "            if self.return_stems:\n",
    "                sources = {\n",
    "                    stem: audio[stem].copy()\n",
    "                    for stem in self.song_to_stem[identifier[\"song_id\"]]\n",
    "                }\n",
    "            else:\n",
    "                sources = None\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            metadata=identifier,\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "\n",
    "\n",
    "class MoisesDBFullTrackDataset(MoisesDBBaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        return_stems: Union[bool, List[str]] = False,\n",
    "        npy_memmap=True,\n",
    "        recompute_mixture=False,\n",
    "        query_file=\"query\",\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            split=split,\n",
    "            data_path=data_root,\n",
    "            return_stems=return_stems,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=recompute_mixture,\n",
    "            query_file=query_file,\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.true_length\n",
    "\n",
    "\n",
    "class MoisesDBVDBOFullTrackDataset(MoisesDBFullTrackDataset):\n",
    "    def __init__(\n",
    "        self, data_root: str, split: str, npy_memmap=True, recompute_mixture=False\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            return_stems=[\"vocals\", \"bass\", \"drums\", \"vdbo_others\"],\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=recompute_mixture,\n",
    "            query_file=None,\n",
    "        )\n",
    "\n",
    "\n",
    "import torch_audiomentations as audiomentations\n",
    "from torch_audiomentations.utils.dsp import convert_decibels_to_amplitude_ratio\n",
    "\n",
    "\n",
    "class SmartGain(audiomentations.Gain):\n",
    "    def __init__(\n",
    "        self, p=0.5, min_gain_in_db=-6, max_gain_in_db=6, dbfs_threshold=-45.0\n",
    "    ):\n",
    "        super().__init__(\n",
    "            p=p, min_gain_in_db=min_gain_in_db, max_gain_in_db=max_gain_in_db\n",
    "        )\n",
    "\n",
    "        self.dbfs_threshold = dbfs_threshold\n",
    "\n",
    "    def randomize_parameters(\n",
    "        self,\n",
    "        samples: Tensor = None,\n",
    "        sample_rate: Optional[int] = None,\n",
    "        targets: Optional[Tensor] = None,\n",
    "        target_rate: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "        dbfs = 10 * torch.log10(torch.mean(torch.square(samples)) + 1e-6)\n",
    "\n",
    "        if dbfs > self.dbfs_threshold:\n",
    "            low = self.min_gain_in_db\n",
    "        else:\n",
    "            low = max(0.0, self.min_gain_in_db)\n",
    "\n",
    "        distribution = torch.distributions.Uniform(\n",
    "            low=torch.tensor(low, dtype=torch.float32, device=samples.device),\n",
    "            high=torch.tensor(\n",
    "                self.max_gain_in_db, dtype=torch.float32, device=samples.device\n",
    "            ),\n",
    "            validate_args=True,\n",
    "        )\n",
    "        selected_batch_size = samples.size(0)\n",
    "        self.transform_parameters[\"gain_factors\"] = (\n",
    "            convert_decibels_to_amplitude_ratio(\n",
    "                distribution.sample(sample_shape=(selected_batch_size,))\n",
    "            )\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(1)\n",
    "        )\n",
    "\n",
    "\n",
    "class Audiomentations(audiomentations.Compose):\n",
    "    def __init__(self, augment=\"gssp\", fs: int = 44100):\n",
    "\n",
    "        if isinstance(augment, str):\n",
    "            if augment == \"gssp\":\n",
    "                augment = OmegaConf.create(\n",
    "                    [\n",
    "                        dict(\n",
    "                            cls=\"Shift\",\n",
    "                            kwargs=dict(p=1.0, min_shift=-0.5, max_shift=0.5),\n",
    "                        ),\n",
    "                        dict(\n",
    "                            cls=\"Gain\",\n",
    "                            kwargs=dict(p=1.0, min_gain_in_db=-6, max_gain_in_db=6),\n",
    "                        ),\n",
    "                        dict(cls=\"ShuffleChannels\", kwargs=dict(p=0.5)),\n",
    "                        dict(cls=\"PolarityInversion\", kwargs=dict(p=0.5)),\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        transforms = []\n",
    "\n",
    "        for transform in augment:\n",
    "\n",
    "            if transform.cls == \"Gain\":\n",
    "                transforms.append(SmartGain(**transform.kwargs))\n",
    "            else:\n",
    "                transforms.append(\n",
    "                    getattr(audiomentations, transform.cls)(**transform.kwargs)\n",
    "                )\n",
    "\n",
    "        super().__init__(transforms=transforms, shuffle=True)\n",
    "\n",
    "        self.fs = fs\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        samples: torch.Tensor = None,\n",
    "    ) -> ObjectDict:\n",
    "        return super().forward(samples, sample_rate=self.fs)\n",
    "\n",
    "\n",
    "class MoisesDBVDBORandomChunkDataset(MoisesDBVDBOFullTrackDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        chunk_size_seconds: float = 4.0,\n",
    "        fs: int = 44100,\n",
    "        target_length: int = 8192,\n",
    "        augment=None,\n",
    "        npy_memmap=True,\n",
    "        recompute_mixture=True,\n",
    "        db_threshold=-24.0,\n",
    "        db_step=-12.0,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=recompute_mixture,\n",
    "        )\n",
    "\n",
    "        self.chunk_size_seconds = chunk_size_seconds\n",
    "        self.chunk_size_samples = int(chunk_size_seconds * fs)\n",
    "        self.fs = fs\n",
    "\n",
    "        self.target_length = target_length\n",
    "\n",
    "        self.db_threshold = db_threshold\n",
    "        self.db_step = db_step\n",
    "\n",
    "        if augment is not None:\n",
    "            assert self.recompute_mixture\n",
    "            self.augment = Audiomentations(augment, fs)\n",
    "        else:\n",
    "            self.augment = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.target_length\n",
    "\n",
    "    def _chunk_audio(self, audio, start, end):\n",
    "        audio = {k: v[..., start:end] for k, v in audio.items()}\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def _get_start_end(self, audio, identifier):\n",
    "        n_samples = audio.shape[-1]\n",
    "        start = np.random.randint(0, n_samples - self.chunk_size_samples)\n",
    "        end = start + self.chunk_size_samples\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    def _get_audio(self, stems, identifier: Dict[str, Any]):\n",
    "        audio = {}\n",
    "\n",
    "        for stem in stems:\n",
    "            audio[stem] = self.get_full_stem(stem=stem, identifier=identifier)\n",
    "\n",
    "        for stem in stems:\n",
    "            if audio[stem] is None:\n",
    "                audio[stem] = np.zeros(\n",
    "                    audio[\n",
    "                        (\n",
    "                            \"mixture\"\n",
    "                            if \"mixture\" in stems\n",
    "                            else [s for s in stems if audio[s] is not None][0]\n",
    "                        )\n",
    "                    ].shape,\n",
    "                    dtype=np.float32,\n",
    "                )\n",
    "\n",
    "        start, end = self._get_start_end(audio[stems[0]], identifier)\n",
    "        audio = self._chunk_audio(audio, start, end)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            audio = {\n",
    "                k: self.augment(torch.from_numpy(v[None, :, :]))[0, :, :].numpy()\n",
    "                for k, v in audio.items()\n",
    "            }\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_audio(self, identifier: Dict[str, Any]):\n",
    "        if self.recompute_mixture:\n",
    "            audio = self._get_audio(\n",
    "                [\"vocals\", \"bass\", \"drums\", \"vdbo_others\"], identifier=identifier\n",
    "            )\n",
    "            audio[\"mixture\"] = self.compute_mixture(audio)\n",
    "            return audio\n",
    "        else:\n",
    "            return self._get_audio(\n",
    "                [\"mixture\", \"vocals\", \"bass\", \"drums\", \"vdbo_others\"],\n",
    "                identifier=identifier,\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        identifier = self.get_identifier(index)\n",
    "        audio = self.get_audio(identifier=identifier)\n",
    "\n",
    "        mixture = audio[\"mixture\"].copy()\n",
    "\n",
    "        sources = {\n",
    "            stem: audio.get(stem, np.zeros_like(mixture)) for stem in self.return_stems\n",
    "        }\n",
    "\n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            metadata=identifier,\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "\n",
    "\n",
    "class MoisesDBVDBODeterministicChunkDataset(MoisesDBVDBORandomChunkDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        chunk_size_seconds: float = 4.0,\n",
    "        hop_size_seconds: float = 8.0,\n",
    "        fs: int = 44100,\n",
    "        npy_memmap=True,\n",
    "        recompute_mixture=False,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            chunk_size_seconds=chunk_size_seconds,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=recompute_mixture,\n",
    "        )\n",
    "\n",
    "        self.hop_size_seconds = hop_size_seconds\n",
    "        self.hop_size_samples = int(hop_size_seconds * fs)\n",
    "\n",
    "        self.index_to_identifiers = self._generate_index()\n",
    "        self.length = len(self.index_to_identifiers)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def _generate_index(self):\n",
    "\n",
    "        identifiers = []\n",
    "\n",
    "        for song_id in self.files:\n",
    "            audio = self.get_full_stem(stem=\"mixture\", identifier=dict(song_id=song_id))\n",
    "            n_samples = audio.shape[-1]\n",
    "            n_chunks = math.floor(\n",
    "                (n_samples - self.chunk_size_samples) / self.hop_size_samples\n",
    "            )\n",
    "\n",
    "            for i in range(n_chunks):\n",
    "                chunk_start = i * self.hop_size_samples\n",
    "                identifiers.append(dict(song_id=song_id, chunk_start=chunk_start))\n",
    "\n",
    "        return identifiers\n",
    "\n",
    "    def get_identifier(self, index):\n",
    "        return self.index_to_identifiers[index]\n",
    "\n",
    "    def _get_start_end(self, audio, identifier):\n",
    "\n",
    "        start = identifier[\"chunk_start\"]\n",
    "        end = start + self.chunk_size_samples\n",
    "\n",
    "        return start, end\n",
    "\n",
    "\n",
    "def round_samples(seconds, fs, hop_size, downsample):\n",
    "    n_frames = math.ceil(seconds * fs / hop_size) + 1\n",
    "    n_frames_down = math.ceil(n_frames / downsample)\n",
    "    n_frames = n_frames_down * downsample\n",
    "    n_samples = (n_frames - 1) * hop_size\n",
    "\n",
    "    return int(n_samples)\n",
    "\n",
    "\n",
    "class MoisesDBRandomChunkRandomQueryDataset(MoisesDBFullTrackDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        target_length: int,\n",
    "        chunk_size_seconds: float = 4.0,\n",
    "        query_size_seconds: float = 1.0,\n",
    "        round_query: bool = False,\n",
    "        min_query_dbfs: float = -40.0,\n",
    "        min_target_dbfs: float = -36.0,\n",
    "        min_target_dbfs_step: float = -12.0,\n",
    "        max_dbfs_tries: int = 10,\n",
    "        top_k_instrument: int = 10,\n",
    "        mixture_stem: str = \"mixture\",\n",
    "        use_own_query: bool = True,\n",
    "        npy_memmap=True,\n",
    "        allowed_stems=None,\n",
    "        query_file=\"query\",\n",
    "        augment=None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=augment is not None,\n",
    "            query_file=query_file,\n",
    "        )\n",
    "\n",
    "        self.mixture_stem = mixture_stem\n",
    "\n",
    "        self.chunk_size_seconds = chunk_size_seconds\n",
    "        self.chunk_size_samples = round_samples(\n",
    "            self.chunk_size_seconds, self.fs, 512, 2**6\n",
    "        )\n",
    "\n",
    "        self.query_size_seconds = query_size_seconds\n",
    "\n",
    "        if round_query:\n",
    "            self.query_size_samples = round_samples(\n",
    "                self.query_size_seconds, self.fs, 512, 2**6\n",
    "            )\n",
    "        else:\n",
    "            self.query_size_samples = int(self.query_size_seconds * self.fs)\n",
    "\n",
    "        self.target_length = target_length\n",
    "\n",
    "        self.min_query_dbfs = min_query_dbfs\n",
    "\n",
    "        if min_target_dbfs is None:\n",
    "            min_target_dbfs = -np.inf\n",
    "            min_target_dbfs_step = None\n",
    "            max_dbfs_tries = 1\n",
    "\n",
    "        self.min_target_dbfs = min_target_dbfs\n",
    "        self.min_target_dbfs_step = min_target_dbfs_step\n",
    "        self.max_dbfs_tries = max_dbfs_tries\n",
    "\n",
    "        self.top_k_instrument = top_k_instrument\n",
    "\n",
    "        if allowed_stems is None:\n",
    "            allowed_stems = INST_BY_OCCURRENCE[: self.top_k_instrument]\n",
    "        else:\n",
    "            self.top_k_instrument = None\n",
    "\n",
    "        self.allowed_stems = allowed_stems\n",
    "\n",
    "        self.song_to_all_stems = {\n",
    "            k: list(set(v) & set(ALL_LEVEL_INSTRUMENTS))\n",
    "            for k, v in self.song_to_stem.items()\n",
    "        }\n",
    "\n",
    "        self.song_to_stem = {\n",
    "            k: list(set(v) & set(self.allowed_stems))\n",
    "            for k, v in self.song_to_stem.items()\n",
    "        }\n",
    "        self.stem_to_song = {\n",
    "            k: list(set(v) & set(self.files)) for k, v in self.stem_to_song.items()\n",
    "        }\n",
    "\n",
    "        self.queriable_songs = [k for k, v in self.song_to_stem.items() if len(v) > 0]\n",
    "\n",
    "        self.use_own_query = use_own_query\n",
    "\n",
    "        if self.use_own_query:\n",
    "            self.files = [k for k in self.files if len(self.song_to_stem[k]) > 0]\n",
    "            self.true_length = len(self.files)\n",
    "\n",
    "        if augment is not None:\n",
    "            assert self.recompute_mixture\n",
    "            self.augment = Audiomentations(augment, self.fs)\n",
    "        else:\n",
    "            self.augment = None\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.target_length\n",
    "\n",
    "    def _chunk_audio(self, audio, start, end):\n",
    "        audio = {k: v[..., start:end] for k, v in audio.items()}\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def _get_start_end(self, audio):\n",
    "        n_samples = audio.shape[-1]\n",
    "        start = np.random.randint(0, n_samples - self.chunk_size_samples)\n",
    "        end = start + self.chunk_size_samples\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    def _target_dbfs(self, audio):\n",
    "        return 10.0 * np.log10(np.mean(np.square(np.abs(audio))) + 1e-6)\n",
    "\n",
    "    def _chunk_and_check_dbfs_threshold(self, audio_, target_stem, threshold):\n",
    "\n",
    "        target_dict = {target_stem: audio_[target_stem]}\n",
    "\n",
    "        for _ in range(self.max_dbfs_tries):\n",
    "            start, end = self._get_start_end(audio_[target_stem])\n",
    "            taudio = self._chunk_audio(target_dict, start, end)\n",
    "\n",
    "            dbfs = self._target_dbfs(taudio[target_stem])\n",
    "            if dbfs > threshold:\n",
    "                return self._chunk_audio(audio_, start, end)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _chunk_and_check_dbfs(self, audio_, target_stem):\n",
    "        out = self._chunk_and_check_dbfs_threshold(\n",
    "            audio_, target_stem, self.min_target_dbfs\n",
    "        )\n",
    "\n",
    "        if out is not None:\n",
    "            return out\n",
    "\n",
    "        out = self._chunk_and_check_dbfs_threshold(\n",
    "            audio_, target_stem, self.min_target_dbfs + self.min_target_dbfs_step\n",
    "        )\n",
    "\n",
    "        if out is not None:\n",
    "            return out\n",
    "\n",
    "        start, end = self._get_start_end(audio_[target_stem])\n",
    "        audio = self._chunk_audio(audio_, start, end)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def _augment(self, audio, target_stem):\n",
    "        stack_audio = np.stack([v for v in audio.values()], axis=0)\n",
    "        aug_audio = self.augment(torch.from_numpy(stack_audio)).numpy()\n",
    "        mixture = np.sum(aug_audio, axis=0)\n",
    "\n",
    "        out = {\n",
    "            \"mixture\": mixture,\n",
    "        }\n",
    "\n",
    "        if target_stem is not None:\n",
    "            target_idx = list(audio.keys()).index(target_stem)\n",
    "            out[target_stem] = aug_audio[target_idx]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _choose_stems_for_augment(self, identifier, target_stem):\n",
    "        stems_for_song = set(self.song_to_all_stems[identifier[\"song_id\"]])\n",
    "\n",
    "        stems_ = []\n",
    "        coarse_level_accounted = set()\n",
    "\n",
    "        is_none_target = target_stem is None\n",
    "        is_coarse_target = target_stem in COARSE_LEVEL_INSTRUMENTS\n",
    "\n",
    "        if is_coarse_target or is_none_target:\n",
    "            coarse_target = target_stem\n",
    "        else:\n",
    "            coarse_target = FINE_TO_COARSE[target_stem]\n",
    "\n",
    "        fine_level_stems = stems_for_song & FINE_LEVEL_INSTRUMENTS\n",
    "        coarse_level_stems = stems_for_song & COARSE_LEVEL_INSTRUMENTS\n",
    "\n",
    "        for s in fine_level_stems:\n",
    "            coarse_level = FINE_TO_COARSE[s]\n",
    "\n",
    "            if is_coarse_target and coarse_level == coarse_target:\n",
    "                continue\n",
    "            else:\n",
    "                stems_.append(s)\n",
    "\n",
    "            coarse_level_accounted.add(coarse_level)\n",
    "\n",
    "        stems_ += list(coarse_level_stems - coarse_level_accounted)\n",
    "\n",
    "        if target_stem is not None:\n",
    "            assert target_stem in stems_, f\"stems: {stems_}, target stem: {target_stem}\"\n",
    "\n",
    "            if len(stems_for_song) > 1:\n",
    "                assert (\n",
    "                    len(stems_) > 1\n",
    "                ), f\"stems: {stems_}, stems in song: {stems_for_song},\\n target stem: {target_stem}\"\n",
    "\n",
    "        assert \"mixture\" not in stems_\n",
    "\n",
    "        return stems_\n",
    "\n",
    "    def _get_audio(\n",
    "        self, stems, identifier: Dict[str, Any], check_dbfs=True, no_target=False\n",
    "    ):\n",
    "\n",
    "        target_stem = stems[0] if not no_target else None\n",
    "\n",
    "        if self.augment is not None:\n",
    "            stems_ = self._choose_stems_for_augment(identifier, target_stem)\n",
    "        else:\n",
    "            stems_ = stems\n",
    "\n",
    "        audio = {}\n",
    "        for stem in stems_:\n",
    "            audio[stem] = self.get_full_stem(stem=stem, identifier=identifier)\n",
    "\n",
    "        audio_ = {k: v.copy() for k, v in audio.items()}\n",
    "\n",
    "        if check_dbfs:\n",
    "            assert target_stem is not None\n",
    "            audio = self._chunk_and_check_dbfs(audio_, target_stem)\n",
    "        else:\n",
    "            first_key = list(audio_.keys())[0]\n",
    "            start, end = self._get_start_end(audio_[first_key])\n",
    "            audio = self._chunk_audio(audio_, start, end)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            assert \"mixture\" not in audio\n",
    "            audio = self._augment(audio, target_stem)\n",
    "            assert \"mixture\" in audio\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        mix_identifier = self.get_identifier(index)\n",
    "        mix_stems = self.song_to_stem[mix_identifier[\"song_id\"]]\n",
    "\n",
    "        if self.use_own_query:\n",
    "            query_id = mix_identifier[\"song_id\"]\n",
    "            query_identifier = dict(song_id=query_id)\n",
    "            possible_stem = mix_stems\n",
    "\n",
    "            assert len(possible_stem) > 0\n",
    "\n",
    "            zero_target = False\n",
    "        else:\n",
    "            query_id = random.choice(self.queriable_songs)\n",
    "            query_identifier = dict(song_id=query_id)\n",
    "            query_stems = self.song_to_stem[query_id]\n",
    "            possible_stem = list(set(mix_stems) & set(query_stems))\n",
    "\n",
    "            if len(possible_stem) == 0:\n",
    "                possible_stem = query_stems\n",
    "                zero_target = True\n",
    "                # print(f\"Mix {mix_identifier['song_id']} and query {query_id} have no common stems.\")\n",
    "                # return self.__getitem__(index + 1)\n",
    "            else:\n",
    "                zero_target = False\n",
    "\n",
    "        assert (\n",
    "            len(possible_stem) > 0\n",
    "        ), f\"{mix_identifier['song_id']} and {query_id} have no common stems. zero target is {zero_target}\"\n",
    "        stem = random.choice(possible_stem)\n",
    "\n",
    "        if zero_target:\n",
    "            audio = self._get_audio(\n",
    "                [self.mixture_stem],\n",
    "                identifier=mix_identifier,\n",
    "                check_dbfs=False,\n",
    "                no_target=True,\n",
    "            )\n",
    "            mixture = audio[self.mixture_stem].copy()\n",
    "            sources = {\"target\": np.zeros_like(mixture)}\n",
    "        else:\n",
    "            audio = self._get_audio(\n",
    "                [stem, self.mixture_stem], identifier=mix_identifier, check_dbfs=True\n",
    "            )\n",
    "            mixture = audio[self.mixture_stem].copy()\n",
    "            sources = {\"target\": audio[stem].copy()}\n",
    "\n",
    "        query = self.get_query_stem(stem=stem, identifier=query_identifier)\n",
    "        query = query.copy()\n",
    "\n",
    "        assert mixture.shape[-1] == self.chunk_size_samples\n",
    "        assert query.shape[-1] == self.query_size_samples\n",
    "        assert sources[\"target\"].shape[-1] == self.chunk_size_samples\n",
    "\n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            query=query,\n",
    "            metadata={\n",
    "                \"mix\": mix_identifier,\n",
    "                \"query\": query_identifier,\n",
    "                \"stem\": stem,\n",
    "            },\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "\n",
    "\n",
    "class MoisesDBRandomChunkBalancedRandomQueryDataset(\n",
    "    MoisesDBRandomChunkRandomQueryDataset\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        target_length: int,\n",
    "        chunk_size_seconds: float = 4,\n",
    "        query_size_seconds: float = 1,\n",
    "        round_query: bool = False,\n",
    "        min_query_dbfs: float = -40.0,\n",
    "        min_target_dbfs: float = -36.0,\n",
    "        min_target_dbfs_step: float = -12.0,\n",
    "        max_dbfs_tries: int = 10,\n",
    "        top_k_instrument: int = 10,\n",
    "        mixture_stem: str = \"mixture\",\n",
    "        use_own_query: bool = True,\n",
    "        npy_memmap=True,\n",
    "        allowed_stems=None,\n",
    "        query_file=\"query\",\n",
    "        augment=None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data_root,\n",
    "            split,\n",
    "            target_length,\n",
    "            chunk_size_seconds,\n",
    "            query_size_seconds,\n",
    "            round_query,\n",
    "            min_query_dbfs,\n",
    "            min_target_dbfs,\n",
    "            min_target_dbfs_step,\n",
    "            max_dbfs_tries,\n",
    "            top_k_instrument,\n",
    "            mixture_stem,\n",
    "            use_own_query,\n",
    "            npy_memmap,\n",
    "            allowed_stems,\n",
    "            query_file,\n",
    "            augment,\n",
    "        )\n",
    "        \n",
    "        self.stem_to_n_songs = {k: len(v) for k, v in self.stem_to_song.items()}\n",
    "        self.trainable_stems = [k for k, v in self.stem_to_n_songs.items() if v > 1]\n",
    "        self.n_allowed_stems = len(self.allowed_stems)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        stem = self.allowed_stems[index % self.n_allowed_stems]\n",
    "        song_ids_with_stem = self.stem_to_song[stem]\n",
    "        \n",
    "        song_id = song_ids_with_stem[index % self.stem_to_n_songs[stem]]\n",
    "        \n",
    "        mix_identifier = dict(song_id=song_id)\n",
    "        \n",
    "        audio = self._get_audio([stem, self.mixture_stem], identifier=mix_identifier, check_dbfs=True)\n",
    "        mixture = audio[self.mixture_stem].copy()\n",
    "        \n",
    "        if self.use_own_query:\n",
    "            query_id = song_id\n",
    "            query_identifier = dict(song_id=query_id)\n",
    "        else:\n",
    "            query_id = random.choice(song_ids_with_stem)\n",
    "            query_identifier = dict(song_id=query_id)\n",
    "            \n",
    "        query = self.get_query_stem(stem=stem, identifier=query_identifier)\n",
    "        query = query.copy()\n",
    "        \n",
    "        sources = {\"target\": audio[stem].copy()}\n",
    "        \n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            query=query,\n",
    "            metadata={\n",
    "                \"mix\": mix_identifier,\n",
    "                \"query\": query_identifier,\n",
    "                \"stem\": stem,\n",
    "            },\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "    MoisesDBRandomChunkRandomQueryDataset\n",
    "):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str,\n",
    "        chunk_size_seconds: float = 4.0,\n",
    "        hop_size_seconds: float = 8.0,\n",
    "        query_size_seconds: float = 1.0,\n",
    "        min_query_dbfs: float = -40.0,\n",
    "        top_k_instrument: int = 10,\n",
    "        n_queries_per_chunk: int = 1,\n",
    "        mixture_stem: str = \"mixture\",\n",
    "        use_own_query: bool = True,\n",
    "        npy_memmap=True,\n",
    "        allowed_stems: List[str] = None,\n",
    "        query_file=\"query\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            target_length=None,\n",
    "            chunk_size_seconds=chunk_size_seconds,\n",
    "            query_size_seconds=query_size_seconds,\n",
    "            min_query_dbfs=min_query_dbfs,\n",
    "            top_k_instrument=top_k_instrument,\n",
    "            mixture_stem=mixture_stem,\n",
    "            use_own_query=use_own_query,\n",
    "            npy_memmap=npy_memmap,\n",
    "            allowed_stems=allowed_stems,\n",
    "            query_file=query_file,\n",
    "        )\n",
    "\n",
    "        if hop_size_seconds is None:\n",
    "            hop_size_seconds = chunk_size_seconds\n",
    "\n",
    "        self.chunk_hop_size_seconds = hop_size_seconds\n",
    "\n",
    "        self.chunk_hop_size_samples = int(hop_size_seconds * self.fs)\n",
    "\n",
    "        self.n_queries_per_chunk = n_queries_per_chunk\n",
    "\n",
    "        self._overwrite = False\n",
    "\n",
    "        self.query_tuples = self.find_query_tuples_or_generate()\n",
    "        self.n_chunks = len(self.query_tuples)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_chunks\n",
    "\n",
    "    def _get_audio(self, stems, identifier: Dict[str, Any]):\n",
    "        audio = {}\n",
    "\n",
    "        for stem in stems:\n",
    "            audio[stem] = self.get_full_stem(stem=stem, identifier=identifier)\n",
    "\n",
    "        start = identifier[\"chunk_start\"]\n",
    "        end = start + self.chunk_size_samples\n",
    "        audio = self._chunk_audio(audio, start, end)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def find_query_tuples_or_generate(self):\n",
    "        query_path = os.path.join(self.data_path, \"queries\")\n",
    "        val_folds = \"-\".join(map(str, self.folds))\n",
    "\n",
    "        path_so_far = os.path.join(query_path, val_folds)\n",
    "\n",
    "        if not os.path.exists(path_so_far):\n",
    "            return self.generate_index()\n",
    "\n",
    "        chunk_specs = f\"chunk{self.chunk_size_samples}-hop{self.chunk_hop_size_samples}\"\n",
    "        path_so_far = os.path.join(path_so_far, chunk_specs)\n",
    "\n",
    "        if not os.path.exists(path_so_far):\n",
    "            return self.generate_index()\n",
    "\n",
    "        query_specs = f\"query{self.query_size_samples}-n{self.n_queries_per_chunk}\"\n",
    "        path_so_far = os.path.join(path_so_far, query_specs)\n",
    "\n",
    "        if not os.path.exists(path_so_far):\n",
    "            return self.generate_index()\n",
    "\n",
    "        if self.top_k_instrument is not None:\n",
    "            path_so_far = os.path.join(\n",
    "                path_so_far, f\"queries-top{self.top_k_instrument}.csv\"\n",
    "            )\n",
    "        else:\n",
    "            if len(self.allowed_stems) > 5:\n",
    "                allowed_stems = (\n",
    "                    str(len(self.allowed_stems))\n",
    "                    + \"stems:\"\n",
    "                    + \":\".join([k[0] for k in self.allowed_stems if k != \"mixture\"])\n",
    "                )\n",
    "            else:\n",
    "                allowed_stems = \":\".join(self.allowed_stems)\n",
    "\n",
    "            path_so_far = os.path.join(path_so_far, f\"queries-{allowed_stems}.csv\")\n",
    "\n",
    "        if not os.path.exists(path_so_far):\n",
    "            return self.generate_index()\n",
    "\n",
    "        print(f\"Loading query tuples from {path_so_far}\")\n",
    "\n",
    "        return pd.read_csv(path_so_far)\n",
    "\n",
    "    def _get_index_path(self):\n",
    "        query_root = os.path.join(self.data_path, \"queries\")\n",
    "        val_folds = \"-\".join(map(str, self.folds))\n",
    "        chunk_specs = f\"chunk{self.chunk_size_samples}-hop{self.chunk_hop_size_samples}\"\n",
    "        query_specs = f\"query{self.query_size_samples}-n{self.n_queries_per_chunk}\"\n",
    "        query_dir = os.path.join(query_root, val_folds, chunk_specs, query_specs)\n",
    "\n",
    "        if self.top_k_instrument is not None:\n",
    "            query_path = os.path.join(\n",
    "                query_dir, f\"queries-top{self.top_k_instrument}.csv\"\n",
    "            )\n",
    "        else:\n",
    "            if len(self.allowed_stems) > 5:\n",
    "                allowed_stems = (\n",
    "                    str(len(self.allowed_stems))\n",
    "                    + \"stems:\"\n",
    "                    + \":\".join([k[0] for k in self.allowed_stems if k != \"mixture\"])\n",
    "                )\n",
    "            else:\n",
    "                allowed_stems = \":\".join(self.allowed_stems)\n",
    "            query_path = os.path.join(query_dir, f\"queries-{allowed_stems}.csv\")\n",
    "\n",
    "        if not self._overwrite:\n",
    "            assert not os.path.exists(\n",
    "                query_path\n",
    "            ), f\"Query path {query_path} already exists.\"\n",
    "\n",
    "        os.makedirs(query_dir, exist_ok=True)\n",
    "\n",
    "        return query_path\n",
    "\n",
    "    def generate_index(self):\n",
    "\n",
    "        query_path = self._get_index_path()\n",
    "\n",
    "        durations = pd.read_csv(os.path.join(self.data_path, \"durations.csv\"))\n",
    "        durations = (\n",
    "            durations[[\"song_id\", \"duration\"]]\n",
    "            .set_index(\"song_id\")[\"duration\"]\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        tuples = []\n",
    "\n",
    "        stems_without_queries = defaultdict(list)\n",
    "\n",
    "        for i, song_id in tqdm(enumerate(self.files), total=len(self.files)):\n",
    "            song_duration = durations[song_id]\n",
    "            mix_stems = self.song_to_stem[song_id]\n",
    "\n",
    "            n_mix_chunks = math.floor(\n",
    "                (song_duration - self.chunk_size_seconds) / self.chunk_hop_size_seconds\n",
    "            )\n",
    "\n",
    "            for stem in mix_stems:\n",
    "                possible_queries = self.stem_to_song[stem]\n",
    "                if song_id in possible_queries:\n",
    "                    possible_queries.remove(song_id)\n",
    "\n",
    "                if len(possible_queries) == 0:\n",
    "                    stems_without_queries[song_id].append(stem)\n",
    "                    continue\n",
    "\n",
    "                for k in tqdm(range(n_mix_chunks), desc=f\"song{i + 1}/{stem}\"):\n",
    "                    mix_chunk_start = int(k * self.chunk_hop_size_samples)\n",
    "\n",
    "                    for j in range(self.n_queries_per_chunk):\n",
    "                        query = random.choice(possible_queries)\n",
    "\n",
    "                        tuples.append(\n",
    "                            dict(\n",
    "                                mix=song_id,\n",
    "                                query=query,\n",
    "                                stem=stem,\n",
    "                                mix_chunk_start=mix_chunk_start,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        if len(stems_without_queries) > 0:\n",
    "            print(\"Stems without queries:\")\n",
    "            for song_id, stems in stems_without_queries.items():\n",
    "                print(f\"{song_id}: {stems}\")\n",
    "\n",
    "        tuples = pd.DataFrame(tuples)\n",
    "\n",
    "        print(\n",
    "            f\"Generating query tuples for {self.split} set with {len(tuples)} tuples.\"\n",
    "        )\n",
    "        print(f\"Saving query tuples to {query_path}\")\n",
    "\n",
    "        tuples.to_csv(query_path, index=False)\n",
    "\n",
    "        return tuples\n",
    "\n",
    "    def index_to_identifiers(self, index: int) -> Tuple[str, str, str, int]:\n",
    "\n",
    "        row = self.query_tuples.iloc[index]\n",
    "        mix_id = row[\"mix\"]\n",
    "\n",
    "        if self.use_own_query:\n",
    "            query_id = mix_id\n",
    "        else:\n",
    "            query_id = row[\"query\"]\n",
    "\n",
    "        stem = row[\"stem\"]\n",
    "        mix_chunk_start = row[\"mix_chunk_start\"]\n",
    "\n",
    "        return mix_id, query_id, stem, mix_chunk_start\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        mix_id, query_id, stem, mix_chunk_start = self.index_to_identifiers(index)\n",
    "\n",
    "        mix_identifier = dict(song_id=mix_id, chunk_start=mix_chunk_start)\n",
    "        query_identifier = dict(song_id=query_id)\n",
    "\n",
    "        audio = self._get_audio([stem, self.mixture_stem], identifier=mix_identifier)\n",
    "        query = self.get_query_stem(stem=stem, identifier=query_identifier)\n",
    "\n",
    "        mixture = audio[self.mixture_stem].copy()\n",
    "        sources = {\"target\": audio[stem].copy()}\n",
    "        query = query.copy()\n",
    "\n",
    "        assert mixture.shape[-1] == self.chunk_size_samples\n",
    "        # print(query.shape[-1], self.query_size_samples)\n",
    "        assert query.shape[-1] == self.query_size_samples\n",
    "        assert sources[\"target\"].shape[-1] == self.chunk_size_samples\n",
    "\n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            query=query,\n",
    "            metadata={\n",
    "                \"mix\": mix_identifier,\n",
    "                \"query\": query_identifier,\n",
    "                \"stem\": stem,\n",
    "            },\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "\n",
    "\n",
    "class MoisesDBFullTrackTestQueryDataset(MoisesDBFullTrackDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        split: str = \"test\",\n",
    "        top_k_instrument: int = 10,\n",
    "        mixture_stem: str = \"mixture\",\n",
    "        use_own_query: bool = True,\n",
    "        npy_memmap=True,\n",
    "        allowed_stems: List[str] = None,\n",
    "        query_file=\"query-10s\",\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data_root=data_root,\n",
    "            split=split,\n",
    "            npy_memmap=npy_memmap,\n",
    "            recompute_mixture=False,\n",
    "            query_file=query_file,\n",
    "        )\n",
    "\n",
    "        self.use_own_query = use_own_query\n",
    "\n",
    "        self.allowed_stems = allowed_stems\n",
    "\n",
    "        test_indices = pd.read_csv(os.path.join(data_root, \"test_indices.csv\"))\n",
    "\n",
    "        test_indices = test_indices[test_indices.stem.isin(self.allowed_stems)]\n",
    "\n",
    "        self.test_indices = test_indices\n",
    "\n",
    "        self.length = len(self.test_indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def index_to_identifiers(self, index: int) -> Tuple[str, str, str]:\n",
    "\n",
    "        row = self.test_indices.iloc[index]\n",
    "        mix_id = row[\"song_id\"]\n",
    "        if self.use_own_query:\n",
    "            query_id = mix_id\n",
    "        else:\n",
    "            query_id = row[\"query_id\"]\n",
    "        stem = row[\"stem\"]\n",
    "\n",
    "        return mix_id, query_id, stem\n",
    "\n",
    "    def _get_audio(self, stems, identifier: Dict[str, Any]):\n",
    "        audio = {}\n",
    "\n",
    "        for stem in stems:\n",
    "            audio[stem] = self.get_full_stem(stem=stem, identifier=identifier)\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        mix_id, query_id, stem = self.index_to_identifiers(index)\n",
    "\n",
    "        mix_identifier = dict(song_id=mix_id)\n",
    "\n",
    "        query_identifier = dict(song_id=query_id)\n",
    "\n",
    "        audio = self._get_audio([stem, \"mixture\"], identifier=mix_identifier)\n",
    "        query = self.get_query_stem(stem=stem, identifier=query_identifier)\n",
    "\n",
    "        mixture = audio[\"mixture\"].copy()\n",
    "        sources = {stem: audio[stem].copy()}\n",
    "        query = query.copy()\n",
    "\n",
    "        return input_dict(\n",
    "            mixture=mixture,\n",
    "            sources=sources,\n",
    "            query=query,\n",
    "            metadata={\n",
    "                \"mix\": mix_identifier[\"song_id\"],\n",
    "                \"query\": query_identifier[\"song_id\"],\n",
    "                \"stem\": stem,\n",
    "            },\n",
    "            modality=\"audio\",\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Beginning\")\n",
    "\n",
    "    config = \"config\\data\\setup-c\\moisesdb-everything-query-d-aug.yml\"\n",
    "\n",
    "    config = OmegaConf.load(config)\n",
    "\n",
    "    print(\"Loaded config\")\n",
    "\n",
    "    dataset = MoisesDBRandomChunkRandomQueryDataset(\n",
    "        data_root=config.data_root, split=\"train\", **config.train_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"Loaded dataset\")\n",
    "\n",
    "    for item in tqdm(dataset, total=len(dataset)):\n",
    "        target_audio = item[\"sources\"][\"target\"][\"audio\"]\n",
    "        mixture = item[\"mixture\"][\"audio\"]\n",
    "\n",
    "        if target_audio is None:\n",
    "            raise ValueError\n",
    "        else:\n",
    "            tdb = 10.0 * torch.log10(torch.mean(torch.square(target_audio)) + 1e-6)\n",
    "            mdb = 10.0 * torch.log10(torch.mean(torch.square(mixture)) + 1e-6)\n",
    "            print(f\"Target db: {tdb}, Mixture db: {mdb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moises data modules\n",
    "import os.path\n",
    "from typing import Mapping, Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# from core.data.base import from_datasets\n",
    "# from core.data.moisesdb.dataset import MoisesDBRandomChunkBalancedRandomQueryDataset, MoisesDBRandomChunkRandomQueryDataset, \\\n",
    "#     MoisesDBDeterministicChunkDeterministicQueryDataset, \\\n",
    "#     MoisesDBFullTrackDataset, MoisesDBVDBODeterministicChunkDataset, \\\n",
    "#     MoisesDBVDBOFullTrackDataset, MoisesDBVDBORandomChunkDataset, \\\n",
    "#     MoisesDBFullTrackTestQueryDataset\n",
    "    \n",
    "def MoisesDataModule(\n",
    "    data_root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 8,\n",
    "    train_kwargs: Optional[Mapping] = None,\n",
    "    val_kwargs: Optional[Mapping] = None,\n",
    "    test_kwargs: Optional[Mapping] = None,\n",
    "    datamodule_kwargs: Optional[Mapping] = None,\n",
    ") -> pl.LightningDataModule:\n",
    "    if train_kwargs is None:\n",
    "        train_kwargs = {}\n",
    "\n",
    "    if val_kwargs is None:\n",
    "        val_kwargs = {}\n",
    "\n",
    "    if test_kwargs is None:\n",
    "        test_kwargs = {}\n",
    "\n",
    "    if datamodule_kwargs is None:\n",
    "        datamodule_kwargs = {}\n",
    "\n",
    "    train_dataset = MoisesDBRandomChunkRandomQueryDataset(\n",
    "        data_root=data_root, split=\"train\", **train_kwargs\n",
    "    )\n",
    "\n",
    "    val_dataset = MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "        data_root=data_root, split=\"val\", **val_kwargs\n",
    "    )\n",
    "\n",
    "    test_dataset = MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "        data_root=data_root, split=\"test\", **test_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule = from_datasets(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **datamodule_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule.predict_dataloader = (  # type: ignore[method-assign]\n",
    "        datamodule.test_dataloader\n",
    "    )\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "def MoisesBalancedTrainDataModule(\n",
    "    data_root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 8,\n",
    "    train_kwargs: Optional[Mapping] = None,\n",
    "    val_kwargs: Optional[Mapping] = None,\n",
    "    test_kwargs: Optional[Mapping] = None,\n",
    "    datamodule_kwargs: Optional[Mapping] = None,\n",
    ") -> pl.LightningDataModule:\n",
    "    if train_kwargs is None:\n",
    "        train_kwargs = {}\n",
    "\n",
    "    if val_kwargs is None:\n",
    "        val_kwargs = {}\n",
    "\n",
    "    if test_kwargs is None:\n",
    "        test_kwargs = {}\n",
    "\n",
    "    if datamodule_kwargs is None:\n",
    "        datamodule_kwargs = {}\n",
    "\n",
    "    train_dataset = MoisesDBRandomChunkBalancedRandomQueryDataset(\n",
    "        data_root=data_root, split=\"train\", **train_kwargs\n",
    "    )\n",
    "\n",
    "    val_dataset = MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "        data_root=data_root, split=\"val\", **val_kwargs\n",
    "    )\n",
    "\n",
    "    test_dataset = MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "        data_root=data_root, split=\"test\", **test_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule = from_datasets(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **datamodule_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule.predict_dataloader = (  # type: ignore[method-assign]\n",
    "        datamodule.test_dataloader\n",
    "    )\n",
    "\n",
    "    return datamodule\n",
    "    \n",
    "\n",
    "def MoisesValidationDataModule(\n",
    "    data_root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 8,\n",
    "    val_kwargs: Optional[Mapping] = None,\n",
    "    datamodule_kwargs: Optional[Mapping] = None,\n",
    "    **kwargs\n",
    ") -> pl.LightningDataModule:\n",
    "    if val_kwargs is None:\n",
    "        val_kwargs = {}\n",
    "\n",
    "    if datamodule_kwargs is None:\n",
    "        datamodule_kwargs = {}\n",
    "        \n",
    "    allowed_stems = val_kwargs.get(\"allowed_stems\", None)\n",
    "    \n",
    "    assert allowed_stems is not None, \"allowed_stems must be provided\"\n",
    "    \n",
    "    val_datasets = []\n",
    "    \n",
    "    for allowed_stem in allowed_stems:\n",
    "        kwargs = val_kwargs.copy()\n",
    "        kwargs[\"allowed_stems\"] = [allowed_stem]\n",
    "        val_dataset = MoisesDBDeterministicChunkDeterministicQueryDataset(\n",
    "            data_root=data_root, split=\"val\", \n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        val_datasets.append(val_dataset)\n",
    "\n",
    "    datamodule = from_datasets(\n",
    "        val_dataset=val_datasets,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **datamodule_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule.predict_dataloader = (  # type: ignore[method-assign]\n",
    "        datamodule.val_dataloader\n",
    "    )\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "def MoisesTestDataModule(\n",
    "    data_root: str,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 8,\n",
    "    test_kwargs: Optional[Mapping] = None,\n",
    "    datamodule_kwargs: Optional[Mapping] = None,\n",
    "    **kwargs\n",
    ") -> pl.LightningDataModule:\n",
    "    if test_kwargs is None:\n",
    "        test_kwargs = {}\n",
    "\n",
    "    if datamodule_kwargs is None:\n",
    "        datamodule_kwargs = {}\n",
    "        \n",
    "    allowed_stems = test_kwargs.get(\"allowed_stems\", None)\n",
    "    \n",
    "    assert allowed_stems is not None, \"allowed_stems must be provided\"\n",
    "\n",
    "    test_dataset = MoisesDBFullTrackTestQueryDataset(\n",
    "        data_root=data_root, split=\"test\",\n",
    "        **test_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule = from_datasets(\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **datamodule_kwargs\n",
    "    )\n",
    "\n",
    "    datamodule.predict_dataloader = (  # type: ignore[method-assign]\n",
    "        datamodule.test_dataloader\n",
    "    )\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "\n",
    "def MoisesVDBODataModule(\n",
    "    data_root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 8,\n",
    "    train_kwargs: Optional[Mapping] = None,\n",
    "    val_kwargs: Optional[Mapping] = None,\n",
    "    test_kwargs: Optional[Mapping] = None,\n",
    "    datamodule_kwargs: Optional[Mapping] = None,\n",
    "):\n",
    "    \n",
    "    \n",
    "    if train_kwargs is None:\n",
    "        train_kwargs = {}\n",
    "\n",
    "    if val_kwargs is None:\n",
    "        val_kwargs = {}\n",
    "\n",
    "    if test_kwargs is None:\n",
    "        test_kwargs = {}\n",
    "\n",
    "    if datamodule_kwargs is None:\n",
    "        datamodule_kwargs = {}\n",
    "        \n",
    "    train_dataset = MoisesDBVDBORandomChunkDataset(\n",
    "        data_root=data_root, split=\"train\", **train_kwargs\n",
    "    )\n",
    "    \n",
    "    val_dataset = MoisesDBVDBODeterministicChunkDataset(\n",
    "        data_root=data_root, split=\"val\", **val_kwargs\n",
    "    )\n",
    "    \n",
    "    test_dataset = MoisesDBVDBOFullTrackDataset(\n",
    "        data_root=data_root, split=\"test\", **test_kwargs\n",
    "    )\n",
    "    \n",
    "    predict_dataset = test_dataset\n",
    "    \n",
    "    datamodule = from_datasets(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        predict_dataset=predict_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **datamodule_kwargs\n",
    "    )\n",
    "    \n",
    "    return datamodule\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **npyify.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot find key: --f=c:\\Users\\Dell\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3f0d43beaf9ae7787c0ec59baf0b0092a7079c985.json\n",
      "Usage: ipykernel_launcher.py <group|command|value>\n",
      "  available groups:      In | Out | exit | quit | os | Callable | np | torch |\n",
      "                         taF | pd | band_defs | mbs | df | nn | math |\n",
      "                         activation_ | warnings | F | rnn | model |\n",
      "                         input_data | Dict | List | Optional | Tuple | Type |\n",
      "                         activation | ta | optim | tm | Union | pl | Iterator |\n",
      "                         Mapping | lr_scheduler | inspect | Sequence | data |\n",
      "                         taxonomy | random | INST_BY_OCCURRENCE |\n",
      "                         FINE_LEVEL_INSTRUMENTS | COARSE_LEVEL_INSTRUMENTS |\n",
      "                         COARSE_TO_FINE | FINE_TO_COARSE |\n",
      "                         ALL_LEVEL_INSTRUMENTS | audiomentations | glob |\n",
      "                         json | shutil | librosa | fine_to_coarse | v |\n",
      "                         possible_coarse | possible_fine | fire\n",
      "  available commands:    get_ipython | open | abstractmethod | Any |\n",
      "                         hz_to_midi | midi_to_hz | Tensor |\n",
      "                         band_widths_from_specs | check_nonzero_bandwidth |\n",
      "                         check_no_overlap | check_no_gap |\n",
      "                         BandsplitSpecification | VocalBandsplitSpecification |\n",
      "                         OtherBandsplitSpecification |\n",
      "                         BassBandsplitSpecification |\n",
      "                         DrumBandsplitSpecification |\n",
      "                         PerceptualBandsplitSpecification | mel_filterbank |\n",
      "                         MelBandsplitSpecification | musical_filterbank |\n",
      "                         MusicalBandsplitSpecification | bands | Conditioning |\n",
      "                         PassThroughConditioning | FiLM | CosineSimiliarity |\n",
      "                         GeneralizedBilinear | BilinearFiLM |\n",
      "                         TimeFrequencyModellingModule | ResidualRNN |\n",
      "                         SeqBandModellingModule | BaseNormMLP | NormMLP |\n",
      "                         MaskEstimationModuleSuperBase |\n",
      "                         MaskEstimationModuleBase |\n",
      "                         OverlappingMaskEstimationModule |\n",
      "                         MaskEstimationModule | NormFC | BandSplitModule |\n",
      "                         get_basic_model | Passt | PasstWrapper |\n",
      "                         SimpleNamespace | TypedDict | OperationMode |\n",
      "                         RawInputType | nested_dict_to_nested_namespace |\n",
      "                         input_dict | SimpleishNamespace | BatchedInputOutput |\n",
      "                         TensorCollection | InputType | OutputType |\n",
      "                         LossOutputType | MetricOutputType | ModelType |\n",
      "                         OptimizerType | SchedulerType | MetricType |\n",
      "                         LossType | OptimizationBundle | LossHandler |\n",
      "                         MetricHandler | AugmentationHandler |\n",
      "                         InferenceHandler | BaseEndToEndModule | BaseBandit |\n",
      "                         Bandit | BaseConditionedBandit |\n",
      "                         PasstFiLMConditionedBandit | defaultdict | chain |\n",
      "                         combinations | pprint | LRScheduler | tqdm |\n",
      "                         EndToEndLightningSystem | ABC | EVAL_DATALOADERS |\n",
      "                         TRAIN_DATALOADERS | LightningDataModule | Dataset |\n",
      "                         DataLoader | IterableDataset | from_datasets |\n",
      "                         BaseSourceSeparationDataset | clean_track_inst |\n",
      "                         OmegaConf | ObjectDict | MoisesDBBaseDataset |\n",
      "                         MoisesDBFullTrackDataset |\n",
      "                         MoisesDBVDBOFullTrackDataset |\n",
      "                         convert_decibels_to_amplitude_ratio | SmartGain |\n",
      "                         Audiomentations | MoisesDBVDBORandomChunkDataset |\n",
      "                         MoisesDBVDBODeterministicChunkDataset |\n",
      "                         round_samples |\n",
      "                         MoisesDBRandomChunkRandomQueryDataset |\n",
      "                         MoisesDBRandomChunkBalancedRandomQueryDataset |\n",
      "                         MoisesDBDeterministicChunkDeterministicQueryDataset |\n",
      "                         MoisesDBFullTrackTestQueryDataset | MoisesDataModule |\n",
      "                         MoisesBalancedTrainDataModule |\n",
      "                         MoisesValidationDataModule | MoisesTestDataModule |\n",
      "                         MoisesVDBODataModule | process_map | tdqm |\n",
      "                         clean_npy_other_vox | save_taxonomy | trim_and_mix |\n",
      "                         retrim_npys | convert_one | convert_to_npy |\n",
      "                         make_others_one | check_vdbo_one | check_vdbo |\n",
      "                         make_others | extract_metadata_one |\n",
      "                         consolidate_metadata | clean_canonical | remove_dbfs |\n",
      "                         make_split | consolidate_stems | get_dbfs |\n",
      "                         get_dbfs_by_chunk_one | clean_data_root |\n",
      "                         get_dbfs_by_chunk | get_query_one |\n",
      "                         get_query_from_onset | get_durations |\n",
      "                         clean_query_root | make_test_indices\n",
      "  available values:      band_name | i | f_min | f_max | device |\n",
      "                         DBFS_HOP_SIZE | DBFS_CHUNK_SIZE | config | k | vv\n",
      "\n",
      "For detailed information on this command, run:\n",
      "  ipykernel_launcher.py --help\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mFireExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\demix\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "from tqdm import tqdm as tdqm, tqdm\n",
    "import torchaudio as ta\n",
    "\n",
    "import librosa\n",
    "\n",
    "taxonomy = {\n",
    "    \"vocals\": [\n",
    "        \"lead male singer\",\n",
    "        \"lead female singer\",\n",
    "        \"human choir\",\n",
    "        \"background vocals\",\n",
    "        \"other (vocoder, beatboxing etc)\",\n",
    "    ],\n",
    "    \"bass\": [\n",
    "        \"bass guitar\",\n",
    "        \"bass synthesizer (moog etc)\",\n",
    "        \"contrabass/double bass (bass of instrings)\",\n",
    "        \"tuba (bass of brass)\",\n",
    "        \"bassoon (bass of woodwind)\",\n",
    "    ],\n",
    "    \"drums\": [\n",
    "        \"snare drum\",\n",
    "        \"toms\",\n",
    "        \"kick drum\",\n",
    "        \"cymbals\",\n",
    "        \"overheads\",\n",
    "        \"full acoustic drumkit\",\n",
    "        \"drum machine\",\n",
    "        \"hi-hat\"\n",
    "    ],\n",
    "    \"other\": [\n",
    "        \"fx/processed sound, scratches, gun shots, explosions etc\",\n",
    "        \"click track\",\n",
    "    ],\n",
    "    \"guitar\": [\n",
    "        \"clean electric guitar\",\n",
    "        \"distorted electric guitar\",\n",
    "        \"lap steel guitar or slide guitar\",\n",
    "        \"acoustic guitar\",\n",
    "    ],\n",
    "    \"other plucked\": [\"banjo, mandolin, ukulele, harp etc\"],\n",
    "    \"percussion\": [\n",
    "        \"a-tonal percussion (claps, shakers, congas, cowbell etc)\",\n",
    "        \"pitched percussion (mallets, glockenspiel, ...)\",\n",
    "    ],\n",
    "    \"piano\": [\n",
    "        \"grand piano\",\n",
    "        \"electric piano (rhodes, wurlitzer, piano sound alike)\",\n",
    "    ],\n",
    "    \"other keys\": [\n",
    "        \"organ, electric organ\",\n",
    "        \"synth pad\",\n",
    "        \"synth lead\",\n",
    "        \"other sounds (hapischord, melotron etc)\",\n",
    "    ],\n",
    "    \"bowed strings\": [\n",
    "        \"violin (solo)\",\n",
    "        \"viola (solo)\",\n",
    "        \"cello (solo)\",\n",
    "        \"violin section\",\n",
    "        \"viola section\",\n",
    "        \"cello section\",\n",
    "        \"string section\",\n",
    "        \"other strings\",\n",
    "    ],\n",
    "    \"wind\": [\n",
    "        \"brass (trumpet, trombone, french horn, brass etc)\",\n",
    "        \"flutes (piccolo, bamboo flute, panpipes, flutes etc)\",\n",
    "        \"reeds (saxophone, clarinets, oboe, english horn, bagpipe)\",\n",
    "        \"other wind\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def clean_npy_other_vox(data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npyq\"):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "    \n",
    "    \n",
    "    npys = [npy for npy in npys if \"other\" in npy]\n",
    "    npys = [npy for npy in npys if \"vdbo_\" not in npy]\n",
    "    npys = [npy for npy in npys if \"other_\" not in npy]\n",
    "\n",
    "    stems = set([\n",
    "        os.path.basename(npy).split(\".\")[0] for npy in npys\n",
    "    ])\n",
    "    \n",
    "    assert len(stems) == 1\n",
    "    \n",
    "    for npy in tqdm(npys):\n",
    "        shutil.move(npy, npy.replace(\"other\", \"other_vocals\"))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def clean_track_inst(inst):\n",
    "    \n",
    "    if \"vocoder\" in inst:\n",
    "        inst = \"other_vocals\"\n",
    "\n",
    "    if \"fx\" in inst:\n",
    "        inst = \"fx\"\n",
    "\n",
    "    if \"contrabass_double_bass\" in inst:\n",
    "        inst = \"double_bass\"\n",
    "\n",
    "    if \"banjo\" in inst:\n",
    "        return \"other_plucked\"\n",
    "\n",
    "    if \"(\" in inst:\n",
    "        inst = inst.split(\"(\")[0]\n",
    "\n",
    "    for s in [\",\", \"-\"]:\n",
    "        if s in inst:\n",
    "            inst = inst.replace(s, \"\")\n",
    "\n",
    "    for s in [\"/\"]:\n",
    "        if s in inst:\n",
    "            inst = inst.replace(s, \"_\")\n",
    "\n",
    "    if inst[-1] == \"_\":\n",
    "        inst = inst[:-1]\n",
    "\n",
    "    return inst\n",
    "\n",
    "\n",
    "taxonomy = {\n",
    "    k.replace(\" \", \"_\"): [clean_track_inst(i.replace(\" \", \"_\")) for i in v] for k, v in taxonomy.items()\n",
    "}\n",
    "\n",
    "fine_to_coarse = {}\n",
    "\n",
    "for k, v in taxonomy.items():\n",
    "    for vv in v:\n",
    "        fine_to_coarse[vv] = k\n",
    "\n",
    "# pprint(fine_to_coarse)\n",
    "\n",
    "def save_taxonomy():\n",
    "    with open(\"taxonomy.json\", \"w\") as f:\n",
    "        json.dump(taxonomy, f, indent=4)\n",
    "\n",
    "    taxonomy_coarse = list(taxonomy.keys())\n",
    "    \n",
    "    with open(\"taxonomy_coarse.json\", \"w\") as f:\n",
    "        json.dump(taxonomy_coarse, f, indent=4)\n",
    "        \n",
    "    taxonomy_fine = list(chain(*taxonomy.values()))\n",
    "    \n",
    "    count_ = defaultdict(int)\n",
    "    for t in taxonomy_fine:\n",
    "        count_[t] += 1\n",
    "        \n",
    "    with open(\"taxonomy_fine.json\", \"w\") as f:\n",
    "        json.dump(taxonomy_fine, f, indent=4)\n",
    "    \n",
    "\n",
    "\n",
    "possible_coarse = list(taxonomy.keys())\n",
    "possible_fine = list(set(chain(*taxonomy.values())))\n",
    "\n",
    "\n",
    "def trim_and_mix(audios, length_=None):\n",
    "    length = min([a.shape[-1] for a in audios])\n",
    "    \n",
    "    if length_ is not None:\n",
    "        length = min(length, length_)\n",
    "    \n",
    "    audios = [a[..., :length] for a in audios]\n",
    "    return np.sum(np.stack(audios, axis=0), axis=0), length\n",
    "\n",
    "\n",
    "def retrim_npys(saved_npy, new_length):\n",
    "    print(\"retrimming\")\n",
    "    for npy in saved_npy:\n",
    "        audio = np.load(npy)\n",
    "        audio = audio[..., :new_length]\n",
    "        np.save(npy, audio)\n",
    "\n",
    "\n",
    "def convert_one(inout):\n",
    "    input_path = inout.input_path\n",
    "    output_root = inout.output_root\n",
    "\n",
    "    song_id = os.path.basename(input_path)\n",
    "    output_root = os.path.join(output_root, song_id)\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    metadata = OmegaConf.load(os.path.join(input_path, \"data.json\"))\n",
    "    stems = metadata.stems\n",
    "\n",
    "    min_length = None\n",
    "    saved_npy = []\n",
    "\n",
    "    all_tracks = []\n",
    "    other_tracks = []\n",
    "    \n",
    "    outfile = None\n",
    "    \n",
    "    added_tracks = set()\n",
    "    duplicated_tracks = set()\n",
    "    track_to_stem = defaultdict(list)\n",
    "    added_stems = set()\n",
    "    duplicated_stems = set()\n",
    "    \n",
    "    stem_name_to_stems = defaultdict(list)\n",
    "    \n",
    "    for stem in stems:\n",
    "        stem_name = stem.stemName\n",
    "        stem_name_to_stems[stem_name].append(stem)\n",
    "    \n",
    "        \n",
    "    for stem_name in tqdm(stem_name_to_stems):\n",
    "        stem_tracks = []\n",
    "        for stem in stem_name_to_stems[stem_name]:\n",
    "            stem_name = stem.stemName\n",
    "            \n",
    "            if stem_name in added_stems:\n",
    "                print(f\"Duplicate stem {stem_name} in {song_id}\")\n",
    "                duplicated_stems.add(stem_name)\n",
    "            \n",
    "            added_stems.add(stem_name)\n",
    "            \n",
    "            for track in stem.tracks:\n",
    "                track_inst = track.trackType\n",
    "                track_inst = clean_track_inst(track_inst)\n",
    "                \n",
    "                if track_inst in added_tracks:\n",
    "                    if stem_name in track_to_stem[track_inst]:\n",
    "                        continue\n",
    "                    print(f\"Duplicate track {track_inst} in {song_id}\")\n",
    "                    print(f\"Stems: {track_to_stem[track_inst]}\")\n",
    "                    duplicated_tracks.add(track_inst)\n",
    "                    raise ValueError\n",
    "                else:\n",
    "                    added_tracks.add(track_inst)\n",
    "                    \n",
    "                track_to_stem[track_inst].append(stem_name)\n",
    "                track_id = track.id\n",
    "                \n",
    "                audio, fs = ta.load(os.path.join(input_path, stem_name, f\"{track_id}.wav\"))\n",
    "\n",
    "                if fs != 44100:\n",
    "                    print(f\"fs is {fs} for {track_id}\")\n",
    "                    with open(os.path.join(output_root, \"fs.txt\"), \"w\") as f:\n",
    "                        f.write(f\"{song_id}\\t{track_id}\\t{fs}\\n\")\n",
    "\n",
    "                if min_length is None:\n",
    "                    min_length = audio.shape[-1]\n",
    "                else:\n",
    "                    if audio.shape[-1] < min_length:\n",
    "                        min_length = audio.shape[-1]\n",
    "\n",
    "                        if len(saved_npy) > 0:\n",
    "                            retrim_npys(saved_npy, min_length)\n",
    "\n",
    "                audio = audio[..., :min_length]\n",
    "                audio = audio.numpy()\n",
    "                audio = audio.astype(np.float32)\n",
    "\n",
    "                if audio.shape[0] == 1:\n",
    "                    print(\"mono\")\n",
    "                if audio.shape[0] > 2:\n",
    "                    print(\"multi channel\")\n",
    "\n",
    "                assert outfile is None\n",
    "                outfile = os.path.join(output_root, f\"{track_inst}.npy\")\n",
    "                np.save(outfile, audio)\n",
    "                saved_npy.append(outfile)\n",
    "                outfile = None\n",
    "                stem_tracks.append(audio)\n",
    "                audio = None\n",
    "                \n",
    "        stem_track, min_length = trim_and_mix(stem_tracks)\n",
    "\n",
    "        assert outfile is None\n",
    "        outfile = os.path.join(output_root, f\"{stem_name}.npy\")\n",
    "        np.save(outfile, stem_track)\n",
    "        saved_npy.append(outfile)\n",
    "        outfile = None\n",
    "        \n",
    "        all_tracks.append(stem_track)\n",
    "        \n",
    "        if stem_name not in [\"vocals\", \"drums\", \"bass\"]:\n",
    "            # print(f\"Putting {stem_name} in other\")\n",
    "            other_tracks.append(stem_track)\n",
    "            \n",
    "        \n",
    "    assert outfile is None\n",
    "    all_track, min_length_ = trim_and_mix(all_tracks, min_length)\n",
    "    outfile = os.path.join(output_root, f\"mixture.npy\")\n",
    "    np.save(outfile, all_track)\n",
    "    \n",
    "    if min_length_ != min_length:\n",
    "        retrim_npys(saved_npy, min_length_)\n",
    "        min_length = min_length_\n",
    "    \n",
    "    saved_npy.append(outfile)\n",
    "    outfile = None\n",
    "    \n",
    "    other_track, min_length_ = trim_and_mix(other_tracks, min_length)\n",
    "    np.save(os.path.join(output_root, f\"vdbo_others.npy\"), other_track)\n",
    "    \n",
    "    if min_length_ != min_length:\n",
    "        retrim_npys(saved_npy, min_length_)\n",
    "        min_length = min_length_\n",
    "\n",
    "\n",
    "def convert_to_npy(\n",
    "    data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/canonical\",\n",
    "    output_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npy2\",\n",
    "):\n",
    "    if output_root is None:\n",
    "        output_root = os.path.join(os.path.dirname(data_root), \"npy\")\n",
    "\n",
    "    files = os.listdir(data_root)\n",
    "    files = [\n",
    "        os.path.join(data_root, f)\n",
    "        for f in files\n",
    "        if os.path.isdir(os.path.join(data_root, f))\n",
    "    ]\n",
    "\n",
    "    inout = [SimpleNamespace(input_path=f, output_root=output_root) for f in files]\n",
    "\n",
    "    process_map(convert_one, inout)\n",
    "\n",
    "    # for io in tdqm(inout):\n",
    "    #     convert_one(io)\n",
    "\n",
    "\n",
    "def make_others_one(input_path, dry_run=False):\n",
    "\n",
    "    other_stems = [k for k in taxonomy.keys() if k not in [\"vocals\", \"bass\", \"drums\"]]\n",
    "    npys = glob.glob(os.path.join(input_path, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    npys = [npy for npy in npys if \".dbfs\" not in npy]\n",
    "    npys = [npy for npy in npys if \".query\" not in npy]\n",
    "    npys = [npy for npy in npys if \"mixture\" not in npy]\n",
    "    npys = [npy for npy in npys if os.path.basename(npy).split(\".\")[0] in other_stems]\n",
    "\n",
    "    print(f\"Using stems: {[os.path.basename(npy).split('.')[0] for npy in npys]}\")\n",
    "\n",
    "    if len(npys) == 0:\n",
    "        audio = np.zeros_like(np.load(os.path.join(input_path, \"mixture.npy\")))\n",
    "    else:\n",
    "        audio = [np.load(npy) for npy in npys]\n",
    "\n",
    "        audio = np.sum(np.stack(audio, axis=0), axis=0)\n",
    "    assert audio.shape[0] == 2\n",
    "\n",
    "    output = os.path.join(input_path, \"vdbo_others.npy\")\n",
    "\n",
    "    if dry_run:\n",
    "        return\n",
    "\n",
    "    np.save(output, audio)\n",
    "\n",
    "\n",
    "def check_vdbo_one(f):\n",
    "    s = np.sum(\n",
    "        np.stack(\n",
    "            [\n",
    "                np.load(os.path.join(f, s + \".npy\"))\n",
    "                for s in [\"vocals\", \"drums\", \"bass\", \"vdbo_others\"]\n",
    "                if os.path.exists(os.path.join(f, s + \".npy\"))\n",
    "            ],\n",
    "            axis=0,\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "    m = np.load(os.path.join(f, \"mixture.npy\"))\n",
    "    snr = 10 * np.log10(np.mean(np.square(m)) / np.mean(np.square(s - m)))\n",
    "    print(snr)\n",
    "    \n",
    "    return snr\n",
    "\n",
    "def check_vdbo(data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npy2\"):\n",
    "    files = os.listdir(data_root)\n",
    "\n",
    "    files = [\n",
    "        os.path.join(data_root, f)\n",
    "        for f in files\n",
    "        if os.path.isdir(os.path.join(data_root, f))\n",
    "    ]\n",
    "\n",
    "    snrs = process_map(check_vdbo_one, files)\n",
    "\n",
    "    np.save(\"/storage/home/hcoda1/1/kwatchar3/data/vdbo.npy\", np.array(snrs))\n",
    "\n",
    "\n",
    "def make_others(data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npy2\"):\n",
    "\n",
    "    files = os.listdir(data_root)\n",
    "\n",
    "    files = [\n",
    "        os.path.join(data_root, f)\n",
    "        for f in files\n",
    "        if os.path.isdir(os.path.join(data_root, f))\n",
    "    ]\n",
    "\n",
    "    process_map(make_others_one, files)\n",
    "\n",
    "    # for f in tqdm(files):\n",
    "    #     make_others_one(f, dry_run=False)\n",
    "\n",
    "\n",
    "def extract_metadata_one(input_path):\n",
    "    song_id = os.path.basename(input_path)\n",
    "    metadata = OmegaConf.load(os.path.join(input_path, \"data.json\"))\n",
    "\n",
    "    song = metadata.song\n",
    "    artist = metadata.artist\n",
    "    genre = metadata.genre\n",
    "\n",
    "    stems = metadata.stems\n",
    "    data_out = []\n",
    "\n",
    "    for stem in stems:\n",
    "        stem_name = stem.stemName\n",
    "        stem_id = stem.id\n",
    "        for track in stem.tracks:\n",
    "            track_inst = track.trackType\n",
    "            track_id = track.id\n",
    "\n",
    "            data_out.append(\n",
    "                {\n",
    "                    \"song_id\": song_id,\n",
    "                    \"song\": song,\n",
    "                    \"artist\": artist,\n",
    "                    \"genre\": genre,\n",
    "                    \"stem_name\": stem_name,\n",
    "                    \"stem_id\": stem_id,\n",
    "                    \"track_inst\": track_inst,\n",
    "                    \"track_id\": track_id,\n",
    "                    \"has_bleed\": track.has_bleed,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return data_out\n",
    "\n",
    "\n",
    "def consolidate_metadata(\n",
    "    data_root=\"/home/kwatchar3/Documents/data/moisesdb/canonical\",\n",
    "):\n",
    "\n",
    "    files = os.listdir(data_root)\n",
    "    files = [\n",
    "        os.path.join(data_root, f)\n",
    "        for f in files\n",
    "        if os.path.isdir(os.path.join(data_root, f))\n",
    "    ]\n",
    "\n",
    "    data = process_map(extract_metadata_one, files)\n",
    "\n",
    "    df = pd.DataFrame.from_records(list(chain(*data)))\n",
    "\n",
    "    df.to_csv(os.path.join(os.path.dirname(data_root), \"metadata.csv\"), index=False)\n",
    "\n",
    "\n",
    "def clean_canonical(data_root=\"/home/kwatchar3/Documents/data/moisesdb/canonical\"):\n",
    "\n",
    "    npy = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    for n in tqdm(npy):\n",
    "        os.remove(n)\n",
    "\n",
    "\n",
    "def remove_dbfs(data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npy\"):\n",
    "    npy = glob.glob(os.path.join(data_root, \"**/*.dbfs.npy\"), recursive=True)\n",
    "\n",
    "    for n in tqdm(npy):\n",
    "        os.remove(n)\n",
    "\n",
    "\n",
    "def make_split(\n",
    "    metadata_path=\"/home/kwatchar3/Documents/data/moisesdb/metadata.csv\",\n",
    "    n_splits=5,\n",
    "    seed=42,\n",
    "):\n",
    "\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    # print(df.columns)\n",
    "    df = df[[\"song_id\", \"genre\"]].drop_duplicates()\n",
    "\n",
    "    genres = df[\"genre\"].value_counts()\n",
    "    genres_map = {g: g if c > n_splits else \"other\" for g, c in genres.items()}\n",
    "\n",
    "    df[\"genre\"] = df[\"genre\"].map(genres_map)\n",
    "\n",
    "    n_samples = len(df)\n",
    "    n_per_split = n_samples // n_splits\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    splits = []\n",
    "\n",
    "    df_ = df.copy()\n",
    "\n",
    "    for i in range(n_splits - 1):\n",
    "        df_, test = train_test_split(\n",
    "            df_,\n",
    "            test_size=n_per_split,\n",
    "            random_state=seed,\n",
    "            stratify=df_[\"genre\"],\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        dfs = test[[\"song_id\"]].copy().sort_values(by=\"song_id\")\n",
    "        dfs[\"split\"] = i + 1\n",
    "        splits.append(dfs)\n",
    "\n",
    "    test = df_\n",
    "    dfs = test[[\"song_id\"]].copy().sort_values(by=\"song_id\")\n",
    "    dfs[\"split\"] = n_splits\n",
    "    splits.append(dfs)\n",
    "\n",
    "    splits = pd.concat(splits)\n",
    "\n",
    "    splits.to_csv(\n",
    "        os.path.join(os.path.dirname(metadata_path), \"splits.csv\"), index=False\n",
    "    )\n",
    "\n",
    "\n",
    "def consolidate_stems(data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\"):\n",
    "\n",
    "    metadata = pd.read_csv(os.path.join(os.path.dirname(data_root), \"metadata.csv\"))\n",
    "\n",
    "    dfg = metadata.groupby(\"song_id\")[[\"stem_name\", \"track_inst\"]]\n",
    "\n",
    "    pprint(dfg)\n",
    "\n",
    "    df = []\n",
    "\n",
    "    def make_stem_dict(song_id, track_inst, stem_names):\n",
    "\n",
    "        d = {\"song_id\": song_id}\n",
    "\n",
    "        for inst in possible_fine:\n",
    "            d[inst] = int(inst in track_inst)\n",
    "\n",
    "        for inst in possible_coarse:\n",
    "            d[inst] = int(inst in stem_names)\n",
    "\n",
    "        return d\n",
    "\n",
    "    for song_id, dfgg in dfg:\n",
    "\n",
    "        track_inst = dfgg[\"track_inst\"].tolist()\n",
    "        track_inst = list(set(track_inst))\n",
    "        track_inst = [clean_track_inst(inst) for inst in track_inst]\n",
    "\n",
    "        stem_names = dfgg[\"stem_name\"].tolist()\n",
    "        stem_names = list(set([clean_track_inst(inst) for inst in stem_names]))\n",
    "\n",
    "        d = make_stem_dict(song_id, track_inst, stem_names)\n",
    "        df.append(d)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df = pd.DataFrame.from_records(df)\n",
    "\n",
    "    df.to_csv(os.path.join(os.path.dirname(data_root), \"stems.csv\"), index=False)\n",
    "\n",
    "\n",
    "def get_dbfs(data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\"):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    dbfs = []\n",
    "\n",
    "    for npy in tqdm(npys):\n",
    "        audio = np.load(npy)\n",
    "        song_id = os.path.basename(os.path.dirname(npy))\n",
    "        track_id = os.path.basename(npy).split(\".\")[0]\n",
    "\n",
    "        dbfs.append(\n",
    "            {\n",
    "                \"song_id\": song_id,\n",
    "                \"track_id\": track_id,\n",
    "                \"dbfs\": 10 * np.log10(np.mean(np.square(audio))),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    dbfs = pd.DataFrame.from_records(dbfs)\n",
    "\n",
    "    dbfs.to_csv(os.path.join(os.path.dirname(data_root), \"dbfs.csv\"), index=False)\n",
    "\n",
    "    return dbfs\n",
    "\n",
    "\n",
    "def get_dbfs_by_chunk_one(inout):\n",
    "\n",
    "    audio = np.load(inout.audio_path, mmap_mode=\"r\")\n",
    "    chunk_size = inout.chunk_size\n",
    "    fs = inout.fs\n",
    "    hop_size = inout.hop_size\n",
    "\n",
    "    n_chan, n_samples = audio.shape\n",
    "    chunk_size_samples = int(chunk_size * fs)\n",
    "    hop_size_samples = int(hop_size * fs)\n",
    "\n",
    "    x2win = np.lib.stride_tricks.sliding_window_view(\n",
    "        np.square(audio), chunk_size_samples, axis=1\n",
    "    )[:, ::hop_size_samples, :]\n",
    "\n",
    "    x2win_mean = np.mean(x2win, axis=(0, 2))\n",
    "    x2win_mean[x2win_mean == 0] = 1e-8\n",
    "    dbfs = 10 * np.log10(x2win_mean)\n",
    "\n",
    "    # song_id = os.path.basename(os.path.dirname(inout.audio_path))\n",
    "    track_id = os.path.basename(inout.audio_path).split(\".\")[0]\n",
    "\n",
    "    np.save(\n",
    "        os.path.join(os.path.dirname(inout.audio_path), f\"{track_id}.dbfs.npy\"), dbfs\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_data_root(data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\"):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    for npy in tqdm(npys):\n",
    "        if \".dbfs\" in npy or \".query\" in npy:\n",
    "            # print(\"removing\", npy)\n",
    "            os.remove(npy)\n",
    "\n",
    "\n",
    "#\n",
    "def get_dbfs_by_chunk(\n",
    "    data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\",\n",
    "    query_root=\"/home/kwatchar3/Documents/data/moisesdb/npyq\",\n",
    "):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    inout = [\n",
    "        SimpleNamespace(\n",
    "            audio_path=npy,\n",
    "            chunk_size=1,\n",
    "            hop_size=0.125,\n",
    "            fs=44100,\n",
    "            output_path=npy.replace(data_root, query_root).replace(\n",
    "                \".npy\", \".query.npy\"\n",
    "            ),\n",
    "        )\n",
    "        for npy in npys\n",
    "    ]\n",
    "\n",
    "    process_map(get_dbfs_by_chunk_one, inout, chunksize=2)\n",
    "\n",
    "\n",
    "def round_samples(seconds, fs, hop_size, downsample):\n",
    "    n_frames = math.ceil(seconds * fs / hop_size) + 1\n",
    "    n_frames_down = math.ceil(n_frames / downsample)\n",
    "    n_frames = n_frames_down * downsample\n",
    "    n_samples = (n_frames - 1) * hop_size\n",
    "\n",
    "    return int(n_samples)\n",
    "\n",
    "\n",
    "def get_query_one(inout):\n",
    "\n",
    "    audio = np.load(inout.audio_path, mmap_mode=\"r\")\n",
    "    chunk_size = inout.chunk_size\n",
    "    fs = inout.fs\n",
    "    output_path = inout.output_path\n",
    "    round = inout.round\n",
    "    hop_size = inout.hop_size\n",
    "\n",
    "    if round:\n",
    "        chunk_size_samples = round_samples(chunk_size, fs, 512, 2**6)\n",
    "    else:\n",
    "        chunk_size_samples = int(chunk_size * fs)\n",
    "\n",
    "    audio_mono = np.mean(audio, axis=0)\n",
    "\n",
    "    onset = librosa.onset.onset_detect(\n",
    "        y=audio_mono, sr=fs, units=\"frames\", hop_length=hop_size\n",
    "    )\n",
    "\n",
    "    onset_strength = librosa.onset.onset_strength(\n",
    "        y=audio_mono, sr=fs, hop_length=hop_size\n",
    "    )\n",
    "\n",
    "    n_frames_per_chunk = chunk_size_samples // hop_size\n",
    "\n",
    "    onset_strength_slide = np.lib.stride_tricks.sliding_window_view(\n",
    "        onset_strength, n_frames_per_chunk, axis=0\n",
    "    )\n",
    "\n",
    "    onset_strength = np.mean(onset_strength_slide, axis=1)\n",
    "\n",
    "    max_onset_frame = np.argmax(onset_strength)\n",
    "\n",
    "    max_onset_samples = librosa.frames_to_samples(max_onset_frame)\n",
    "\n",
    "    track_id = os.path.basename(inout.audio_path).split(\".\")[0]\n",
    "\n",
    "    segment = audio[:, max_onset_samples : max_onset_samples + chunk_size_samples]\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    np.save(output_path, segment)\n",
    "\n",
    "\n",
    "def get_query_from_onset(\n",
    "    data_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npy2\",  # \"/home/kwatchar3/Documents/data/moisesdb/npy\",\n",
    "    query_root=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/npyq\",  # \"/home/kwatchar3/Documents/data/moisesdb/npyq\",\n",
    "    query_file=\"query-10s\",\n",
    "    pmap=True,\n",
    "):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.npy\"), recursive=True)\n",
    "\n",
    "    npys = [npy for npy in npys if \"dbfs\" not in npy]\n",
    "\n",
    "    inout = [\n",
    "        SimpleNamespace(\n",
    "            audio_path=npy,\n",
    "            chunk_size=10,\n",
    "            hop_size=512,\n",
    "            round=False,\n",
    "            fs=44100,\n",
    "            output_path=npy.replace(data_root, query_root).replace(\n",
    "                \".npy\", f\".{query_file}.npy\"\n",
    "            ),\n",
    "        )\n",
    "        for npy in npys\n",
    "    ]\n",
    "\n",
    "    if pmap:\n",
    "        process_map(get_query_one, inout, chunksize=2, max_workers=24)\n",
    "    else:\n",
    "        for io in tqdm(inout):\n",
    "            get_query_one(io)\n",
    "\n",
    "\n",
    "def get_durations(data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\"):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/mixture.npy\"), recursive=True)\n",
    "\n",
    "    durations = []\n",
    "\n",
    "    for npy in tqdm(npys):\n",
    "        audio = np.load(npy, mmap_mode=\"r\")\n",
    "        song_id = os.path.basename(os.path.dirname(npy))\n",
    "        track_id = os.path.basename(npy).split(\".\")[0]\n",
    "\n",
    "        durations.append(\n",
    "            {\n",
    "                \"song_id\": song_id,\n",
    "                \"track_id\": track_id,\n",
    "                \"duration\": audio.shape[-1] / 44100,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    durations = pd.DataFrame.from_records(durations)\n",
    "\n",
    "    durations.to_csv(\n",
    "        os.path.join(os.path.dirname(data_root), \"durations.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    return durations\n",
    "\n",
    "\n",
    "def clean_query_root(\n",
    "    data_root=\"/home/kwatchar3/Documents/data/moisesdb/npy\",\n",
    "    query_root=\"/home/kwatchar3/Documents/data/moisesdb/npyq\",\n",
    "):\n",
    "    npys = glob.glob(os.path.join(data_root, \"**/*.query.npy\"), recursive=True)\n",
    "\n",
    "    for npy in tqdm(npys):\n",
    "        dst = npy.replace(data_root, query_root)\n",
    "        dstdir = os.path.dirname(dst)\n",
    "        os.makedirs(dstdir, exist_ok=True)\n",
    "        shutil.move(npy, dst)\n",
    "\n",
    "\n",
    "def make_test_indices(\n",
    "    metadata_path=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/metadata.csv\",\n",
    "    stem_path=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/stems.csv\",\n",
    "    splits_path=\"/storage/home/hcoda1/1/kwatchar3/data/data/moisesdb/splits.csv\",\n",
    "    test_split=5,\n",
    "):\n",
    "    \n",
    "    coarse_stems = set(taxonomy.keys())\n",
    "    fine_stems = set(chain(*taxonomy.values()))\n",
    "\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    splits = pd.read_csv(splits_path)\n",
    "    stems = pd.read_csv(stem_path)\n",
    "\n",
    "    file_in_test = splits[splits[\"split\"] == test_split][\"song_id\"].tolist()\n",
    "    \n",
    "    stems_test = stems[stems[\"song_id\"].isin(file_in_test)]\n",
    "    metadata_test = metadata[metadata[\"song_id\"].isin(file_in_test)]\n",
    "    splits_test = splits[splits[\"split\"] == test_split]\n",
    "    \n",
    "    stems_test = stems_test.set_index(\"song_id\")\n",
    "    metadata_test = metadata_test.drop_duplicates(\"song_id\").set_index(\"song_id\")\n",
    "    splits_test = splits_test.set_index(\"song_id\")\n",
    "    \n",
    "    stem_to_song_id = defaultdict(list)\n",
    "    song_id_to_stem = defaultdict(list)\n",
    "    \n",
    "    for song_id in file_in_test:\n",
    "        \n",
    "        stems_ = stems_test.loc[song_id]\n",
    "        stem_names = stems_.T\n",
    "        stem_names = stem_names[stem_names == 1].index.tolist()\n",
    "        \n",
    "        for stem in stem_names:\n",
    "            stem_to_song_id[stem].append(song_id)\n",
    "            \n",
    "        song_id_to_stem[song_id] = stem_names\n",
    "        \n",
    "        \n",
    "    indices = []\n",
    "    no_query = []\n",
    "    \n",
    "    for song_id in file_in_test:\n",
    "        \n",
    "        genre = metadata_test.loc[song_id, \"genre\"]\n",
    "        # print(genre)\n",
    "        artist = metadata_test.loc[song_id, \"artist\"]\n",
    "        # print(artist)\n",
    "        \n",
    "        stems_ = song_id_to_stem[song_id]\n",
    "        \n",
    "        for stem in stems_:\n",
    "            possible_query = stem_to_song_id[stem]\n",
    "            possible_query = [p for p in possible_query if p != song_id]\n",
    "            \n",
    "            if len(possible_query) == 0:\n",
    "                print(f\"No possible query for {song_id} with {stem}\")\n",
    "                \n",
    "                no_query.append(\n",
    "                    {\n",
    "                        \"song_id\": song_id,\n",
    "                        \"stem\": stem\n",
    "                    }\n",
    "                )\n",
    "                continue\n",
    "            \n",
    "            query_df = metadata_test.loc[possible_query, [\"genre\", \"artist\"]]\n",
    "            \n",
    "            assert len(query_df) > 0\n",
    "            \n",
    "            query_df_ = query_df.copy()\n",
    "            \n",
    "            same_genre = True\n",
    "            different_artist = True\n",
    "            query_df = query_df[(query_df[\"genre\"] == genre) & (query_df[\"artist\"] != artist)]\n",
    "            \n",
    "            if len(query_df) == 0:\n",
    "                \n",
    "                same_genre = False\n",
    "                different_artist = True\n",
    "                \n",
    "                query_df = query_df_.copy()\n",
    "                query_df = query_df[(query_df[\"artist\"] != artist)]\n",
    "            \n",
    "            if len(query_df) == 0:\n",
    "                \n",
    "                same_genre = True\n",
    "                different_artist = False\n",
    "                \n",
    "                query_df = query_df_.copy()\n",
    "                query_df = query_df[(query_df[\"genre\"] == genre)]\n",
    "            \n",
    "            if len(query_df) == 0:\n",
    "                \n",
    "                same_genre = False\n",
    "                different_artist = False\n",
    "                \n",
    "                query_df = query_df_.copy()\n",
    "            \n",
    "            query_id = query_df.sample(1).index[0]\n",
    "            \n",
    "            indices.append(\n",
    "                {\n",
    "                    \"song_id\": song_id,\n",
    "                    \"query_id\": query_id,\n",
    "                    \"stem\": stem,\n",
    "                    \"same_genre\": same_genre,\n",
    "                    \"different_artist\": different_artist\n",
    "                }   \n",
    "            )\n",
    "            \n",
    "    indices = pd.DataFrame.from_records(indices)\n",
    "    no_query = pd.DataFrame.from_records(no_query)\n",
    "    \n",
    "    indices.to_csv(\n",
    "        os.path.join(os.path.dirname(metadata_path), \"test_indices.csv\"), index=False\n",
    "    )\n",
    "    \n",
    "    no_query.to_csv(\n",
    "        os.path.join(os.path.dirname(metadata_path), \"no_query.csv\"), index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Total number of queries:\", len(indices))\n",
    "    print(\"Total number of no queries:\", len(no_query))\n",
    "    \n",
    "    query_type = indices.groupby([\"same_genre\", \"different_artist\"]).size()\n",
    "    \n",
    "    print(query_type)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import fire\n",
    "\n",
    "    fire.Fire()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Audio/Dataset & Passt EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# ***Loss Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "from typing import Dict, List, Optional, Union\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "# from core.types import BatchedInputOutput\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BaseLossHandler(nn.Module):\n",
    "    def __init__(\n",
    "        self, loss: nn.Module, modality: Union[str, List[str]], name: Optional[str] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = loss\n",
    "\n",
    "        if isinstance(modality, str):\n",
    "            modality = [modality]\n",
    "\n",
    "        self.modality = modality\n",
    "\n",
    "        if name is None:\n",
    "            name = \"loss\"\n",
    "\n",
    "        if name == \"__auto__\":\n",
    "            name = self.loss.__class__.__name__\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "    def _audio_preprocess(self, y_pred, y_true):\n",
    "\n",
    "        n_sample_true = y_true.shape[-1]\n",
    "        n_sample_pred = y_pred.shape[-1]\n",
    "\n",
    "        if n_sample_pred > n_sample_true:\n",
    "            y_pred = y_pred[..., :n_sample_true]\n",
    "        elif n_sample_pred < n_sample_true:\n",
    "            y_true = y_true[..., :n_sample_pred]\n",
    "\n",
    "        return y_pred, y_true\n",
    "\n",
    "    def forward(self, batch: BatchedInputOutput):\n",
    "        y_true = batch.sources\n",
    "        y_pred = batch.estimates\n",
    "\n",
    "        loss_contribs = {}\n",
    "\n",
    "        stem_contribs = {\n",
    "            stem: 0.0 for stem in y_pred.keys()\n",
    "        }\n",
    "\n",
    "        for stem in y_pred.keys():\n",
    "            for modality in self.modality:\n",
    "\n",
    "                if modality not in y_pred[stem].keys():\n",
    "                    continue\n",
    "\n",
    "                if y_pred[stem][modality].shape[-1] == 0:\n",
    "                    continue\n",
    "\n",
    "                y_true_ = y_true[stem][modality]\n",
    "                y_pred_ = y_pred[stem][modality]\n",
    "\n",
    "                if modality == \"audio\":\n",
    "                    y_pred_, y_true_ = self._audio_preprocess(y_pred_, y_true_)\n",
    "                elif modality == \"spectrogram\":\n",
    "                    y_pred_ = torch.view_as_real(y_pred_)\n",
    "                    y_true_ = torch.view_as_real(y_true_)\n",
    "\n",
    "                loss_contribs[f\"{self.name}/{stem}/{modality}\"] = self.loss(\n",
    "                    y_pred_, y_true_\n",
    "                )\n",
    "\n",
    "                stem_contribs[stem] += loss_contribs[f\"{self.name}/{stem}/{modality}\"]\n",
    "\n",
    "        total_loss = sum(stem_contribs.values())\n",
    "        loss_contribs[self.name] = total_loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for stem in stem_contribs.keys():\n",
    "                loss_contribs[f\"{self.name}/{stem}\"] = stem_contribs[stem]\n",
    "\n",
    "        return loss_contribs\n",
    "\n",
    "\n",
    "class AdversarialLossHandler(BaseLossHandler):\n",
    "    def __init__(self, loss: nn.Module, modality: str, name: Optional[str] = \"adv_loss\"):\n",
    "\n",
    "        super().__init__(loss, modality, name)\n",
    "\n",
    "    def discriminator_forward(self, batch: BatchedInputOutput):\n",
    "\n",
    "        y_true = batch.sources\n",
    "        y_pred = batch.estimates\n",
    "\n",
    "        # g_loss_contribs = {}\n",
    "        d_loss_contribs = {}\n",
    "\n",
    "        for stem in y_pred.keys():\n",
    "\n",
    "            if self.modality not in y_pred[stem].keys():\n",
    "                continue\n",
    "\n",
    "            if y_pred[stem][self.modality].shape[-1] == 0:\n",
    "                continue\n",
    "\n",
    "            y_true_ = y_true[stem][self.modality]\n",
    "            y_pred_ = y_pred[stem][self.modality]\n",
    "\n",
    "            if self.modality == \"audio\":\n",
    "                y_pred_, y_true_ = self._audio_preprocess(y_pred_, y_true_)\n",
    "\n",
    "            # g_loss_contribs[f\"{self.name}:g/{stem}\"] = self.loss.generator_loss(\n",
    "            #     y_pred_, y_true_\n",
    "            # )\n",
    "\n",
    "            d_loss_contribs[f\"{self.name}:d/{stem}\"] = self.loss.discriminator_loss(\n",
    "                y_pred_, y_true_\n",
    "            )\n",
    "\n",
    "        # g_total_loss = sum(g_loss_contribs.values())\n",
    "        d_total_loss = sum(d_loss_contribs.values())\n",
    "\n",
    "        # g_loss_contribs[\"loss\"] = g_total_loss\n",
    "        d_loss_contribs[\"disc_loss\"] = d_total_loss\n",
    "\n",
    "        return d_loss_contribs\n",
    "\n",
    "    def generator_forward(self, batch: BatchedInputOutput):\n",
    "\n",
    "        y_true = batch.sources\n",
    "        y_pred = batch.estimates\n",
    "\n",
    "        g_loss_contribs = {}\n",
    "        # d_loss_contribs = {}\n",
    "\n",
    "        for stem in y_pred.keys():\n",
    "\n",
    "            if self.modality not in y_pred[stem].keys():\n",
    "                continue\n",
    "\n",
    "            if y_pred[stem][self.modality].shape[-1] == 0:\n",
    "                continue\n",
    "\n",
    "            y_true_ = y_true[stem][self.modality]\n",
    "            y_pred_ = y_pred[stem][self.modality]\n",
    "\n",
    "            if self.modality == \"audio\":\n",
    "                y_pred_, y_true_ = self._audio_preprocess(y_pred_, y_true_)\n",
    "\n",
    "            g_loss_contribs[f\"{self.name}:g/{stem}\"] = self.loss.generator_loss(\n",
    "                y_pred_, y_true_\n",
    "            )\n",
    "\n",
    "            # d_loss_contribs[f\"{self.name}:g/{stem}\"] = self.loss.discriminator_loss(\n",
    "            #     y_pred_, y_true_\n",
    "            # )\n",
    "\n",
    "        g_total_loss = sum(g_loss_contribs.values())\n",
    "        # d_total_loss = sum(d_loss_contribs.values())\n",
    "\n",
    "        g_loss_contribs[\"gen_loss\"] = g_total_loss\n",
    "        # d_loss_contribs[\"loss\"] = d_total_loss\n",
    "\n",
    "        return g_loss_contribs\n",
    "\n",
    "    def forward(self, batch: BatchedInputOutput):\n",
    "        return {\n",
    "            \"generator\": self.generator_forward(batch),\n",
    "            \"discriminator\": self.discriminator_forward(batch)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1SNR Loss\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WeightedL1Loss(_Loss):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        ndim = y_pred.ndim\n",
    "        dims = list(range(1, ndim))\n",
    "        loss = F.l1_loss(y_pred, y_true, reduction='none')\n",
    "        loss = torch.mean(loss, dim=dims)\n",
    "        weights = torch.mean(torch.abs(y_true), dim=dims)\n",
    "\n",
    "        loss = torch.sum(loss * weights) / torch.sum(weights)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class L1MatchLoss(_Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size = y_pred.shape[0]\n",
    "\n",
    "        y_pred = y_pred.reshape(batch_size, -1)\n",
    "        y_true = y_true.reshape(batch_size, -1)\n",
    "\n",
    "        l1_true = torch.mean(torch.abs(y_true), dim=-1)\n",
    "        l1_pred = torch.mean(torch.abs(y_pred), dim=-1)\n",
    "        loss = torch.mean(torch.abs(l1_pred - l1_true))\n",
    "\n",
    "        return loss\n",
    "\n",
    "class DecibelMatchLoss(_Loss):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size = y_pred.shape[0]\n",
    "\n",
    "        y_pred = y_pred.reshape(batch_size, -1)\n",
    "        y_true = y_true.reshape(batch_size, -1)\n",
    "\n",
    "        db_true = 10.0 * torch.log10(self.eps + torch.mean(torch.square(torch.abs(y_true)), dim=-1))\n",
    "        db_pred = 10.0 * torch.log10(self.eps + torch.mean(torch.square(torch.abs(y_pred)), dim=-1))\n",
    "        loss = torch.mean(torch.abs(db_pred - db_true))\n",
    "\n",
    "        return loss\n",
    "\n",
    "class L1SNRLoss(_Loss):\n",
    "    def __init__(self, eps=1e-3):\n",
    "        super().__init__()\n",
    "        self.eps = torch.tensor(eps)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size = y_pred.shape[0]\n",
    "\n",
    "        y_pred = y_pred.reshape(batch_size, -1)\n",
    "        y_true = y_true.reshape(batch_size, -1)\n",
    "\n",
    "        l1_error = torch.mean(torch.abs(y_pred - y_true), dim=-1)\n",
    "        l1_true = torch.mean(torch.abs(y_true), dim=-1)\n",
    "\n",
    "        snr = 20.0 * torch.log10((l1_true + self.eps) / (l1_error + self.eps))\n",
    "\n",
    "        return -torch.mean(snr)\n",
    "    \n",
    "class L1SNRLossIgnoreSilence(_Loss):\n",
    "    def __init__(self, eps=1e-3, dbthresh=-20, dbthresh_step=20):\n",
    "        super().__init__()\n",
    "        self.eps = torch.tensor(eps)\n",
    "        self.dbthresh = dbthresh\n",
    "        self.dbthresh_step = dbthresh_step\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        batch_size = y_pred.shape[0]\n",
    "\n",
    "        y_pred = y_pred.reshape(batch_size, -1)\n",
    "        y_true = y_true.reshape(batch_size, -1)\n",
    "\n",
    "        l1_error = torch.mean(torch.abs(y_pred - y_true), dim=-1)\n",
    "        l1_true = torch.mean(torch.abs(y_true), dim=-1)\n",
    "\n",
    "        snr = 20.0 * torch.log10((l1_true + self.eps) / (l1_error + self.eps))\n",
    "        \n",
    "        db = 10.0 * torch.log10(torch.mean(torch.square(y_true), dim=-1) + 1e-6)\n",
    "        \n",
    "        if torch.sum(db > self.dbthresh) == 0:\n",
    "            if torch.sum(db > self.dbthresh - self.dbthresh_step) == 0:\n",
    "                return -torch.mean(snr)\n",
    "            else:\n",
    "                return -torch.mean(snr[db > self.dbthresh  - self.dbthresh_step])\n",
    "\n",
    "        return -torch.mean(snr[db > self.dbthresh])\n",
    "\n",
    "class L1SNRDecibelMatchLoss(_Loss):\n",
    "    def __init__(self, db_weight=0.1, l1snr_eps=1e-3, dbeps=1e-3):\n",
    "        super().__init__()\n",
    "        self.l1snr = L1SNRLoss(l1snr_eps)\n",
    "        self.decibel_match = DecibelMatchLoss(dbeps)\n",
    "        self.db_weight = db_weight\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        return self.l1snr(y_pred, y_true) + self.decibel_match(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# ***Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "from typing import Dict, Optional\n",
    "from torch import nn\n",
    "import torchmetrics as tm\n",
    "\n",
    "# from core.types import BatchedInputOutput, OperationMode\n",
    "\n",
    "\n",
    "class BaseMetricHandler(nn.Module):\n",
    "    def __init__(\n",
    "        self, stem: str, metric: tm.Metric, modality: str, name: Optional[str] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.metric = metric\n",
    "        self.modality = modality\n",
    "        self.stem = stem\n",
    "\n",
    "        if name is None or name == \"__auto__\":\n",
    "            name = self.metric.__class__.__name__\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "    def update(self, batch: BatchedInputOutput):\n",
    "        y_true = batch.sources[self.stem]\n",
    "        y_pred = batch.estimates[self.stem]\n",
    "\n",
    "        self.metric.update(y_pred[self.modality].cuda(), y_true[self.modality].cuda())\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "\n",
    "        metric = self.metric.compute()\n",
    "\n",
    "        if isinstance(metric, dict):\n",
    "            return {f\"{self.name}/{k}\": v for k, v in metric.items()}\n",
    "\n",
    "        return {self.name: self.metric.compute()}\n",
    "\n",
    "    def reset(self):\n",
    "        self.metric.reset()\n",
    "\n",
    "\n",
    "class MultiModeMetricHandler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_metrics: Dict[str, BaseMetricHandler],\n",
    "        val_metrics: Dict[str, BaseMetricHandler],\n",
    "        test_metrics: Dict[str, BaseMetricHandler],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_metrics = nn.ModuleDict(train_metrics)\n",
    "        self.val_metrics = nn.ModuleDict(val_metrics)\n",
    "        self.test_metrics = nn.ModuleDict(test_metrics)\n",
    "\n",
    "    def get_mode(self, mode: OperationMode) -> BaseMetricHandler:\n",
    "        if mode == OperationMode.TRAIN:\n",
    "            return self.train_metrics\n",
    "        elif mode == OperationMode.VAL:\n",
    "            return self.val_metrics\n",
    "        elif mode == OperationMode.TEST:\n",
    "            return self.test_metrics\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNR\n",
    "from typing import Any, Tuple\n",
    "import torch\n",
    "import torchmetrics as tm\n",
    "from torchmetrics.audio.snr import SignalNoiseRatio\n",
    "from torchmetrics.functional.audio.snr import signal_noise_ratio, scale_invariant_signal_noise_ratio\n",
    "from torchmetrics.utilities.checks import _check_same_shape\n",
    "\n",
    "\n",
    "def safe_signal_noise_ratio(\n",
    "    preds: torch.Tensor, target: torch.Tensor, zero_mean: bool = False\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    return torch.nan_to_num(\n",
    "        signal_noise_ratio(preds, target, zero_mean=zero_mean), nan=torch.nan, posinf=100.0, neginf=-100.0\n",
    "    )\n",
    "\n",
    "\n",
    "def safe_scale_invariant_signal_noise_ratio(\n",
    "    preds: torch.Tensor, target: torch.Tensor,\n",
    "    zero_mean: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"`Scale-invariant signal-to-distortion ratio`_ (SI-SDR).\n",
    "\n",
    "    The SI-SDR value is in general considered an overall measure of how good a source sound.\n",
    "\n",
    "    Args:\n",
    "        preds: float tensor with shape ``(...,time)``\n",
    "        target: float tensor with shape ``(...,time)``\n",
    "        zero_mean: If to zero mean target and preds or not\n",
    "\n",
    "    Returns:\n",
    "        Float tensor with shape ``(...,)`` of SDR values per sample\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError:\n",
    "            If ``preds`` and ``target`` does not have the same shape\n",
    "    \"\"\"\n",
    "    return torch.nan_to_num(\n",
    "        scale_invariant_signal_noise_ratio(preds, target), nan=torch.nan, posinf=100.0,\n",
    "        neginf=-100.0\n",
    "    )\n",
    "\n",
    "def decibels(x: torch.Tensor, threshold: float = 1e-6) -> torch.Tensor:\n",
    "    mean_squared = torch.mean(torch.square(x), dim=-1)\n",
    "    n_samples = x.shape[0]\n",
    "    return torch.sum(10 * torch.log10(mean_squared + threshold)), n_samples\n",
    "\n",
    "class Decibels(tm.Metric):\n",
    "    def __init__(self, threshold: float = 1e-6, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.add_state(\"running_mean\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"running_count\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, y):\n",
    "\n",
    "        db, count = decibels(y, self.threshold)\n",
    "        self.running_mean += db.cpu()\n",
    "        self.running_count += count\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.running_mean / self.running_count\n",
    "\n",
    "    # def reset(self) -> None:\n",
    "    #     self.running_mean = torch.tensor(0.0)\n",
    "    #     self.running_count = torch.tensor(0)\n",
    "\n",
    "class PredictedDecibels(Decibels):\n",
    "    def update(self, ypred, ytrue) -> None:\n",
    "        return super().update(ypred)\n",
    "\n",
    "class TargetDecibels(Decibels):\n",
    "    def update(self, ypred, ytrue) -> None:\n",
    "        return super().update(ytrue)\n",
    "\n",
    "\n",
    "class SafeSignalNoiseRatio(SignalNoiseRatio):\n",
    "    def __init__(\n",
    "        self,\n",
    "        zero_mean: bool = False,\n",
    "        threshold: float = 1e-6,\n",
    "        fs: int = 44100,\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        super().__init__(zero_mean, **kwargs)\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.fs = fs\n",
    "\n",
    "        self.sample_mismatch_thresh_seconds = 0.1\n",
    "\n",
    "        self.add_state(\"snr_list\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def _fix_shape(self, preds: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        n_samples_preds = preds.shape[-1]\n",
    "        n_samples_target = target.shape[-1]\n",
    "\n",
    "        if n_samples_preds != n_samples_target:\n",
    "            if (\n",
    "                    abs(n_samples_preds - n_samples_target) / self.fs\n",
    "                    > self.sample_mismatch_thresh_seconds\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"The difference between the number of samples of the predictions and the target is too large (100 ms)\"\n",
    "                )\n",
    "\n",
    "            min_samples = min(n_samples_preds, n_samples_target)\n",
    "            preds = preds[..., :min_samples]\n",
    "            target = target[..., :min_samples]\n",
    "\n",
    "        return preds, target\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n",
    "        \"\"\"Update state with predictions and targets.\"\"\"\n",
    "\n",
    "        preds, target = self._fix_shape(preds, target)\n",
    "\n",
    "        snr_batch = safe_signal_noise_ratio(\n",
    "            preds=preds, target=target, zero_mean=self.zero_mean\n",
    "        )\n",
    "\n",
    "        self.snr_list.append(snr_batch)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute metric.\"\"\"\n",
    "\n",
    "        if len(self.snr_list) == 0:\n",
    "            return torch.tensor(float(\"nan\"))\n",
    "\n",
    "        return torch.nanmedian(torch.cat(self.snr_list))\n",
    "\n",
    "\n",
    "class SafeScaleInvariantSignalNoiseRatio(SafeSignalNoiseRatio):\n",
    "    def __init__(\n",
    "        self,\n",
    "        zero_mean: bool = False,\n",
    "        threshold: float = 1e-6,\n",
    "        fs: int = 44100,\n",
    "        **kwargs: Any\n",
    "    ) -> None:\n",
    "        super().__init__(zero_mean, threshold, fs, **kwargs)\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor) -> None:\n",
    "        \"\"\"Update state with predictions and targets.\"\"\"\n",
    "\n",
    "        preds, target = self._fix_shape(preds, target)\n",
    "\n",
    "        snr_batch = safe_scale_invariant_signal_noise_ratio(\n",
    "            preds=preds, target=target, zero_mean=self.zero_mean\n",
    "        )\n",
    "\n",
    "        self.snr_list.append(snr_batch)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute metric.\"\"\"\n",
    "\n",
    "        if len(self.snr_list) == 0:\n",
    "            return torch.tensor(float(\"nan\"))\n",
    "\n",
    "        return torch.nanmedian(torch.cat(self.snr_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "# ***Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot find key: --f=c:\\Users\\Dell\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3f0d43beaf9ae7787c0ec59baf0b0092a7079c985.json\n",
      "Usage: ipykernel_launcher.py <group|command|value>\n",
      "  available groups:      In | Out | exit | quit | os | Callable | np | torch |\n",
      "                         taF | pd | band_defs | mbs | df | nn | math |\n",
      "                         activation_ | warnings | F | rnn | model |\n",
      "                         input_data | Dict | List | Optional | Tuple | Type |\n",
      "                         activation | ta | optim | tm | Union | pl | Iterator |\n",
      "                         Mapping | lr_scheduler | inspect | Sequence | data |\n",
      "                         taxonomy | random | INST_BY_OCCURRENCE |\n",
      "                         FINE_LEVEL_INSTRUMENTS | COARSE_LEVEL_INSTRUMENTS |\n",
      "                         COARSE_TO_FINE | FINE_TO_COARSE |\n",
      "                         ALL_LEVEL_INSTRUMENTS | audiomentations | glob |\n",
      "                         json | shutil | librosa | fine_to_coarse | v |\n",
      "                         possible_coarse | possible_fine | fire | string |\n",
      "                         pytorch_lightning | ALLOWED_MODELS |\n",
      "                         ALLOWED_MODELS_DICT | ALLOWED_DATAMODULES |\n",
      "                         ALLOWED_DATAMODULE_DICT | ALLOWED_LOSSES |\n",
      "                         ALLOWED_LOSS_DICT\n",
      "  available commands:    get_ipython | open | abstractmethod | Any |\n",
      "                         hz_to_midi | midi_to_hz | Tensor |\n",
      "                         band_widths_from_specs | check_nonzero_bandwidth |\n",
      "                         check_no_overlap | check_no_gap |\n",
      "                         BandsplitSpecification | VocalBandsplitSpecification |\n",
      "                         OtherBandsplitSpecification |\n",
      "                         BassBandsplitSpecification |\n",
      "                         DrumBandsplitSpecification |\n",
      "                         PerceptualBandsplitSpecification | mel_filterbank |\n",
      "                         MelBandsplitSpecification | musical_filterbank |\n",
      "                         MusicalBandsplitSpecification | bands | Conditioning |\n",
      "                         PassThroughConditioning | FiLM | CosineSimiliarity |\n",
      "                         GeneralizedBilinear | BilinearFiLM |\n",
      "                         TimeFrequencyModellingModule | ResidualRNN |\n",
      "                         SeqBandModellingModule | BaseNormMLP | NormMLP |\n",
      "                         MaskEstimationModuleSuperBase |\n",
      "                         MaskEstimationModuleBase |\n",
      "                         OverlappingMaskEstimationModule |\n",
      "                         MaskEstimationModule | NormFC | BandSplitModule |\n",
      "                         get_basic_model | Passt | PasstWrapper |\n",
      "                         SimpleNamespace | TypedDict | OperationMode |\n",
      "                         RawInputType | nested_dict_to_nested_namespace |\n",
      "                         input_dict | SimpleishNamespace | BatchedInputOutput |\n",
      "                         TensorCollection | InputType | OutputType |\n",
      "                         LossOutputType | MetricOutputType | ModelType |\n",
      "                         OptimizerType | SchedulerType | MetricType |\n",
      "                         LossType | OptimizationBundle | LossHandler |\n",
      "                         MetricHandler | AugmentationHandler |\n",
      "                         InferenceHandler | BaseEndToEndModule | BaseBandit |\n",
      "                         Bandit | BaseConditionedBandit |\n",
      "                         PasstFiLMConditionedBandit | defaultdict | chain |\n",
      "                         combinations | pprint | LRScheduler | tqdm |\n",
      "                         EndToEndLightningSystem | ABC | EVAL_DATALOADERS |\n",
      "                         TRAIN_DATALOADERS | LightningDataModule | Dataset |\n",
      "                         DataLoader | IterableDataset | from_datasets |\n",
      "                         BaseSourceSeparationDataset | clean_track_inst |\n",
      "                         OmegaConf | ObjectDict | MoisesDBBaseDataset |\n",
      "                         MoisesDBFullTrackDataset |\n",
      "                         MoisesDBVDBOFullTrackDataset |\n",
      "                         convert_decibels_to_amplitude_ratio | SmartGain |\n",
      "                         Audiomentations | MoisesDBVDBORandomChunkDataset |\n",
      "                         MoisesDBVDBODeterministicChunkDataset |\n",
      "                         round_samples |\n",
      "                         MoisesDBRandomChunkRandomQueryDataset |\n",
      "                         MoisesDBRandomChunkBalancedRandomQueryDataset |\n",
      "                         MoisesDBDeterministicChunkDeterministicQueryDataset |\n",
      "                         MoisesDBFullTrackTestQueryDataset | MoisesDataModule |\n",
      "                         MoisesBalancedTrainDataModule |\n",
      "                         MoisesValidationDataModule | MoisesTestDataModule |\n",
      "                         MoisesVDBODataModule | process_map | tdqm |\n",
      "                         clean_npy_other_vox | save_taxonomy | trim_and_mix |\n",
      "                         retrim_npys | convert_one | convert_to_npy |\n",
      "                         make_others_one | check_vdbo_one | check_vdbo |\n",
      "                         make_others | extract_metadata_one |\n",
      "                         consolidate_metadata | clean_canonical | remove_dbfs |\n",
      "                         make_split | consolidate_stems | get_dbfs |\n",
      "                         get_dbfs_by_chunk_one | clean_data_root |\n",
      "                         get_dbfs_by_chunk | get_query_one |\n",
      "                         get_query_from_onset | get_durations |\n",
      "                         clean_query_root | make_test_indices |\n",
      "                         BaseLossHandler | AdversarialLossHandler |\n",
      "                         WeightedL1Loss | L1MatchLoss | DecibelMatchLoss |\n",
      "                         L1SNRLoss | L1SNRLossIgnoreSilence |\n",
      "                         L1SNRDecibelMatchLoss | BaseMetricHandler |\n",
      "                         MultiModeMetricHandler | SignalNoiseRatio |\n",
      "                         signal_noise_ratio |\n",
      "                         scale_invariant_signal_noise_ratio |\n",
      "                         safe_signal_noise_ratio |\n",
      "                         safe_scale_invariant_signal_noise_ratio | decibels |\n",
      "                         Decibels | PredictedDecibels | TargetDecibels |\n",
      "                         SafeSignalNoiseRatio |\n",
      "                         SafeScaleInvariantSignalNoiseRatio |\n",
      "                         AdvancedProfiler | train | query_validate |\n",
      "                         query_test | query_inference | clean_validation_metrics\n",
      "  available values:      band_name | i | f_min | f_max | device |\n",
      "                         DBFS_HOP_SIZE | DBFS_CHUNK_SIZE | config | k | vv\n",
      "\n",
      "For detailed information on this command, run:\n",
      "  ipykernel_launcher.py --help\n"
     ]
    },
    {
     "ename": "FireExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mFireExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\demix\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "from pprint import pprint\n",
    "import random\n",
    "import string\n",
    "from types import SimpleNamespace\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from core.data.moisesdb.datamodule import (\n",
    "#     MoisesTestDataModule,\n",
    "#     MoisesValidationDataModule,\n",
    "#     MoisesDataModule,\n",
    "#     MoisesBalancedTrainDataModule,\n",
    "#     MoisesVDBODataModule,\n",
    "# )\n",
    "# from core.losses.base import AdversarialLossHandler, BaseLossHandler\n",
    "# from core.losses.l1snr import (\n",
    "#     L1SNRDecibelMatchLoss,\n",
    "#     L1SNRLoss,\n",
    "#     WeightedL1Loss,\n",
    "#     L1SNRLossIgnoreSilence,\n",
    "# )\n",
    "# from core.metrics.base import BaseMetricHandler, MultiModeMetricHandler\n",
    "# from core.metrics.snr import (\n",
    "#     SafeScaleInvariantSignalNoiseRatio,\n",
    "#     SafeSignalNoiseRatio,\n",
    "#     PredictedDecibels,\n",
    "#     TargetDecibels,\n",
    "# )\n",
    "# from core.models.ebase import EndToEndLightningSystem\n",
    "# from core.models.e2e.resunet.resunet import (\n",
    "#     ResUNetPasstConditionedSeparator,\n",
    "#     ResUNetResQueryConditionedSeparator,\n",
    "#     # StupidNet\n",
    "# )\n",
    "\n",
    "# from core.models.e2e.bandit.bandit import Bandit, PasstFiLMConditionedBandit\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "# from core.types import LossHandler, OptimizationBundle\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchmetrics as tm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks\n",
    "import pytorch_lightning.loggers\n",
    "from pytorch_lightning.profilers import AdvancedProfiler\n",
    "\n",
    "import torch.backends.cudnn\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def _allowed_classes_to_dict(allowed_classes):\n",
    "    return {cls.__name__: cls for cls in allowed_classes}\n",
    "\n",
    "\n",
    "ALLOWED_MODELS = [\n",
    "    # ResUNetPasstConditionedSeparator,\n",
    "    # ResUNetResQueryConditionedSeparator,\n",
    "    # StupidNet\n",
    "    Bandit,\n",
    "    PasstFiLMConditionedBandit,\n",
    "]\n",
    "\n",
    "ALLOWED_MODELS_DICT = _allowed_classes_to_dict(ALLOWED_MODELS)\n",
    "\n",
    "ALLOWED_DATAMODULES = [\n",
    "    MoisesDataModule,\n",
    "    MoisesBalancedTrainDataModule,\n",
    "    MoisesVDBODataModule,\n",
    "    MoisesValidationDataModule,\n",
    "    MoisesTestDataModule,\n",
    "]\n",
    "\n",
    "ALLOWED_DATAMODULE_DICT = _allowed_classes_to_dict(ALLOWED_DATAMODULES)\n",
    "\n",
    "ALLOWED_LOSSES = [\n",
    "    L1SNRLoss,\n",
    "    WeightedL1Loss,\n",
    "    L1SNRDecibelMatchLoss,\n",
    "    L1SNRLossIgnoreSilence,\n",
    "]\n",
    "\n",
    "ALLOWED_LOSS_DICT = _allowed_classes_to_dict(ALLOWED_LOSSES)\n",
    "\n",
    "\n",
    "def _build_model(config: OmegaConf) -> nn.Module:\n",
    "\n",
    "    model_config = config.model\n",
    "\n",
    "    model_name = model_config.cls\n",
    "    kwargs = model_config.get(\"kwargs\", {})\n",
    "\n",
    "    if model_name in ALLOWED_MODELS_DICT:\n",
    "        model = ALLOWED_MODELS_DICT[model_name](**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _build_inner_loss(config: OmegaConf) -> nn.Module:\n",
    "    loss_config = config.loss\n",
    "\n",
    "    loss_name = loss_config.cls\n",
    "    kwargs = loss_config.get(\"kwargs\", {})\n",
    "\n",
    "    if loss_name in ALLOWED_LOSS_DICT:\n",
    "        loss = ALLOWED_LOSS_DICT[loss_name](**kwargs)\n",
    "    elif loss_name in nn.modules.loss.__dict__:\n",
    "        loss = nn.modules.loss.__dict__[loss_name](**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss name: {loss_name}\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _build_loss(config: OmegaConf) -> BaseLossHandler:\n",
    "    loss_handler = BaseLossHandler(\n",
    "        loss=_build_inner_loss(config),\n",
    "        modality=config.loss.modality,\n",
    "        name=config.loss.get(\"name\", None),\n",
    "    )\n",
    "\n",
    "    return loss_handler\n",
    "\n",
    "\n",
    "def _dummy_metrics(config: OmegaConf) -> MultiModeMetricHandler:\n",
    "    metrics = MultiModeMetricHandler(\n",
    "        train_metrics={\n",
    "            stem: BaseMetricHandler(\n",
    "                stem=stem,\n",
    "                metric=tm.MetricCollection(\n",
    "                    SafeSignalNoiseRatio(),\n",
    "                    SafeScaleInvariantSignalNoiseRatio(),\n",
    "                    PredictedDecibels(),\n",
    "                    TargetDecibels(),\n",
    "                ),\n",
    "                modality=\"audio\",\n",
    "                name=\"snr\",\n",
    "            )\n",
    "            for stem in config.stems\n",
    "        },\n",
    "        val_metrics={\n",
    "            stem: BaseMetricHandler(\n",
    "                stem=stem,\n",
    "                metric=tm.MetricCollection(\n",
    "                    SafeSignalNoiseRatio(),\n",
    "                    SafeScaleInvariantSignalNoiseRatio(),\n",
    "                    PredictedDecibels(),\n",
    "                    TargetDecibels(),\n",
    "                ),\n",
    "                modality=\"audio\",\n",
    "                name=\"snr\",\n",
    "            )\n",
    "            for stem in config.stems\n",
    "        },\n",
    "        test_metrics={\n",
    "            stem: BaseMetricHandler(\n",
    "                stem=stem,\n",
    "                metric=tm.MetricCollection(\n",
    "                    SafeSignalNoiseRatio(),\n",
    "                    SafeScaleInvariantSignalNoiseRatio(),\n",
    "                    PredictedDecibels(),\n",
    "                    TargetDecibels(),\n",
    "                ),\n",
    "                modality=\"audio\",\n",
    "                name=\"snr\",\n",
    "            )\n",
    "            for stem in config.stems\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _build_optimization_bundle(config: OmegaConf) -> OptimizationBundle:\n",
    "    optim_config = config.optim\n",
    "\n",
    "    print(optim_config)\n",
    "\n",
    "    optimizer_name = optim_config.optimizer.cls\n",
    "    kwargs = optim_config.optimizer.get(\"kwargs\", {})\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)\n",
    "\n",
    "    optim_bundle = SimpleNamespace(\n",
    "        optimizer=SimpleNamespace(cls=optimizer, kwargs=kwargs), scheduler=None\n",
    "    )\n",
    "\n",
    "    scheduler_config = optim_config.get(\"scheduler\", None)\n",
    "\n",
    "    if scheduler_config is not None:\n",
    "        scheduler_name = scheduler_config.cls\n",
    "        scheduler_kwargs = scheduler_config.get(\"kwargs\", {})\n",
    "\n",
    "        if scheduler_name in lr_scheduler.__dict__:\n",
    "            scheduler = lr_scheduler.__dict__[scheduler_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler name: {scheduler_name}\")\n",
    "\n",
    "        optim_bundle.scheduler = SimpleNamespace(cls=scheduler, kwargs=scheduler_kwargs)\n",
    "\n",
    "    return optim_bundle\n",
    "\n",
    "\n",
    "def _dummy_augmentation():\n",
    "    return nn.Identity()\n",
    "\n",
    "\n",
    "def _load_config(config_path: str) -> OmegaConf:\n",
    "    config = OmegaConf.load(config_path)\n",
    "\n",
    "    config_dict = {}\n",
    "\n",
    "    for k, v in config.items():\n",
    "        if isinstance(v, str) and v.endswith(\".yml\"):\n",
    "            config_dict[k] = OmegaConf.load(v)\n",
    "        else:\n",
    "            config_dict[k] = v\n",
    "\n",
    "    config = OmegaConf.merge(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def _build_datamodule(config: OmegaConf) -> pl.LightningDataModule:\n",
    "\n",
    "    DataModule = ALLOWED_DATAMODULE_DICT[config.data.cls]\n",
    "\n",
    "    datamodule = DataModule(\n",
    "        data_root=config.data.data_root,\n",
    "        batch_size=config.data.batch_size,\n",
    "        num_workers=config.data.num_workers,\n",
    "        train_kwargs=config.data.get(\"train_kwargs\", None),\n",
    "        val_kwargs=config.data.get(\"val_kwargs\", None),\n",
    "        test_kwargs=config.data.get(\"test_kwargs\", None),\n",
    "        datamodule_kwargs=config.data.get(\"datamodule_kwargs\", None),\n",
    "    )\n",
    "\n",
    "    return datamodule\n",
    "\n",
    "\n",
    "def train(\n",
    "    config_path: str,\n",
    "    profile: bool = False,\n",
    "    ckpt_path: str = None,\n",
    "    validate_only: bool = False,\n",
    "    inference_only: bool = False,\n",
    "    output_dir: str = None,\n",
    "    test_datamodule: bool = False,\n",
    "    precision=32,\n",
    "):\n",
    "    config = _load_config(config_path)\n",
    "\n",
    "    pl.seed_everything(config.seed, workers=True)\n",
    "\n",
    "    if inference_only:\n",
    "        config[\"data\"][\"batch_size\"] = 1\n",
    "\n",
    "    datamodule = _build_datamodule(config)\n",
    "\n",
    "    if test_datamodule:\n",
    "        for batch in tqdm(datamodule.train_dataloader()):\n",
    "            pass\n",
    "\n",
    "        for batch in tqdm(datamodule.val_dataloader()):\n",
    "            pass\n",
    "\n",
    "        for batch in tqdm(datamodule.test_dataloader()):\n",
    "            pass\n",
    "\n",
    "        return\n",
    "\n",
    "    model = _build_model(config)\n",
    "    loss_handler = _build_loss(config)\n",
    "\n",
    "    system = EndToEndLightningSystem(\n",
    "        model=model,\n",
    "        loss_handler=loss_handler,\n",
    "        metrics=_dummy_metrics(config),\n",
    "        augmentation_handler=_dummy_augmentation(),\n",
    "        inference_handler=config.get(\"inference\", None),\n",
    "        optimization_bundle=_build_optimization_bundle(config),\n",
    "        fast_run=config.fast_run,\n",
    "        batch_size=config.data.batch_size,\n",
    "        effective_batch_size=config.data.get(\"effective_batch_size\", None),\n",
    "        commitment_weight=config.get(\"commitment_weight\", 1.0),\n",
    "    )\n",
    "\n",
    "    rand_str = \"\".join(\n",
    "        random.choice(string.ascii_uppercase + string.digits) for _ in range(6)\n",
    "    )\n",
    "\n",
    "    logger = pytorch_lightning.loggers.TensorBoardLogger(\n",
    "        save_dir=os.path.join(\n",
    "            config.trainer.logger.save_dir, os.environ.get(\"SLURM_JOB_ID\", rand_str)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "            monitor=config.trainer.callbacks.checkpoint.monitor,\n",
    "            mode=config.trainer.callbacks.checkpoint.mode,\n",
    "            save_top_k=config.trainer.callbacks.checkpoint.save_top_k,\n",
    "            save_last=config.trainer.callbacks.checkpoint.save_last,\n",
    "        ),\n",
    "        pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "            monitor=None,\n",
    "        ),  # also save the last 3 epochs\n",
    "        pytorch_lightning.callbacks.RichModelSummary(max_depth=3),\n",
    "    ]\n",
    "\n",
    "    if profile:\n",
    "        profiler = AdvancedProfiler(filename=\"profile.txt\", dirpath=\".\")\n",
    "\n",
    "    if config.trainer.accumulate_grad_batches is None:\n",
    "        config.trainer.accumulate_grad_batches = 1\n",
    "        if config.data.effective_batch_size is not None:\n",
    "            config.trainer.accumulate_grad_batches = int(\n",
    "                config.data.effective_batch_size / config.data.batch_size\n",
    "            )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        max_epochs=1 if profile else config.trainer.max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        profiler=profiler if profile else None,\n",
    "        limit_train_batches=int(8) if profile else float(1.0),\n",
    "        limit_val_batches=int(8) if profile else float(1.0),\n",
    "        accumulate_grad_batches=config.trainer.accumulate_grad_batches,\n",
    "        precision=precision,\n",
    "        gradient_clip_val=config.trainer.get(\"gradient_clip_val\", None),\n",
    "        gradient_clip_algorithm=config.trainer.get(\"gradient_clip_algorithm\", \"norm\"),\n",
    "    )\n",
    "\n",
    "    if validate_only:\n",
    "        trainer.validate(system, datamodule, ckpt_path=ckpt_path)\n",
    "    elif inference_only:\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.join(\n",
    "                os.path.dirname(os.path.dirname(ckpt_path)), \"inference\"\n",
    "            )\n",
    "            system.set_output_path(output_dir)\n",
    "        trainer.predict(system, datamodule, ckpt_path=ckpt_path)\n",
    "    else:\n",
    "        trainer.logger.log_hyperparams(OmegaConf.to_object(config))\n",
    "        trainer.logger.save()\n",
    "        trainer.fit(system, datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "\n",
    "def query_validate(config_path: str, ckpt_path: str):\n",
    "    config = _load_config(config_path)\n",
    "\n",
    "    datamodule = _build_datamodule(config)\n",
    "\n",
    "    model = _build_model(config)\n",
    "    loss_handler = _build_loss(config)\n",
    "\n",
    "    system = EndToEndLightningSystem(\n",
    "        model=model,\n",
    "        loss_handler=loss_handler,\n",
    "        metrics=_dummy_metrics(config),\n",
    "        augmentation_handler=_dummy_augmentation(),\n",
    "        inference_handler=None,\n",
    "        optimization_bundle=_build_optimization_bundle(config),\n",
    "        fast_run=config.fast_run,\n",
    "        batch_size=config.data.batch_size,\n",
    "        effective_batch_size=config.data.get(\"effective_batch_size\", None),\n",
    "        commitment_weight=config.get(\"commitment_weight\", 1.0),\n",
    "    )\n",
    "\n",
    "    logger = pytorch_lightning.loggers.CSVLogger(\n",
    "        save_dir=os.path.join(config.trainer.logger.save_dir, \"validate\"),\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    allowed_stems = config.data.val_kwargs.get(\"allowed_stems\", None)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    os.makedirs(trainer.logger.log_dir, exist_ok=True)\n",
    "\n",
    "    with open(trainer.logger.log_dir + \"/config.txt\", \"w\") as f:\n",
    "        f.write(ckpt_path)\n",
    "\n",
    "    dl = datamodule.val_dataloader()\n",
    "\n",
    "    for stem, val_dl in zip(allowed_stems, dl):\n",
    "        metrics = trainer.validate(system, val_dl, ckpt_path=ckpt_path)[0]\n",
    "        print(stem)\n",
    "        pprint(metrics)\n",
    "\n",
    "        for metric, value in metrics.items():\n",
    "            data.append({\"metric\": metric, \"value\": value, \"stem\": stem})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df.to_csv(\n",
    "        os.path.join(trainer.logger.log_dir, \"validation_metrics.csv\"), index=False\n",
    "    )\n",
    "\n",
    "\n",
    "def query_test(config_path: str, ckpt_path: str):\n",
    "    config = _load_config(config_path)\n",
    "\n",
    "    pprint(config)\n",
    "    pprint(config.data.inference_kwargs)\n",
    "\n",
    "    datamodule = _build_datamodule(config)\n",
    "\n",
    "    model = _build_model(config)\n",
    "    loss_handler = _build_loss(config)\n",
    "\n",
    "    system = EndToEndLightningSystem(\n",
    "        model=model,\n",
    "        loss_handler=loss_handler,\n",
    "        metrics=_dummy_metrics(config),\n",
    "        augmentation_handler=_dummy_augmentation(),\n",
    "        inference_handler=config.data.inference_kwargs,\n",
    "        optimization_bundle=_build_optimization_bundle(config),\n",
    "        fast_run=config.fast_run,\n",
    "        batch_size=config.data.batch_size,\n",
    "        effective_batch_size=config.data.get(\"effective_batch_size\", None),\n",
    "        commitment_weight=config.get(\"commitment_weight\", 1.0),\n",
    "    )\n",
    "\n",
    "    rand_str = \"\".join(\n",
    "        random.choice(string.ascii_uppercase + string.digits) for _ in range(6)\n",
    "    )\n",
    "\n",
    "    use_own_query = config.data.test_kwargs.get(\"use_own_query\", False)\n",
    "\n",
    "    prefix = \"test-o\" if use_own_query else \"test\"\n",
    "\n",
    "    logger = pytorch_lightning.loggers.CSVLogger(\n",
    "        save_dir=os.path.join(\n",
    "            config.trainer.logger.save_dir,\n",
    "            prefix,\n",
    "            os.environ.get(\"SLURM_JOB_ID\", rand_str),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    os.makedirs(trainer.logger.log_dir, exist_ok=True)\n",
    "\n",
    "    with open(trainer.logger.log_dir + \"/config.txt\", \"w\") as f:\n",
    "        f.write(ckpt_path)\n",
    "\n",
    "    trainer.logger.log_hyperparams(OmegaConf.to_object(config))\n",
    "    trainer.logger.save()\n",
    "\n",
    "    dl = datamodule.test_dataloader()\n",
    "\n",
    "    trainer.test(system, dl, ckpt_path=ckpt_path)\n",
    "\n",
    "def query_inference(config_path: str, ckpt_path: str):\n",
    "    config = _load_config(config_path)\n",
    "\n",
    "    pprint(config)\n",
    "    pprint(config.data.inference_kwargs)\n",
    "\n",
    "    datamodule = _build_datamodule(config)\n",
    "\n",
    "    model = _build_model(config)\n",
    "    loss_handler = _build_loss(config)\n",
    "\n",
    "    system = EndToEndLightningSystem(\n",
    "        model=model,\n",
    "        loss_handler=loss_handler,\n",
    "        metrics=_dummy_metrics(config),\n",
    "        augmentation_handler=_dummy_augmentation(),\n",
    "        inference_handler=config.data.inference_kwargs,\n",
    "        optimization_bundle=_build_optimization_bundle(config),\n",
    "        fast_run=config.fast_run,\n",
    "        batch_size=config.data.batch_size,\n",
    "        effective_batch_size=config.data.get(\"effective_batch_size\", None),\n",
    "        commitment_weight=config.get(\"commitment_weight\", 1.0),\n",
    "    )\n",
    "\n",
    "    rand_str = \"\".join(\n",
    "        random.choice(string.ascii_uppercase + string.digits) for _ in range(6)\n",
    "    )\n",
    "\n",
    "    use_own_query = config.data.test_kwargs.get(\"use_own_query\", False)\n",
    "\n",
    "    prefix = \"inference-o\" if use_own_query else \"inference-d\"\n",
    "\n",
    "    logger = pytorch_lightning.loggers.CSVLogger(\n",
    "        save_dir=os.path.join(\n",
    "            config.trainer.logger.save_dir,\n",
    "            prefix,\n",
    "            os.environ.get(\"SLURM_JOB_ID\", rand_str),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "    os.makedirs(trainer.logger.log_dir, exist_ok=True)\n",
    "\n",
    "    with open(trainer.logger.log_dir + \"/config.txt\", \"w\") as f:\n",
    "        f.write(ckpt_path)\n",
    "\n",
    "    trainer.logger.log_hyperparams(OmegaConf.to_object(config))\n",
    "    trainer.logger.save()\n",
    "\n",
    "    dl = datamodule.test_dataloader()\n",
    "\n",
    "    trainer.predict(system, dl, ckpt_path=ckpt_path)\n",
    "\n",
    "\n",
    "def clean_validation_metrics(path):\n",
    "    df = pd.read_csv(path).T\n",
    "\n",
    "    data = []\n",
    "\n",
    "    stems = [\n",
    "        \"drums\",\n",
    "        \"lead_male_singer\",\n",
    "        \"lead_female_singer\",\n",
    "        # \"human_choir\",\n",
    "        \"background_vocals\",\n",
    "        # \"other_vocals\",\n",
    "        \"bass_guitar\",\n",
    "        \"bass_synthesizer\",\n",
    "        # \"contrabass_double_bass\",\n",
    "        # \"tuba\",\n",
    "        # \"bassoon\",\n",
    "        \"fx\",\n",
    "        \"clean_electric_guitar\",\n",
    "        \"distorted_electric_guitar\",\n",
    "        # \"lap_steel_guitar_or_slide_guitar\",\n",
    "        \"acoustic_guitar\",\n",
    "        \"other_plucked\",\n",
    "        \"pitched_percussion\",\n",
    "        \"grand_piano\",\n",
    "        \"electric_piano\",\n",
    "        \"organ_electric_organ\",\n",
    "        \"synth_pad\",\n",
    "        \"synth_lead\",\n",
    "        # \"violin\",\n",
    "        # \"viola\",\n",
    "        # \"cello\",\n",
    "        # \"violin_section\",\n",
    "        # \"viola_section\",\n",
    "        # \"cello_section\",\n",
    "        \"string_section\",\n",
    "        \"other_strings\",\n",
    "        \"brass\",\n",
    "        # \"flutes\",\n",
    "        \"reeds\",\n",
    "        \"other_wind\",\n",
    "    ]\n",
    "\n",
    "    for metric, value in df.iterrows():\n",
    "\n",
    "        mm = metric.split(\"/\")\n",
    "        idx = mm[-1]\n",
    "        m = \"/\".join(mm[:-1])\n",
    "\n",
    "        print(metric, idx)\n",
    "\n",
    "        try:\n",
    "            idx = int(idx.split(\"_\")[-1])\n",
    "        except ValueError as e:\n",
    "            assert \"invalid literal for int() with base 10\" in str(e)\n",
    "            continue\n",
    "\n",
    "        data.append({m: value, \"stem\": stems[idx]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    new_path = path.replace(\".csv\", \"_clean.csv\")\n",
    "\n",
    "    df.to_csv(new_path, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import fire\n",
    "\n",
    "    fire.Fire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
